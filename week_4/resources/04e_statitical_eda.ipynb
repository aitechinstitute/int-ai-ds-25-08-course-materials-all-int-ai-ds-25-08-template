{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8287c6",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 04 ¬∑ Notebook 05 ‚Äì Statistical EDA Fundamentals\n",
    "**Instructor:** Amir Charkhi  |  **Goal:** Master statistical techniques for exploratory data analysis.\n",
    "\n",
    "> Format: short theory ‚Üí quick practice ‚Üí build understanding ‚Üí mini-challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a35e8bb",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Objectives\n",
    "- Use statistical summaries to understand data\n",
    "- Detect outliers and anomalies statistically\n",
    "- Understand relationships between variables\n",
    "- Build comprehensive EDA workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade0238f",
   "metadata": {},
   "source": [
    "## 1. The Five-Number Summary & Beyond\n",
    "Starting with the basics but going deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f18528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fa7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic e-commerce data\n",
    "np.random.seed(42)\n",
    "n_customers = 1000\n",
    "\n",
    "# Create customer purchase data\n",
    "customer_data = pd.DataFrame({\n",
    "    'customer_id': range(1, n_customers + 1),\n",
    "    'age': np.random.normal(35, 12, n_customers).clip(18, 80).astype(int),\n",
    "    'total_purchases': np.random.poisson(5, n_customers),\n",
    "    'avg_order_value': np.random.lognormal(3.5, 0.8, n_customers),\n",
    "    'days_since_signup': np.random.exponential(180, n_customers),\n",
    "    'email_opens': np.random.binomial(20, 0.3, n_customers),\n",
    "    'is_premium': np.random.choice([0, 1], n_customers, p=[0.8, 0.2])\n",
    "})\n",
    "\n",
    "# Add some outliers\n",
    "customer_data.loc[np.random.choice(customer_data.index, 10), 'avg_order_value'] *= 10\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(customer_data.head())\n",
    "print(f\"\\nShape: {customer_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_summary(df, column):\n",
    "    \"\"\"Enhanced statistical summary\"\"\"\n",
    "    data = df[column].dropna()\n",
    "    \n",
    "    summary = {\n",
    "        'count': len(data),\n",
    "        'missing': df[column].isna().sum(),\n",
    "        'mean': data.mean(),\n",
    "        'median': data.median(),\n",
    "        'mode': data.mode()[0] if len(data.mode()) > 0 else None,\n",
    "        'std': data.std(),\n",
    "        'variance': data.var(),\n",
    "        'min': data.min(),\n",
    "        'Q1': data.quantile(0.25),\n",
    "        'Q2': data.quantile(0.50),\n",
    "        'Q3': data.quantile(0.75),\n",
    "        'max': data.max(),\n",
    "        'IQR': data.quantile(0.75) - data.quantile(0.25),\n",
    "        'range': data.max() - data.min(),\n",
    "        'skewness': data.skew(),\n",
    "        'kurtosis': data.kurtosis(),\n",
    "        'CV': data.std() / data.mean() * 100  # Coefficient of variation\n",
    "    }\n",
    "    \n",
    "    return pd.Series(summary)\n",
    "\n",
    "# Apply to our data\n",
    "print(\"Comprehensive Summary: Average Order Value\")\n",
    "print(\"=\"*50)\n",
    "summary = comprehensive_summary(customer_data, 'avg_order_value')\n",
    "for key, value in summary.items():\n",
    "    if key == 'CV':\n",
    "        print(f\"{key:15s}: {value:.1f}%\")\n",
    "    else:\n",
    "        print(f\"{key:15s}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd91af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual statistical summary\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Box plot with annotations\n",
    "box = axes[0].boxplot(customer_data['avg_order_value'], vert=True, patch_artist=True)\n",
    "box['boxes'][0].set_facecolor('lightblue')\n",
    "axes[0].set_ylabel('Average Order Value ($)')\n",
    "axes[0].set_title('Five-Number Summary')\n",
    "\n",
    "# Annotate quartiles\n",
    "quartiles = customer_data['avg_order_value'].quantile([0.25, 0.5, 0.75])\n",
    "for q, label in zip(quartiles, ['Q1', 'Median', 'Q3']):\n",
    "    axes[0].annotate(f'{label}: ${q:.0f}', xy=(1.2, q), fontsize=10)\n",
    "\n",
    "# Histogram with statistics overlay\n",
    "axes[1].hist(customer_data['avg_order_value'], bins=50, density=True, alpha=0.7, color='green')\n",
    "axes[1].axvline(customer_data['avg_order_value'].mean(), color='red', linestyle='--', label=f'Mean: ${customer_data[\"avg_order_value\"].mean():.0f}')\n",
    "axes[1].axvline(customer_data['avg_order_value'].median(), color='blue', linestyle='--', label=f'Median: ${customer_data[\"avg_order_value\"].median():.0f}')\n",
    "axes[1].set_xlabel('Average Order Value ($)')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title('Distribution with Central Tendency')\n",
    "axes[1].legend()\n",
    "\n",
    "# Violin plot for shape understanding\n",
    "parts = axes[2].violinplot([customer_data['avg_order_value']], positions=[1], \n",
    "                           showmeans=True, showmedians=True)\n",
    "axes[2].set_ylabel('Average Order Value ($)')\n",
    "axes[2].set_title('Distribution Shape')\n",
    "axes[2].set_xticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f40a1",
   "metadata": {},
   "source": [
    "**Exercise 1 ‚Äì Interpret Skewness (easy)**  \n",
    "Calculate and interpret skewness for different variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n",
    "# Calculate skewness for age, total_purchases, and days_since_signup\n",
    "# Interpret what each tells you about the distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265e60a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "variables = ['age', 'total_purchases', 'days_since_signup']\n",
    "\n",
    "print(\"Skewness Analysis:\")\n",
    "print(\"=\"*50)\n",
    "for var in variables:\n",
    "    skew = customer_data[var].skew()\n",
    "    print(f\"\\n{var}:\")\n",
    "    print(f\"  Skewness: {skew:.3f}\")\n",
    "    \n",
    "    if abs(skew) < 0.5:\n",
    "        interpretation = \"Fairly symmetric\"\n",
    "    elif skew > 0.5:\n",
    "        interpretation = \"Right-skewed (long tail to the right)\"\n",
    "    else:\n",
    "        interpretation = \"Left-skewed (long tail to the left)\"\n",
    "    \n",
    "    print(f\"  Interpretation: {interpretation}\")\n",
    "    \n",
    "    if skew > 1:\n",
    "        print(f\"  üí° Consider log transformation\")\n",
    "    elif skew < -1:\n",
    "        print(f\"  üí° Consider square transformation\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i, var in enumerate(variables):\n",
    "    axes[i].hist(customer_data[var], bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_title(f'{var}\\nSkew: {customer_data[var].skew():.2f}')\n",
    "    axes[i].set_xlabel(var)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c78e3",
   "metadata": {},
   "source": [
    "## 2. Statistical Outlier Detection\n",
    "Multiple methods to find anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948a3661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_multiple_methods(df, column):\n",
    "    \"\"\"Compare different outlier detection methods\"\"\"\n",
    "    data = df[column].dropna()\n",
    "    \n",
    "    outliers = {}\n",
    "    \n",
    "    # Method 1: IQR Method\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers['IQR'] = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    \n",
    "    # Method 2: Z-Score Method\n",
    "    z_scores = np.abs(stats.zscore(data))\n",
    "    outliers['Z-Score'] = data[z_scores > 3]\n",
    "    \n",
    "    # Method 3: Modified Z-Score (using MAD)\n",
    "    median = data.median()\n",
    "    mad = np.median(np.abs(data - median))\n",
    "    modified_z_scores = 0.6745 * (data - median) / mad\n",
    "    outliers['Modified Z-Score'] = data[np.abs(modified_z_scores) > 3.5]\n",
    "    \n",
    "    # Method 4: Isolation Forest\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "    predictions = iso_forest.fit_predict(data.values.reshape(-1, 1))\n",
    "    outliers['Isolation Forest'] = data[predictions == -1]\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Apply outlier detection\n",
    "outliers = detect_outliers_multiple_methods(customer_data, 'avg_order_value')\n",
    "\n",
    "print(\"Outlier Detection Results:\")\n",
    "print(\"=\"*50)\n",
    "for method, outlier_values in outliers.items():\n",
    "    print(f\"{method:20s}: {len(outlier_values)} outliers found\")\n",
    "    if len(outlier_values) > 0:\n",
    "        print(f\"  Range: ${outlier_values.min():.0f} - ${outlier_values.max():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82fed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "methods = list(outliers.keys())\n",
    "\n",
    "for idx, (method, outlier_values) in enumerate(outliers.items()):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Plot all data\n",
    "    ax.scatter(range(len(customer_data)), customer_data['avg_order_value'], \n",
    "              alpha=0.5, s=20, label='Normal')\n",
    "    \n",
    "    # Highlight outliers\n",
    "    outlier_indices = customer_data[customer_data['avg_order_value'].isin(outlier_values)].index\n",
    "    ax.scatter(outlier_indices, outlier_values, \n",
    "              color='red', s=50, label='Outliers', zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('Customer Index')\n",
    "    ax.set_ylabel('Average Order Value ($)')\n",
    "    ax.set_title(f'{method} Method\\n({len(outlier_values)} outliers)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93cc6d3",
   "metadata": {},
   "source": [
    "**Exercise 2 ‚Äì Outlier Impact Analysis (medium)**  \n",
    "Compare statistics with and without outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127850d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n",
    "# Remove outliers using IQR method\n",
    "# Compare mean, median, std before and after\n",
    "# Which statistics are robust to outliers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f25302",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Remove outliers using IQR\n",
    "data = customer_data['avg_order_value']\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "data_clean = data[(data >= lower) & (data <= upper)]\n",
    "n_outliers = len(data) - len(data_clean)\n",
    "\n",
    "# Compare statistics\n",
    "comparison = pd.DataFrame({\n",
    "    'With Outliers': [\n",
    "        data.mean(),\n",
    "        data.median(),\n",
    "        data.std(),\n",
    "        data.quantile(0.25),\n",
    "        data.quantile(0.75)\n",
    "    ],\n",
    "    'Without Outliers': [\n",
    "        data_clean.mean(),\n",
    "        data_clean.median(),\n",
    "        data_clean.std(),\n",
    "        data_clean.quantile(0.25),\n",
    "        data_clean.quantile(0.75)\n",
    "    ]\n",
    "}, index=['Mean', 'Median', 'Std Dev', 'Q1', 'Q3'])\n",
    "\n",
    "comparison['% Change'] = (comparison['Without Outliers'] - comparison['With Outliers']) / comparison['With Outliers'] * 100\n",
    "\n",
    "print(f\"Outlier Impact Analysis ({n_outliers} outliers removed)\")\n",
    "print(\"=\"*60)\n",
    "print(comparison.round(2))\n",
    "\n",
    "print(\"\\nüìä Insights:\")\n",
    "print(f\"‚Ä¢ Mean changed by {comparison.loc['Mean', '% Change']:.1f}% (NOT robust)\")\n",
    "print(f\"‚Ä¢ Median changed by {comparison.loc['Median', '% Change']:.1f}% (Robust)\")\n",
    "print(f\"‚Ä¢ Std Dev changed by {comparison.loc['Std Dev', '% Change']:.1f}% (NOT robust)\")\n",
    "print(f\"‚Ä¢ Quartiles changed by <{max(abs(comparison.loc['Q1', '% Change']), abs(comparison.loc['Q3', '% Change'])):.1f}% (Robust)\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac1848",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis & Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different types of correlation\n",
    "def correlation_analysis(df):\n",
    "    \"\"\"Comprehensive correlation analysis\"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Pearson correlation (linear relationships)\n",
    "    pearson_corr = df[numeric_cols].corr(method='pearson')\n",
    "    \n",
    "    # Spearman correlation (monotonic relationships)\n",
    "    spearman_corr = df[numeric_cols].corr(method='spearman')\n",
    "    \n",
    "    # Kendall correlation (ordinal relationships)\n",
    "    kendall_corr = df[numeric_cols].corr(method='kendall')\n",
    "    \n",
    "    return pearson_corr, spearman_corr, kendall_corr\n",
    "\n",
    "pearson, spearman, kendall = correlation_analysis(customer_data)\n",
    "\n",
    "# Visualize different correlations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Pearson\n",
    "sns.heatmap(pearson, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            square=True, ax=axes[0], cbar_kws={\"shrink\": 0.8})\n",
    "axes[0].set_title('Pearson Correlation\\n(Linear Relationships)')\n",
    "\n",
    "# Spearman\n",
    "sns.heatmap(spearman, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            square=True, ax=axes[1], cbar_kws={\"shrink\": 0.8})\n",
    "axes[1].set_title('Spearman Correlation\\n(Monotonic Relationships)')\n",
    "\n",
    "# Kendall\n",
    "sns.heatmap(kendall, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            square=True, ax=axes[2], cbar_kws={\"shrink\": 0.8})\n",
    "axes[2].set_title('Kendall Correlation\\n(Ordinal Relationships)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948a3661-2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical significance of correlations\n",
    "def correlation_significance(df, var1, var2):\n",
    "    \"\"\"Test correlation significance\"\"\"\n",
    "    data1 = df[var1].dropna()\n",
    "    data2 = df[var2].dropna()\n",
    "    \n",
    "    # Ensure same length\n",
    "    common_idx = data1.index.intersection(data2.index)\n",
    "    data1 = data1[common_idx]\n",
    "    data2 = data2[common_idx]\n",
    "    \n",
    "    # Pearson\n",
    "    pearson_r, pearson_p = stats.pearsonr(data1, data2)\n",
    "    \n",
    "    # Spearman\n",
    "    spearman_r, spearman_p = stats.spearmanr(data1, data2)\n",
    "    \n",
    "    print(f\"Correlation between {var1} and {var2}:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Pearson:  r = {pearson_r:.3f}, p-value = {pearson_p:.4f}\")\n",
    "    print(f\"Spearman: œÅ = {spearman_r:.3f}, p-value = {spearman_p:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if pearson_p < 0.05:\n",
    "        strength = \"Strong\" if abs(pearson_r) > 0.7 else \"Moderate\" if abs(pearson_r) > 0.3 else \"Weak\"\n",
    "        direction = \"positive\" if pearson_r > 0 else \"negative\"\n",
    "        print(f\"\\n‚úÖ Significant {strength} {direction} linear relationship\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå No significant linear relationship\")\n",
    "    \n",
    "    # Scatter plot with regression line\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(data1, data2, alpha=0.5)\n",
    "    z = np.polyfit(data1, data2, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(data1, p(data1), \"r--\", alpha=0.8, label=f'r = {pearson_r:.3f}')\n",
    "    plt.xlabel(var1)\n",
    "    plt.ylabel(var2)\n",
    "    plt.title(f'Relationship: {var1} vs {var2}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Test a relationship\n",
    "correlation_significance(customer_data, 'total_purchases', 'email_opens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c85c5e",
   "metadata": {},
   "source": [
    "## 4. Distribution Testing & Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b637638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_normality(data, var_name):\n",
    "    \"\"\"Multiple normality tests\"\"\"\n",
    "    print(f\"Normality Tests for {var_name}:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Shapiro-Wilk Test\n",
    "    stat_sw, p_sw = stats.shapiro(data[:min(5000, len(data))])  # Limited to 5000 samples\n",
    "    print(f\"Shapiro-Wilk: statistic={stat_sw:.4f}, p-value={p_sw:.4f}\")\n",
    "    \n",
    "    # Kolmogorov-Smirnov Test\n",
    "    stat_ks, p_ks = stats.kstest(data, 'norm', args=(data.mean(), data.std()))\n",
    "    print(f\"Kolmogorov-Smirnov: statistic={stat_ks:.4f}, p-value={p_ks:.4f}\")\n",
    "    \n",
    "    # Anderson-Darling Test\n",
    "    result_ad = stats.anderson(data, dist='norm')\n",
    "    print(f\"Anderson-Darling: statistic={result_ad.statistic:.4f}\")\n",
    "    print(f\"  Critical values: {result_ad.critical_values}\")\n",
    "    print(f\"  Significance levels: {result_ad.significance_level}%\")\n",
    "    \n",
    "    # D'Agostino-Pearson Test\n",
    "    stat_dp, p_dp = stats.normaltest(data)\n",
    "    print(f\"D'Agostino-Pearson: statistic={stat_dp:.4f}, p-value={p_dp:.4f}\")\n",
    "    \n",
    "    # Overall conclusion\n",
    "    p_values = [p_sw, p_ks, p_dp]\n",
    "    normal_count = sum(p > 0.05 for p in p_values)\n",
    "    \n",
    "    print(f\"\\nüìä Conclusion: {normal_count}/3 tests suggest normality\")\n",
    "    if normal_count >= 2:\n",
    "        print(\"‚úÖ Data appears to be normally distributed\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Data may not be normally distributed\")\n",
    "        print(\"Consider transformations: log, sqrt, or Box-Cox\")\n",
    "\n",
    "# Test normality for age\n",
    "test_normality(customer_data['age'].values, 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual normality assessment\n",
    "def visual_normality_check(data, var_name):\n",
    "    \"\"\"Visual methods to assess normality\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Histogram with normal overlay\n",
    "    axes[0, 0].hist(data, bins=30, density=True, alpha=0.7, color='blue', edgecolor='black')\n",
    "    mu, std = data.mean(), data.std()\n",
    "    x = np.linspace(data.min(), data.max(), 100)\n",
    "    axes[0, 0].plot(x, stats.norm.pdf(x, mu, std), 'r-', linewidth=2, label='Normal fit')\n",
    "    axes[0, 0].set_title(f'Histogram of {var_name}')\n",
    "    axes[0, 0].set_xlabel('Value')\n",
    "    axes[0, 0].set_ylabel('Density')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Q-Q plot\n",
    "    stats.probplot(data, dist=\"norm\", plot=axes[0, 1])\n",
    "    axes[0, 1].set_title('Q-Q Plot')\n",
    "    \n",
    "    # Box plot\n",
    "    axes[1, 0].boxplot(data, vert=True)\n",
    "    axes[1, 0].set_title('Box Plot')\n",
    "    axes[1, 0].set_ylabel('Value')\n",
    "    \n",
    "    # P-P plot\n",
    "    probplot = stats.probplot(data, dist=\"norm\")\n",
    "    theoretical_percentiles = np.linspace(0, 100, len(data))\n",
    "    sample_percentiles = np.percentile(data, theoretical_percentiles)\n",
    "    norm_percentiles = stats.norm.ppf(theoretical_percentiles/100, mu, std)\n",
    "    \n",
    "    axes[1, 1].scatter(norm_percentiles, sample_percentiles, alpha=0.5)\n",
    "    axes[1, 1].plot([data.min(), data.max()], [data.min(), data.max()], 'r--', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Theoretical Percentiles')\n",
    "    axes[1, 1].set_ylabel('Sample Percentiles')\n",
    "    axes[1, 1].set_title('P-P Plot')\n",
    "    \n",
    "    plt.suptitle(f'Normality Assessment: {var_name}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visual_normality_check(customer_data['age'].values, 'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f39466",
   "metadata": {},
   "source": [
    "**Exercise 3 ‚Äì Transform to Normality (medium)**  \n",
    "Apply transformations to make skewed data more normal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n",
    "# Take avg_order_value (skewed) and try:\n",
    "# 1. Log transformation\n",
    "# 2. Square root transformation\n",
    "# 3. Box-Cox transformation\n",
    "# Which works best?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54870cb",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Original data\n",
    "original = customer_data['avg_order_value'].values\n",
    "\n",
    "# Transformations\n",
    "log_transform = np.log1p(original)  # log(1+x) to handle zeros\n",
    "sqrt_transform = np.sqrt(original)\n",
    "boxcox_transform, lambda_param = stats.boxcox(original + 1)  # Add 1 to handle zeros\n",
    "\n",
    "# Test normality for each\n",
    "transformations = {\n",
    "    'Original': original,\n",
    "    'Log': log_transform,\n",
    "    'Square Root': sqrt_transform,\n",
    "    'Box-Cox': boxcox_transform\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, data in transformations.items():\n",
    "    stat, p_value = stats.shapiro(data[:5000])\n",
    "    skew = stats.skew(data)\n",
    "    kurt = stats.kurtosis(data)\n",
    "    results.append({\n",
    "        'Transformation': name,\n",
    "        'Shapiro p-value': p_value,\n",
    "        'Skewness': skew,\n",
    "        'Kurtosis': kurt,\n",
    "        'Normal?': 'Yes' if p_value > 0.05 else 'No'\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Transformation Comparison:\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(f\"\\nBox-Cox lambda: {lambda_param:.3f}\")\n",
    "\n",
    "# Visualize best transformation\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, (name, data) in enumerate(transformations.items()):\n",
    "    axes[i].hist(data, bins=30, density=True, alpha=0.7)\n",
    "    mu, std = data.mean(), data.std()\n",
    "    x = np.linspace(data.min(), data.max(), 100)\n",
    "    axes[i].plot(x, stats.norm.pdf(x, mu, std), 'r-', linewidth=2)\n",
    "    axes[i].set_title(f'{name}\\nSkew: {stats.skew(data):.2f}')\n",
    "    axes[i].set_xlabel('Value')\n",
    "\n",
    "plt.suptitle('Transformation Effects on Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Best transformation: {results_df.loc[results_df['Shapiro p-value'].idxmax(), 'Transformation']}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93813ac",
   "metadata": {},
   "source": [
    "## 5. Comprehensive EDA Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de98963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalEDA:\n",
    "    \"\"\"Complete statistical EDA framework\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        self.categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    def basic_info(self):\n",
    "        \"\"\"Dataset overview\"\"\"\n",
    "        print(\"üìä DATASET OVERVIEW\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Shape: {self.df.shape[0]} rows √ó {self.df.shape[1]} columns\")\n",
    "        print(f\"Memory usage: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        print(f\"\\nColumn Types:\")\n",
    "        print(self.df.dtypes.value_counts())\n",
    "        print(f\"\\nMissing Values:\")\n",
    "        missing = self.df.isnull().sum()\n",
    "        if missing.sum() > 0:\n",
    "            print(missing[missing > 0].sort_values(ascending=False))\n",
    "        else:\n",
    "            print(\"No missing values\")\n",
    "    \n",
    "    def univariate_analysis(self):\n",
    "        \"\"\"Analyze each variable individually\"\"\"\n",
    "        print(\"\\nüìà UNIVARIATE ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for col in self.numeric_cols:\n",
    "            print(f\"\\n{col}:\")\n",
    "            data = self.df[col].dropna()\n",
    "            \n",
    "            # Key statistics\n",
    "            print(f\"  Mean: {data.mean():.2f}, Median: {data.median():.2f}\")\n",
    "            print(f\"  Std: {data.std():.2f}, IQR: {data.quantile(0.75) - data.quantile(0.25):.2f}\")\n",
    "            print(f\"  Skewness: {data.skew():.2f}, Kurtosis: {data.kurtosis():.2f}\")\n",
    "            \n",
    "            # Normality test\n",
    "            if len(data) > 20:\n",
    "                _, p_value = stats.shapiro(data[:5000])\n",
    "                print(f\"  Normal: {'Yes' if p_value > 0.05 else 'No'} (p={p_value:.4f})\")\n",
    "    \n",
    "    def bivariate_analysis(self):\n",
    "        \"\"\"Analyze relationships between variables\"\"\"\n",
    "        print(\"\\nüîó BIVARIATE ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Find strong correlations\n",
    "        corr_matrix = self.df[self.numeric_cols].corr()\n",
    "        strong_corr = []\n",
    "        \n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                if abs(corr_matrix.iloc[i, j]) > 0.5:\n",
    "                    strong_corr.append((\n",
    "                        corr_matrix.columns[i],\n",
    "                        corr_matrix.columns[j],\n",
    "                        corr_matrix.iloc[i, j]\n",
    "                    ))\n",
    "        \n",
    "        if strong_corr:\n",
    "            print(\"Strong correlations (|r| > 0.5):\")\n",
    "            for var1, var2, corr in sorted(strong_corr, key=lambda x: abs(x[2]), reverse=True):\n",
    "                print(f\"  {var1} ‚Üî {var2}: {corr:.3f}\")\n",
    "        else:\n",
    "            print(\"No strong correlations found\")\n",
    "    \n",
    "    def outlier_summary(self):\n",
    "        \"\"\"Summarize outliers across all variables\"\"\"\n",
    "        print(\"\\n‚ö†Ô∏è OUTLIER SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for col in self.numeric_cols:\n",
    "            data = self.df[col].dropna()\n",
    "            Q1 = data.quantile(0.25)\n",
    "            Q3 = data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = data[(data < Q1 - 1.5*IQR) | (data > Q3 + 1.5*IQR)]\n",
    "            \n",
    "            if len(outliers) > 0:\n",
    "                print(f\"{col}: {len(outliers)} outliers ({len(outliers)/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate complete EDA report\"\"\"\n",
    "        self.basic_info()\n",
    "        self.univariate_analysis()\n",
    "        self.bivariate_analysis()\n",
    "        self.outlier_summary()\n",
    "\n",
    "# Run comprehensive EDA\n",
    "eda = StatisticalEDA(customer_data)\n",
    "eda.generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de98963a1",
   "metadata": {},
   "source": [
    "**Exercise 4 ‚Äì Custom EDA Function (hard)**  \n",
    "Create a function that automatically detects and reports data quality issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n",
    "# Create data_quality_check() function that:\n",
    "# 1. Checks for duplicates\n",
    "# 2. Identifies constant columns\n",
    "# 3. Finds high-cardinality categoricals\n",
    "# 4. Detects potential data leakage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cecfa82",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "def data_quality_check(df):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    print(\"üîç DATA QUALITY CHECK\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # 1. Check for duplicates\n",
    "    n_duplicates = df.duplicated().sum()\n",
    "    if n_duplicates > 0:\n",
    "        issues.append(f\"‚ö†Ô∏è {n_duplicates} duplicate rows found\")\n",
    "        print(f\"Duplicate rows: {n_duplicates} ({n_duplicates/len(df)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"‚úÖ No duplicate rows\")\n",
    "    \n",
    "    # 2. Identify constant columns\n",
    "    constant_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() == 1:\n",
    "            constant_cols.append(col)\n",
    "            issues.append(f\"‚ö†Ô∏è Column '{col}' has only one unique value\")\n",
    "    \n",
    "    if constant_cols:\n",
    "        print(f\"\\nConstant columns: {constant_cols}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No constant columns\")\n",
    "    \n",
    "    # 3. High cardinality check\n",
    "    print(\"\\nCardinality Check:\")\n",
    "    for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "        cardinality = df[col].nunique()\n",
    "        cardinality_ratio = cardinality / len(df)\n",
    "        \n",
    "        if cardinality_ratio > 0.95:\n",
    "            issues.append(f\"‚ö†Ô∏è Column '{col}' has very high cardinality ({cardinality_ratio:.1%})\")\n",
    "            print(f\"  {col}: {cardinality} unique values ({cardinality_ratio:.1%} of rows)\")\n",
    "    \n",
    "    # 4. Potential data leakage detection\n",
    "    print(\"\\nData Leakage Check:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        # Check for perfect correlations\n",
    "        for other_col in numeric_cols:\n",
    "            if col != other_col:\n",
    "                corr = df[col].corr(df[other_col])\n",
    "                if abs(corr) > 0.99:\n",
    "                    issues.append(f\"‚ö†Ô∏è Potential leakage: '{col}' and '{other_col}' are perfectly correlated\")\n",
    "                    print(f\"  {col} ‚Üî {other_col}: correlation = {corr:.3f}\")\n",
    "    \n",
    "    # 5. Missing value patterns\n",
    "    print(\"\\nMissing Value Patterns:\")\n",
    "    missing_cols = df.columns[df.isnull().any()]\n",
    "    if len(missing_cols) > 0:\n",
    "        for col in missing_cols:\n",
    "            missing_pct = df[col].isnull().sum() / len(df) * 100\n",
    "            if missing_pct > 50:\n",
    "                issues.append(f\"‚ö†Ô∏è Column '{col}' has {missing_pct:.1f}% missing values\")\n",
    "            print(f\"  {col}: {missing_pct:.1f}% missing\")\n",
    "    else:\n",
    "        print(\"  No missing values\")\n",
    "    \n",
    "    # 6. Outlier prevalence\n",
    "    print(\"\\nOutlier Prevalence:\")\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]\n",
    "        outlier_pct = len(outliers) / len(df) * 100\n",
    "        \n",
    "        if outlier_pct > 10:\n",
    "            issues.append(f\"‚ö†Ô∏è Column '{col}' has {outlier_pct:.1f}% outliers\")\n",
    "            print(f\"  {col}: {outlier_pct:.1f}% outliers\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"SUMMARY: Found {len(issues)} potential issues\")\n",
    "    if issues:\n",
    "        print(\"\\nIssues to address:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  {issue}\")\n",
    "    else:\n",
    "        print(\"‚úÖ Data quality looks good!\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Run data quality check\n",
    "issues = data_quality_check(customer_data)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88bba8",
   "metadata": {},
   "source": [
    "## 6. Mini-Challenges\n",
    "- **M1 (easy):** Calculate and interpret the coefficient of variation for all numeric columns\n",
    "- **M2 (medium):** Implement a function to detect multimodal distributions\n",
    "- **M3 (hard):** Build an automated EDA report generator with visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f834bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn - try the challenges!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2e9d0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solutions</b></summary>\n",
    "\n",
    "```python\n",
    "# M1 - Coefficient of Variation\n",
    "cv_results = []\n",
    "for col in customer_data.select_dtypes(include=[np.number]).columns:\n",
    "    mean = customer_data[col].mean()\n",
    "    std = customer_data[col].std()\n",
    "    cv = (std / mean) * 100 if mean != 0 else np.inf\n",
    "    cv_results.append({'Column': col, 'CV': cv})\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results).sort_values('CV')\n",
    "print(\"Coefficient of Variation (Relative Variability):\")\n",
    "for _, row in cv_df.iterrows():\n",
    "    interpretation = \"Low\" if row['CV'] < 30 else \"Moderate\" if row['CV'] < 60 else \"High\"\n",
    "    print(f\"{row['Column']:20s}: {row['CV']:6.1f}% ({interpretation} variability)\")\n",
    "\n",
    "# M2 - Multimodal Detection\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def detect_multimodal(data, col_name):\n",
    "    hist, bins = np.histogram(data, bins=50)\n",
    "    hist = hist / hist.max()  # Normalize\n",
    "    \n",
    "    # Find peaks\n",
    "    peaks, properties = find_peaks(hist, height=0.3, distance=5)\n",
    "    \n",
    "    n_modes = len(peaks)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(data, bins=50, alpha=0.7, density=True)\n",
    "    \n",
    "    for peak in peaks:\n",
    "        plt.axvline(bins[peak], color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.title(f'{col_name}: {n_modes} mode(s) detected')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()\n",
    "    \n",
    "    if n_modes > 1:\n",
    "        print(f\"‚ö†Ô∏è {col_name} appears to be multimodal with {n_modes} peaks\")\n",
    "        print(\"This might indicate multiple subpopulations or segments\")\n",
    "    return n_modes\n",
    "\n",
    "detect_multimodal(customer_data['avg_order_value'].values, 'Average Order Value')\n",
    "\n",
    "# M3 - Automated EDA Report\n",
    "def generate_visual_eda_report(df, output_file='eda_report.html'):\n",
    "    from io import StringIO\n",
    "    import base64\n",
    "    from io import BytesIO\n",
    "    \n",
    "    html_report = StringIO()\n",
    "    html_report.write('<html><head><title>EDA Report</title></head><body>')\n",
    "    html_report.write('<h1>Automated EDA Report</h1>')\n",
    "    \n",
    "    # Basic info\n",
    "    html_report.write('<h2>Dataset Overview</h2>')\n",
    "    html_report.write(f'<p>Shape: {df.shape[0]} rows √ó {df.shape[1]} columns</p>')\n",
    "    \n",
    "    # Statistics table\n",
    "    html_report.write('<h2>Statistical Summary</h2>')\n",
    "    html_report.write(df.describe().to_html())\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation Matrix')\n",
    "    \n",
    "    # Save plot to base64 string\n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "    img_str = base64.b64encode(buffer.read()).decode()\n",
    "    html_report.write(f'<h2>Correlation Matrix</h2>')\n",
    "    html_report.write(f'<img src=\"data:image/png;base64,{img_str}\" />')\n",
    "    plt.close()\n",
    "    \n",
    "    html_report.write('</body></html>')\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(html_report.getvalue())\n",
    "    \n",
    "    print(f\"‚úÖ Report saved to {output_file}\")\n",
    "\n",
    "# Generate report (uncomment to run)\n",
    "# generate_visual_eda_report(customer_data, 'customer_eda.html')\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba28e65",
   "metadata": {},
   "source": [
    "## Wrap-Up & Next Steps\n",
    "‚úÖ You can perform comprehensive statistical summaries  \n",
    "‚úÖ You know multiple methods for outlier detection  \n",
    "‚úÖ You can assess relationships and correlations  \n",
    "‚úÖ You can test distributions and normality  \n",
    "‚úÖ You built a complete EDA framework  \n",
    "\n",
    "**Next:** Sampling Theory and Law of Large Numbers - Understanding the foundations of statistical inference!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
