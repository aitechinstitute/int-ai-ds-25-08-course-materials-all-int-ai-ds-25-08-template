{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8287c6",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** · *Intermediate AI & Data Science*\n",
    "### Week 04 · Notebook 05 – Statistical EDA Fundamentals\n",
    "**Instructor:** Amir Charkhi  |  **Goal:** Master statistical techniques for exploratory data analysis.\n",
    "\n",
    "> Format: short theory → quick practice → build understanding → mini-challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a35e8bb",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Objectives\n",
    "- Use statistical summaries to understand data\n",
    "- Detect outliers and anomalies statistically\n",
    "- Understand relationships between variables\n",
    "- Build comprehensive EDA workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade0238f",
   "metadata": {},
   "source": [
    "## 1. The Five-Number Summary & Beyond\n",
    "Starting with the basics but going deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f18528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fa7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic e-commerce data\n",
    "np.random.seed(42)\n",
    "n_customers = 1000\n",
    "\n",
    "# Create customer purchase data\n",
    "customer_data = pd.DataFrame({\n",
    "    'customer_id': range(1, n_customers + 1),\n",
    "    'age': np.random.normal(35, 12, n_customers).clip(18, 80).astype(int),\n",
    "    'total_purchases': np.random.poisson(5, n_customers),\n",
    "    'avg_order_value': np.random.lognormal(3.5, 0.8, n_customers),\n",
    "    'days_since_signup': np.random.exponential(180, n_customers),\n",
    "    'email_opens': np.random.binomial(20, 0.3, n_customers),\n",
    "    'is_premium': np.random.choice([0, 1], n_customers, p=[0.8, 0.2])\n",
    "})\n",
    "\n",
    "# Add some outliers\n",
    "customer_data.loc[np.random.choice(customer_data.index, 10), 'avg_order_value'] *= 10\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(customer_data.head())\n",
    "print(f\"\\nShape: {customer_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_summary(df, column):\n",
    "    \"\"\"Enhanced statistical summary\"\"\"\n",
    "    data = df[column].dropna()\n",
    "    \n",
    "    summary = {\n",
    "        'count': len(data),\n",
    "        'missing': df[column].isna().sum(),\n",
    "        'mean': data.mean(),\n",
    "        'median': data.median(),\n",
    "        'mode': data.mode()[0] if len(data.mode()) > 0 else None,\n",
    "        'std': data.std(),\n",
    "        'variance': data.var(),\n",
    "        'min': data.min(),\n",
    "        'Q1': data.quantile(0.25),\n",
    "        'Q2': data.quantile(0.50),\n",
    "        'Q3': data.quantile(0.75),\n",
    "        'max': data.max(),\n",
    "        'IQR': data.quantile(0.75) - data.quantile(0.25),\n",
    "        'range': data.max() - data.min(),\n",
    "        'skewness': data.skew(),\n",
    "        'kurtosis': data.kurtosis(),\n",
    "        'CV': data.std() / data.mean() * 100  # Coefficient of variation\n",
    "    }\n",
    "    \n",
    "    return pd.Series(summary)\n",
    "\n",
    "# Apply to our data\n",
    "print(\"Comprehensive Summary: Average Order Value\")\n",
    "print(\"=\"*50)\n",
    "summary = comprehensive_summary(customer_data, 'avg_order_value')\n",
    "for key, value in summary.items():\n",
    "    if key == 'CV':\n",
    "        print(f\"{key:15s}: {value:.1f}%\")\n",
    "    else:\n",
    "        print(f\"{key:15s}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd91af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual statistical summary\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Box plot with annotations\n",
    "box = axes[0].boxplot(customer_data['avg_order_value'], vert=True, patch_artist=True)\n",
    "box['boxes'][0].set_facecolor('lightblue')\n",
    "axes[0].set_ylabel('Average Order Value ($)')\n",
    "axes[0].set_title('Five-Number Summary')\n",
    "\n",
    "# Annotate quartiles\n",
    "quartiles = customer_data['avg_order_value'].quantile([0.25, 0.5, 0.75])\n",
    "for q, label in zip(quartiles, ['Q1', 'Median', 'Q3']):\n",
    "    axes[0].annotate(f'{label}: ${q:.0f}', xy=(1.2, q), fontsize=10)\n",
    "\n",
    "# Histogram with statistics overlay\n",
    "axes[1].hist(customer_data['avg_order_value'], bins=50, density=True, alpha=0.7, color='green')\n",
    "axes[1].axvline(customer_data['avg_order_value'].mean(), color='red', linestyle='--', label=f'Mean: ${customer_data[\"avg_order_value\"].mean():.0f}')\n",
    "axes[1].axvline(customer_data['avg_order_value'].median(), color='blue', linestyle='--', label=f'Median: ${customer_data[\"avg_order_value\"].median():.0f}')\n",
    "axes[1].set_xlabel('Average Order Value ($)')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].set_title('Distribution with Central Tendency')\n",
    "axes[1].legend()\n",
    "\n",
    "# Violin plot for shape understanding\n",
    "parts = axes[2].violinplot([customer_data['avg_order_value']], positions=[1], \n",
    "                           showmeans=True, showmedians=True)\n",
    "axes[2].set_ylabel('Average Order Value ($)')\n",
    "axes[2].set_title('Distribution Shape')\n",
    "axes[2].set_xticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f40a1",
   "metadata": {},
   "source": [
    "**Exercise 1 – Interpret Skewness (easy)**  \n",
    "Calculate and interpret skewness for different variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n",
    "# Calculate skewness for age, total_purchases, and days_since_signup\n",
    "# Interpret what each tells you about the distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265e60a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "variables = ['age', 'total_purchases', 'days_since_signup']\n",
    "\n",
    "print(\"Skewness Analysis:\")\n",
    "print(\"=\"*50)\n",
    "for var in variables:\n",
    "    skew = customer_data[var].skew()\n",
    "    print(f\"\\n{var}:\")\n",
    "    print(f\"  Skewness: {skew:.3f}\")\n",
    "    \n",
    "    if abs(skew) < 0.5:\n",
    "        interpretation = \"Fairly symmetric\"\n",
    "    elif skew > 0.5:\n",
    "        interpretation = \"Right-skewed (long tail to the right)\"\n",
    "    else:\n",
    "        interpretation = \"Left-skewed (long tail to the left)\"\n",
    "    \n",
    "    print(f\"  Interpretation: {interpretation}\")\n",
    "    \n",
    "    if skew > 1:\n",
    "        print(f\"  💡 Consider log transformation\")\n",
    "    elif skew < -1:\n",
    "        print(f\"  💡 Consider square transformation\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i, var in enumerate(variables):\n",
    "    axes[i].hist(customer_data[var], bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_title(f'{var}\\nSkew: {customer_data[var].skew():.2f}')\n",
    "    axes[i].set_xlabel(var)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c78e3",
   "metadata": {},
   "source": [
    "## 2. Statistical Outlier Detection\n",
    "Multiple methods to find anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948a3661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_multiple_methods(df, column):\n",
    "    \"\"\"Compare different outlier detection methods\"\"\"\n",
    "    data = df[column].dropna()\n",
    "    \n",
    "    outliers = {}\n",
    "    \n",
    "    # Method 1: IQR Method\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers['IQR'] = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    \n",
    "    # Method 2: Z-Score Method\n",
    "    z_scores = np.abs(stats.zscore(data))\n",
    "    outliers['Z-Score'] = data[z_scores > 3]\n",
    "    \n",
    "    # Method 3: Modified Z-Score (using MAD)\n",
    "    median = data.median()\n",
    "    mad = np.median(np.abs(data - median))\n",
    "    modified_z_scores = 0.6745 * (data - median) / mad\n",
    "    outliers['Modified Z-Score'] = data[np.abs(modified_z_scores) > 3.5]\n",
    "    \n",
    "    # Method 4: Isolation Forest\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "    predictions = iso_forest.fit_predict(data.values.reshape(-1, 1))\n",
    "    outliers['Isolation Forest'] = data[predictions == -1]\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Apply outlier detection\n",
    "outliers = detect_outliers_multiple_methods(customer_data, 'avg_order_value')\n",
    "\n",
    "print(\"Outlier Detection Results:\")\n",
    "print(\"=\"*50)\n",
    "for method, outlier_values in outliers.items():\n",
    "    print(f\"{method:20s}: {len(outlier_values)} outliers found\")\n",
    "    if len(outlier_values) > 0:\n",
    "        print(f\"  Range: ${outlier_values.min():.0f} - ${outlier_values.max():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82fed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "methods = list(outliers.keys())\n",
    "\n",
    "for idx, (method, outlier_values) in enumerate(outliers.items()):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Plot all data\n",
    "    ax.scatter(range(len(customer_data)), customer_data['avg_order_value'], \n",
    "              alpha=0.5, s=20, label='Normal')\n",
    "    \n",
    "    # Highlight outliers\n",
    "    outlier_indices = customer_data[customer_data['avg_order_value'].isin(outlier_values)].index\n",
    "    ax.scatter(outlier_indices, outlier_values, \n",
    "              color='red', s=50, label='Outliers', zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('Customer Index')\n",
    "    ax.set_ylabel('Average Order Value ($)')\n",
    "    ax.set_title(f'{method} Method\\n({len(outlier_values)} outliers)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93cc6d3",
   "metadata": {},
   "source": [
    "**Exercise 2 – Outlier Impact Analysis (medium)**  \n",
    "Compare statistics with and without outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127850d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n",
    "# Remove outliers using IQR method\n",
    "# Compare mean, median, std before and after\n",
    "# Which statistics are robust to outliers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f25302",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Remove outliers using IQR\n",
    "data = customer_data['avg_order_value']\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "data_clean = data[(data >= lower) & (data <= upper)]\n",
    "n_outliers = len(data) - len(data_clean)\n",
    "\n",
    "# Compare statistics\n",
    "comparison = pd.DataFrame({\n",
    "    'With Outliers': [\n",
    "        data.mean(),\n",
    "        data.median(),\n",
    "        data.std(),\n",
    "        data.quantile(0.25),\n",
    "        data.quantile(0.75)\n",
    "    ],\n",
    "    'Without Outliers': [\n",
    "        data_clean.mean(),\n",
    "        data_clean.median(),\n",
    "        data_clean.std(),\n",
    "        data_clean.quantile(0.25),\n",
    "        data_clean.quantile(0.75)\n",
    "    ]\n",
    "}, index=['Mean', 'Median', 'Std Dev', 'Q1', 'Q3'])\n",
    "\n",
    "comparison['% Change'] = (comparison['Without Outliers'] - comparison['With Outliers']) / comparison['With Outliers'] * 100\n",
    "\n",
    "print(f\"Outlier Impact Analysis ({n_outliers} outliers removed)\")\n",
    "print(\"=\"*60)\n",
    "print(comparison.round(2))\n",
    "\n",
    "print(\"\\n📊 Insights:\")\n",
    "print(f\"• Mean changed by {comparison.loc['Mean', '% Change']:.1f}% (NOT robust)\")\n",
    "print(f\"• Median changed by {comparison.loc['Median', '% Change']:.1f}% (Robust)\")\n",
    "print(f\"• Std Dev changed by {comparison.loc['Std Dev', '% Change']:.1f}% (NOT robust)\")\n",
    "print(f\"• Quartiles changed by <{max(abs(comparison.loc['Q1', '% Change']), abs(comparison.loc['Q3', '% Change'])):.1f}% (Robust)\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac1848",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis & Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different types of correlation\n",
    "def correlation_analysis(df):\n",
    "    \"\"\"Comprehensive correlation analysis\"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Pearson correlation (linear relationships)\n",
    "    pearson_corr = df[numeric_cols].corr(method='pearson')\n",
    "    \n",
    "    # Spearman correlation (monotonic relationships)\n",
    "    spearman_corr = df[numeric_cols].corr(method='spearman')\n",
    "    \n",
    "    # Kendall correlation (ordinal relationships)\n",
    "    kendall_corr = df[numeric_cols].corr(method='kendall')\n",
    "    \n",
    "    return pearson_corr, spearman_corr, kendall_corr\n",
    "\n",
    "pearson, spearman, kendall = correlation_analysis(customer_data)\n",
    "\n",
    "# Visualize different correlations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Pearson\n",
    "sns.heatmap(pearson, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            square=True, ax=axes[0], cbar_kws={\"shrink\": 0.8})\n",
    "axes[0].set_title('Pearson Correlation\\n(Linear Relationships)')\n",
    "\n",
    "# Spearman\n",
    "sns.heatmap(spearman, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            square=True, ax=axes[1], cbar_kws={\"shrink\": 0.8})\n",
    "axes[1].set_title('Spearman Correlation\\n(Monotonic Relationships)')\n",
    "\n",
    "# Kendall\n",
    "sns.heatmap(kendall, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            square=True, ax=axes[2], cbar_kws={\"shrink\": 0.8})\n",
    "axes[2].set_title('Kendall Correlation\\n(Ordinal Relationships)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948a3661-2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical significance of correlations\n",
    "def correlation_significance(df, var1, var2):\n",
    "    \"\"\"Test correlation significance\"\"\"\n",
    "    data1 = df[var1].dropna()\n",
    "    data2 = df[var2].dropna()\n",
    "    \n",
    "    # Ensure same length\n",
    "    common_idx = data1.index.intersection(data2.index)\n",
    "    data1 = data1[common_idx]\n",
    "    data2 = data2[common_idx]\n",
    "    \n",
    "    # Pearson\n",
    "    pearson_r, pearson_p = stats.pearsonr(data1, data2)\n",
    "    \n",
    "    # Spearman\n",
    "    spearman_r, spearman_p = stats.spearmanr(data1, data2)\n",
    "    \n",
    "    print(f\"Correlation between {var1} and {var2}:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Pearson:  r = {pearson_r:.3f}, p-value = {pearson_p:.4f}\")\n",
    "    print(f\"Spearman: ρ = {spearman_r:.3f}, p-value = {spearman_p:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if pearson_p < 0.05:\n",
    "        strength = \"Strong\" if abs(pearson_r) > 0.7 else \"Moderate\" if abs(pearson_r) > 0.3 else \"Weak\"\n",
    "        direction = \"positive\" if pearson_r > 0 else \"negative\"\n",
    "        print(f\"\\n✅ Significant {strength} {direction} linear relationship\")\n",
    "    else:\n",
    "        print(f\"\\n❌ No significant linear relationship\")\n",
    "    \n",
    "    # Scatter plot with regression line\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(data1, data2, alpha=0.5)\n",
    "    z = np.polyfit(data1, data2, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(data1, p(data1), \"r--\", alpha=0.8, label=f'r = {pearson_r:.3f}')\n",
    "    plt.xlabel(var1)\n",
    "    plt.ylabel(var2)\n",
    "    plt.title(f'Relationship: {var1} vs {var2}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Test a relationship\n",
    "correlation_significance(customer_data, 'total_purchases', 'email_opens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c85c5e",
   "metadata": {},
   "source": [
    "## 4. Distribution Testing & Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b637638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_normality(data, var_name):\n",
    "    \"\"\"Multiple normality tests\"\"\"\n",
    "    print(f\"Normality Tests for {var_name}:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Shapiro-Wilk Test\n",
    "    stat_sw, p_sw = stats.shapiro(data[:min(5000, len(data))])  # Limited to 5000 samples\n",
    "    print(f\"Shapiro-Wilk: statistic={stat_sw:.4f}, p-value={p_sw:.4f}\")\n",
    "    \n",
    "    # Kolmogorov-Smirnov Test\n",
    "    stat_ks, p_ks = stats.kstest(data, 'norm', args=(data.mean(), data.std()))\n",
    "    print(f\"Kolmogorov-Smirnov: statistic={stat_ks:.4f}, p-value={p_ks:.4f}\")\n",
    "    \n",
    "    # Anderson-Darling Test\n",
    "    result_ad = stats.anderson(data, dist='norm')\n",
    "    print(f\"Anderson-Darling: statistic={result_ad.statistic:.4f}\")\n",
    "    print(f\"  Critical values: {result_ad.critical_values}\")\n",
    "    print(f\"  Significance levels: {result_ad.significance_level}%\")\n",
    "    \n",
    "    # D'Agostino-Pearson Test\n",
    "    stat_dp, p_dp = stats.normaltest(data)\n",
    "    print(f\"D'Agostino-Pearson: statistic={stat_dp:.4f}, p-value={p_dp:.4f}\")\n",
    "    \n",
    "    # Overall conclusion\n",
    "    p_values = [p_sw, p_ks, p_dp]\n",
    "    normal_count = sum(p > 0.05 for p in p_values)\n",
    "    \n",
    "    print(f\"\\n📊 Conclusion: {normal_count}/3 tests suggest normality\")\n",
    "    if normal_count >= 2:\n",
    "        print(\"✅ Data appears to be normally distributed\")\n",
    "    else:\n",
    "        print(\"⚠️ Data may not be normally distributed\")\n",
    "        print(\"Consider transformations: log, sqrt, or Box-Cox\")\n",
    "\n",
    "# Test normality for age\n",
    "test_normality(customer_data['age'].values, 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual normality assessment\n",
    "def visual_normality_check(data, var_name):\n",
    "    \"\"\"Visual methods to assess normality\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Histogram with normal overlay\n",
    "    axes[0, 0].hist(data, bins=30, density=True, alpha=0.7, color='blue', edgecolor='black')\n",
    "    mu, std = data.mean(), data.std()\n",
    "    x = np.linspace(data.min(), data.max(), 100)\n",
    "    axes[0, 0].plot(x, stats.norm.pdf(x, mu, std), 'r-', linewidth=2, label='Normal fit')\n",
    "    axes[0, 0].set_title(f'Histogram of {var_name}')\n",
    "    axes[0, 0].set_xlabel('Value')\n",
    "    axes[0, 0].set_ylabel('Density')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Q-Q plot\n",
    "    stats.probplot(data, dist=\"norm\", plot=axes[0, 1])\n",
    "    axes[0, 1].set_title('Q-Q Plot')\n",
    "    \n",
    "    # Box plot\n",
    "    axes[1, 0].boxplot(data, vert=True)\n",
    "    axes[1, 0].set_title('Box Plot')\n",
    "    axes[1, 0].set_ylabel('Value')\n",
    "    \n",
    "    # P-P plot\n",
    "    probplot = stats.probplot(data, dist=\"norm\")\n",
    "    theoretical_percentiles = np.linspace(0, 100, len(data))\n",
    "    sample_percentiles = np.percentile(data, theoretical_percentiles)\n",
    "    norm_percentiles = stats.norm.ppf(theoretical_percentiles/100, mu, std)\n",
    "    \n",
    "    axes[1, 1].scatter(norm_percentiles, sample_percentiles, alpha=0.5)\n",
    "    axes[1, 1].plot([data.min(), data.max()], [data.min(), data.max()], 'r--', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Theoretical Percentiles')\n",
    "    axes[1, 1].set_ylabel('Sample Percentiles')\n",
    "    axes[1, 1].set_title('P-P Plot')\n",
    "    \n",
    "    plt.suptitle(f'Normality Assessment: {var_name}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visual_normality_check(customer_data['age'].values, 'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f39466",
   "metadata": {},
   "source": [
    "**Exercise 3 – Transform to Normality (medium)**  \n",
    "Apply transformations to make skewed data more normal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n",
    "# Take avg_order_value (skewed) and try:\n",
    "# 1. Log transformation\n",
    "# 2. Square root transformation\n",
    "# 3. Box-Cox transformation\n",
    "# Which works best?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54870cb",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Original data\n",
    "original = customer_data['avg_order_value'].values\n",
    "\n",
    "# Transformations\n",
    "log_transform = np.log1p(original)  # log(1+x) to handle zeros\n",
    "sqrt_transform = np.sqrt(original)\n",
    "boxcox_transform, lambda_param = stats.boxcox(original + 1)  # Add 1 to handle zeros\n",
    "\n",
    "# Test normality for each\n",
    "transformations = {\n",
    "    'Original': original,\n",
    "    'Log': log_transform,\n",
    "    'Square Root': sqrt_transform,\n",
    "    'Box-Cox': boxcox_transform\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, data in transformations.items():\n",
    "    stat, p_value = stats.shapiro(data[:5000])\n",
    "    skew = stats.skew(data)\n",
    "    kurt = stats.kurtosis(data)\n",
    "    results.append({\n",
    "        'Transformation': name,\n",
    "        'Shapiro p-value': p_value,\n",
    "        'Skewness': skew,\n",
    "        'Kurtosis': kurt,\n",
    "        'Normal?': 'Yes' if p_value > 0.05 else 'No'\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Transformation Comparison:\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(f\"\\nBox-Cox lambda: {lambda_param:.3f}\")\n",
    "\n",
    "# Visualize best transformation\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, (name, data) in enumerate(transformations.items()):\n",
    "    axes[i].hist(data, bins=30, density=True, alpha=0.7)\n",
    "    mu, std = data.mean(), data.std()\n",
    "    x = np.linspace(data.min(), data.max(), 100)\n",
    "    axes[i].plot(x, stats.norm.pdf(x, mu, std), 'r-', linewidth=2)\n",
    "    axes[i].set_title(f'{name}\\nSkew: {stats.skew(data):.2f}')\n",
    "    axes[i].set_xlabel('Value')\n",
    "\n",
    "plt.suptitle('Transformation Effects on Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✅ Best transformation: {results_df.loc[results_df['Shapiro p-value'].idxmax(), 'Transformation']}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93813ac",
   "metadata": {},
   "source": [
    "## 5. Comprehensive EDA Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de98963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalEDA:\n",
    "    \"\"\"Complete statistical EDA framework\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        self.categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    def basic_info(self):\n",
    "        \"\"\"Dataset overview\"\"\"\n",
    "        print(\"📊 DATASET OVERVIEW\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Shape: {self.df.shape[0]} rows × {self.df.shape[1]} columns\")\n",
    "        print(f\"Memory usage: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        print(f\"\\nColumn Types:\")\n",
    "        print(self.df.dtypes.value_counts())\n",
    "        print(f\"\\nMissing Values:\")\n",
    "        missing = self.df.isnull().sum()\n",
    "        if missing.sum() > 0:\n",
    "            print(missing[missing > 0].sort_values(ascending=False))\n",
    "        else:\n",
    "            print(\"No missing values\")\n",
    "    \n",
    "    def univariate_analysis(self):\n",
    "        \"\"\"Analyze each variable individually\"\"\"\n",
    "        print(\"\\n📈 UNIVARIATE ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for col in self.numeric_cols:\n",
    "            print(f\"\\n{col}:\")\n",
    "            data = self.df[col].dropna()\n",
    "            \n",
    "            # Key statistics\n",
    "            print(f\"  Mean: {data.mean():.2f}, Median: {data.median():.2f}\")\n",
    "            print(f\"  Std: {data.std():.2f}, IQR: {data.quantile(0.75) - data.quantile(0.25):.2f}\")\n",
    "            print(f\"  Skewness: {data.skew():.2f}, Kurtosis: {data.kurtosis():.2f}\")\n",
    "            \n",
    "            # Normality test\n",
    "            if len(data) > 20:\n",
    "                _, p_value = stats.shapiro(data[:5000])\n",
    "                print(f\"  Normal: {'Yes' if p_value > 0.05 else 'No'} (p={p_value:.4f})\")\n",
    "    \n",
    "    def bivariate_analysis(self):\n",
    "        \"\"\"Analyze relationships between variables\"\"\"\n",
    "        print(\"\\n🔗 BIVARIATE ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Find strong correlations\n",
    "        corr_matrix = self.df[self.numeric_cols].corr()\n",
    "        strong_corr = []\n",
    "        \n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                if abs(corr_matrix.iloc[i, j]) > 0.5:\n",
    "                    strong_corr.append((\n",
    "                        corr_matrix.columns[i],\n",
    "                        corr_matrix.columns[j],\n",
    "                        corr_matrix.iloc[i, j]\n",
    "                    ))\n",
    "        \n",
    "        if strong_corr:\n",
    "            print(\"Strong correlations (|r| > 0.5):\")\n",
    "            for var1, var2, corr in sorted(strong_corr, key=lambda x: abs(x[2]), reverse=True):\n",
    "                print(f\"  {var1} ↔ {var2}: {corr:.3f}\")\n",
    "        else:\n",
    "            print(\"No strong correlations found\")\n",
    "    \n",
    "    def outlier_summary(self):\n",
    "        \"\"\"Summarize outliers across all variables\"\"\"\n",
    "        print(\"\\n⚠️ OUTLIER SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for col in self.numeric_cols:\n",
    "            data = self.df[col].dropna()\n",
    "            Q1 = data.quantile(0.25)\n",
    "            Q3 = data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = data[(data < Q1 - 1.5*IQR) | (data > Q3 + 1.5*IQR)]\n",
    "            \n",
    "            if len(outliers) > 0:\n",
    "                print(f\"{col}: {len(outliers)} outliers ({len(outliers)/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate complete EDA report\"\"\"\n",
    "        self.basic_info()\n",
    "        self.univariate_analysis()\n",
    "        self.bivariate_analysis()\n",
    "        self.outlier_summary()\n",
    "\n",
    "# Run comprehensive EDA\n",
    "eda = StatisticalEDA(customer_data)\n",
    "eda.generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de98963a1",
   "metadata": {},
   "source": [
    "**Exercise 4 – Custom EDA Function (hard)**  \n",
    "Create a function that automatically detects and reports data quality issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n",
    "# Create data_quality_check() function that:\n",
    "# 1. Checks for duplicates\n",
    "# 2. Identifies constant columns\n",
    "# 3. Finds high-cardinality categoricals\n",
    "# 4. Detects potential data leakage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cecfa82",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "def data_quality_check(df):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    print(\"🔍 DATA QUALITY CHECK\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # 1. Check for duplicates\n",
    "    n_duplicates = df.duplicated().sum()\n",
    "    if n_duplicates > 0:\n",
    "        issues.append(f\"⚠️ {n_duplicates} duplicate rows found\")\n",
    "        print(f\"Duplicate rows: {n_duplicates} ({n_duplicates/len(df)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"✅ No duplicate rows\")\n",
    "    \n",
    "    # 2. Identify constant columns\n",
    "    constant_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() == 1:\n",
    "            constant_cols.append(col)\n",
    "            issues.append(f\"⚠️ Column '{col}' has only one unique value\")\n",
    "    \n",
    "    if constant_cols:\n",
    "        print(f\"\\nConstant columns: {constant_cols}\")\n",
    "    else:\n",
    "        print(\"\\n✅ No constant columns\")\n",
    "    \n",
    "    # 3. High cardinality check\n",
    "    print(\"\\nCardinality Check:\")\n",
    "    for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "        cardinality = df[col].nunique()\n",
    "        cardinality_ratio = cardinality / len(df)\n",
    "        \n",
    "        if cardinality_ratio > 0.95:\n",
    "            issues.append(f\"⚠️ Column '{col}' has very high cardinality ({cardinality_ratio:.1%})\")\n",
    "            print(f\"  {col}: {cardinality} unique values ({cardinality_ratio:.1%} of rows)\")\n",
    "    \n",
    "    # 4. Potential data leakage detection\n",
    "    print(\"\\nData Leakage Check:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        # Check for perfect correlations\n",
    "        for other_col in numeric_cols:\n",
    "            if col != other_col:\n",
    "                corr = df[col].corr(df[other_col])\n",
    "                if abs(corr) > 0.99:\n",
    "                    issues.append(f\"⚠️ Potential leakage: '{col}' and '{other_col}' are perfectly correlated\")\n",
    "                    print(f\"  {col} ↔ {other_col}: correlation = {corr:.3f}\")\n",
    "    \n",
    "    # 5. Missing value patterns\n",
    "    print(\"\\nMissing Value Patterns:\")\n",
    "    missing_cols = df.columns[df.isnull().any()]\n",
    "    if len(missing_cols) > 0:\n",
    "        for col in missing_cols:\n",
    "            missing_pct = df[col].isnull().sum() / len(df) * 100\n",
    "            if missing_pct > 50:\n",
    "                issues.append(f\"⚠️ Column '{col}' has {missing_pct:.1f}% missing values\")\n",
    "            print(f\"  {col}: {missing_pct:.1f}% missing\")\n",
    "    else:\n",
    "        print(\"  No missing values\")\n",
    "    \n",
    "    # 6. Outlier prevalence\n",
    "    print(\"\\nOutlier Prevalence:\")\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]\n",
    "        outlier_pct = len(outliers) / len(df) * 100\n",
    "        \n",
    "        if outlier_pct > 10:\n",
    "            issues.append(f\"⚠️ Column '{col}' has {outlier_pct:.1f}% outliers\")\n",
    "            print(f\"  {col}: {outlier_pct:.1f}% outliers\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"SUMMARY: Found {len(issues)} potential issues\")\n",
    "    if issues:\n",
    "        print(\"\\nIssues to address:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  {issue}\")\n",
    "    else:\n",
    "        print(\"✅ Data quality looks good!\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Run data quality check\n",
    "issues = data_quality_check(customer_data)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88bba8",
   "metadata": {},
   "source": [
    "## 6. Mini-Challenges\n",
    "- **M1 (easy):** Calculate and interpret the coefficient of variation for all numeric columns\n",
    "- **M2 (medium):** Implement a function to detect multimodal distributions\n",
    "- **M3 (hard):** Build an automated EDA report generator with visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f834bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn - try the challenges!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2e9d0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solutions</b></summary>\n",
    "\n",
    "```python\n",
    "# M1 - Coefficient of Variation\n",
    "cv_results = []\n",
    "for col in customer_data.select_dtypes(include=[np.number]).columns:\n",
    "    mean = customer_data[col].mean()\n",
    "    std = customer_data[col].std()\n",
    "    cv = (std / mean) * 100 if mean != 0 else np.inf\n",
    "    cv_results.append({'Column': col, 'CV': cv})\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results).sort_values('CV')\n",
    "print(\"Coefficient of Variation (Relative Variability):\")\n",
    "for _, row in cv_df.iterrows():\n",
    "    interpretation = \"Low\" if row['CV'] < 30 else \"Moderate\" if row['CV'] < 60 else \"High\"\n",
    "    print(f\"{row['Column']:20s}: {row['CV']:6.1f}% ({interpretation} variability)\")\n",
    "\n",
    "# M2 - Multimodal Detection\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def detect_multimodal(data, col_name):\n",
    "    hist, bins = np.histogram(data, bins=50)\n",
    "    hist = hist / hist.max()  # Normalize\n",
    "    \n",
    "    # Find peaks\n",
    "    peaks, properties = find_peaks(hist, height=0.3, distance=5)\n",
    "    \n",
    "    n_modes = len(peaks)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(data, bins=50, alpha=0.7, density=True)\n",
    "    \n",
    "    for peak in peaks:\n",
    "        plt.axvline(bins[peak], color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.title(f'{col_name}: {n_modes} mode(s) detected')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()\n",
    "    \n",
    "    if n_modes > 1:\n",
    "        print(f\"⚠️ {col_name} appears to be multimodal with {n_modes} peaks\")\n",
    "        print(\"This might indicate multiple subpopulations or segments\")\n",
    "    return n_modes\n",
    "\n",
    "detect_multimodal(customer_data['avg_order_value'].values, 'Average Order Value')\n",
    "\n",
    "# M3 - Automated EDA Report\n",
    "def generate_visual_eda_report(df, output_file='eda_report.html'):\n",
    "    from io import StringIO\n",
    "    import base64\n",
    "    from io import BytesIO\n",
    "    \n",
    "    html_report = StringIO()\n",
    "    html_report.write('<html><head><title>EDA Report</title></head><body>')\n",
    "    html_report.write('<h1>Automated EDA Report</h1>')\n",
    "    \n",
    "    # Basic info\n",
    "    html_report.write('<h2>Dataset Overview</h2>')\n",
    "    html_report.write(f'<p>Shape: {df.shape[0]} rows × {df.shape[1]} columns</p>')\n",
    "    \n",
    "    # Statistics table\n",
    "    html_report.write('<h2>Statistical Summary</h2>')\n",
    "    html_report.write(df.describe().to_html())\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation Matrix')\n",
    "    \n",
    "    # Save plot to base64 string\n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "    img_str = base64.b64encode(buffer.read()).decode()\n",
    "    html_report.write(f'<h2>Correlation Matrix</h2>')\n",
    "    html_report.write(f'<img src=\"data:image/png;base64,{img_str}\" />')\n",
    "    plt.close()\n",
    "    \n",
    "    html_report.write('</body></html>')\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(html_report.getvalue())\n",
    "    \n",
    "    print(f\"✅ Report saved to {output_file}\")\n",
    "\n",
    "# Generate report (uncomment to run)\n",
    "# generate_visual_eda_report(customer_data, 'customer_eda.html')\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba28e65",
   "metadata": {},
   "source": [
    "## Wrap-Up & Next Steps\n",
    "✅ You can perform comprehensive statistical summaries  \n",
    "✅ You know multiple methods for outlier detection  \n",
    "✅ You can assess relationships and correlations  \n",
    "✅ You can test distributions and normality  \n",
    "✅ You built a complete EDA framework  \n",
    "\n",
    "**Next:** Sampling Theory and Law of Large Numbers - Understanding the foundations of statistical inference!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
