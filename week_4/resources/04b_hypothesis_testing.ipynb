{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8287c6",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** · *Intermediate AI & Data Science*\n",
    "### Week 04 · Notebook 02 – Hypothesis Testing\n",
    "**Instructor:** Amir Charkhi  |  **Goal:** Make statistical decisions with confidence.\n",
    "\n",
    "> Format: short theory → quick practice → build understanding → mini-challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a35e8bb",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Objectives\n",
    "- Understand null and alternative hypotheses\n",
    "- Master t-tests and p-values\n",
    "- Learn Type I and Type II errors\n",
    "- Apply hypothesis testing to business decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade0238f",
   "metadata": {},
   "source": [
    "## 1. The Hypothesis Testing Framework\n",
    "Converting business questions into statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f18528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fa7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Question: Has our new training program improved sales?\n",
    "# Old average sales per rep: $5000/month\n",
    "# Sample of new trained reps:\n",
    "\n",
    "np.random.seed(42)\n",
    "new_sales = np.random.normal(5200, 800, 30)  # 30 reps after training\n",
    "\n",
    "print(\"Hypothesis Testing Steps:\")\n",
    "print(\"1. H₀ (Null): Training has no effect (μ = $50,000)\")\n",
    "print(\"2. H₁ (Alternative): Training improves sales (μ > $50,000)\")\n",
    "print(\"3. Significance level: α = 0.05\")\n",
    "print(\"4. Run the test...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-sample t-test\n",
    "old_average = 50000\n",
    "t_stat, p_value = stats.ttest_1samp(new_sales, old_average)\n",
    "\n",
    "print(f\"\\nSample mean: ${new_sales.mean():,.2f}\")\n",
    "print(f\"t-statistic: {t_stat:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Since we're testing if it's greater (one-tailed)\n",
    "p_value_one_tailed = p_value / 2\n",
    "print(f\"p-value (one-tailed): {p_value_one_tailed:.4f}\")\n",
    "\n",
    "if p_value_one_tailed < 0.05:\n",
    "    print(\"\\n✅ Reject H₀: Training appears to improve sales!\")\n",
    "else:\n",
    "    print(\"\\n❌ Fail to reject H₀: No significant improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f40a1",
   "metadata": {},
   "source": [
    "**Exercise 1 – Website Speed Test (easy)**  \n",
    "Test if new server reduced page load time from 2.5 seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n",
    "# old_load_time = 2.5  # seconds\n",
    "# new_load_times = [2.1, 2.3, 2.0, 2.2, 1.9, 2.4, 2.1, 2.0, 2.2, 2.1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265e60a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "old_load_time = 2.5  # seconds\n",
    "new_load_times = [2.1, 2.3, 2.0, 2.2, 1.9, 2.4, 2.1, 2.0, 2.2, 2.1]\n",
    "\n",
    "# Test if new times are less than old\n",
    "t_stat, p_value = stats.ttest_1samp(new_load_times, old_load_time)\n",
    "\n",
    "print(f\"Old average: {old_load_time} seconds\")\n",
    "print(f\"New average: {np.mean(new_load_times):.2f} seconds\")\n",
    "print(f\"Improvement: {old_load_time - np.mean(new_load_times):.2f} seconds\")\n",
    "print(f\"\\nt-statistic: {t_stat:.3f}\")\n",
    "print(f\"p-value (two-tailed): {p_value:.4f}\")\n",
    "print(f\"p-value (one-tailed): {p_value/2:.4f}\")\n",
    "\n",
    "if p_value/2 < 0.05:\n",
    "    print(\"\\n✅ Significant improvement in load time!\")\n",
    "else:\n",
    "    print(\"\\n❌ No significant improvement\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c78e3",
   "metadata": {},
   "source": [
    "## 2. Two-Sample t-Tests\n",
    "Comparing two groups (the foundation of A/B testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd91af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two marketing campaigns\n",
    "np.random.seed(42)\n",
    "campaign_a = np.random.normal(125, 20, 50)  # clicks per day\n",
    "campaign_b = np.random.normal(135, 22, 50)\n",
    "\n",
    "print(f\"Campaign A: {campaign_a.mean():.1f} ± {campaign_a.std():.1f} clicks/day\")\n",
    "print(f\"Campaign B: {campaign_b.mean():.1f} ± {campaign_b.std():.1f} clicks/day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948a3661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-sample t-test\n",
    "t_stat, p_value = stats.ttest_ind(campaign_a, campaign_b)\n",
    "\n",
    "print(f\"\\nDifference: {campaign_b.mean() - campaign_a.mean():.1f} clicks/day\")\n",
    "print(f\"t-statistic: {t_stat:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"\\n✅ Significant difference between campaigns\")\n",
    "else:\n",
    "    print(\"\\n❌ No significant difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82fed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Histograms\n",
    "ax1.hist(campaign_a, alpha=0.5, label='Campaign A', color='blue', bins=15)\n",
    "ax1.hist(campaign_b, alpha=0.5, label='Campaign B', color='red', bins=15)\n",
    "ax1.set_xlabel('Clicks per Day')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.legend()\n",
    "ax1.set_title('Distribution Comparison')\n",
    "\n",
    "# Box plots\n",
    "ax2.boxplot([campaign_a, campaign_b], labels=['Campaign A', 'Campaign B'])\n",
    "ax2.set_ylabel('Clicks per Day')\n",
    "ax2.set_title('Box Plot Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93cc6d3",
   "metadata": {},
   "source": [
    "**Exercise 2 – Customer Satisfaction Comparison (medium)**  \n",
    "Compare satisfaction scores between two customer service teams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127850d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n",
    "# team_1 = [8, 7, 9, 8, 7, 8, 9, 8, 7, 8, 9, 8]  # scores out of 10\n",
    "# team_2 = [7, 6, 7, 8, 6, 7, 6, 7, 8, 7, 6, 7]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f25302",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "team_1 = np.array([8, 7, 9, 8, 7, 8, 9, 8, 7, 8, 9, 8])  # scores out of 10\n",
    "team_2 = np.array([7, 6, 7, 8, 6, 7, 6, 7, 8, 7, 6, 7])\n",
    "\n",
    "print(f\"Team 1: {team_1.mean():.2f} ± {team_1.std():.2f}\")\n",
    "print(f\"Team 2: {team_2.mean():.2f} ± {team_2.std():.2f}\")\n",
    "\n",
    "# Two-sample t-test\n",
    "t_stat, p_value = stats.ttest_ind(team_1, team_2)\n",
    "\n",
    "print(f\"\\nDifference: {team_1.mean() - team_2.mean():.2f} points\")\n",
    "print(f\"t-statistic: {t_stat:.3f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Effect size (Cohen's d)\n",
    "pooled_std = np.sqrt((team_1.std()**2 + team_2.std()**2) / 2)\n",
    "cohens_d = (team_1.mean() - team_2.mean()) / pooled_std\n",
    "print(f\"Cohen's d: {cohens_d:.2f} (effect size)\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\n✅ Team 1 significantly outperforms Team 2\")\n",
    "else:\n",
    "    print(f\"\\n❌ No significant difference\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac1848",
   "metadata": {},
   "source": [
    "## 3. Understanding p-values & Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multiple tests to understand p-values\n",
    "np.random.seed(42)\n",
    "n_simulations = 1000\n",
    "p_values = []\n",
    "\n",
    "# Run many tests on random data (no real effect)\n",
    "for _ in range(n_simulations):\n",
    "    group_a = np.random.normal(100, 15, 30)\n",
    "    group_b = np.random.normal(100, 15, 30)  # Same distribution!\n",
    "    _, p = stats.ttest_ind(group_a, group_b)\n",
    "    p_values.append(p)\n",
    "\n",
    "# How many false positives at α = 0.05?\n",
    "false_positives = sum(p < 0.05 for p in p_values)\n",
    "print(f\"False positive rate: {false_positives/n_simulations:.1%}\")\n",
    "print(f\"Expected: 5.0%\")\n",
    "print(f\"\\nThis is Type I error - rejecting H₀ when it's true!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdbd6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize p-value distribution under null hypothesis\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(p_values, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=0.05, color='red', linestyle='--', label='α = 0.05')\n",
    "plt.xlabel('p-value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('p-value Distribution Under Null Hypothesis (Should be Uniform)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c85c5e",
   "metadata": {},
   "source": [
    "## 4. Statistical Power & Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948a3661-2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How sample size affects our ability to detect differences\n",
    "def run_test_with_sample_size(n, effect_size=0.5):\n",
    "    \"\"\"Run t-test with given sample size and effect\"\"\"\n",
    "    np.random.seed(42)\n",
    "    group_a = np.random.normal(100, 15, n)\n",
    "    group_b = np.random.normal(100 + effect_size*15, 15, n)  # Small effect\n",
    "    _, p = stats.ttest_ind(group_a, group_b)\n",
    "    return p < 0.05  # Did we detect the difference?\n",
    "\n",
    "sample_sizes = [10, 20, 50, 100, 200, 500]\n",
    "for n in sample_sizes:\n",
    "    detected = run_test_with_sample_size(n)\n",
    "    print(f\"n={n:3d}: {'✅ Detected' if detected else '❌ Missed'} the difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b637638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power calculation: How many samples do we need?\n",
    "from statsmodels.stats.power import ttest_power\n",
    "\n",
    "# For 80% power to detect medium effect (d=0.5)\n",
    "effect_size = 0.5\n",
    "alpha = 0.05\n",
    "power = 0.8\n",
    "\n",
    "# Calculate required sample size\n",
    "from statsmodels.stats.power import tt_ind_solve_power\n",
    "n_required = tt_ind_solve_power(\n",
    "    effect_size=effect_size,\n",
    "    alpha=alpha,\n",
    "    power=power,\n",
    "    ratio=1  # Equal group sizes\n",
    ")\n",
    "\n",
    "print(f\"Required sample size per group: {n_required:.0f}\")\n",
    "print(f\"Total participants needed: {2*n_required:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f39466",
   "metadata": {},
   "source": [
    "**Exercise 3 – Power Analysis (medium)**  \n",
    "Calculate sample size needed to detect 10% improvement in conversion rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n",
    "# Current conversion rate: 5%\n",
    "# Want to detect: 5.5% (10% relative improvement)\n",
    "# Calculate required sample size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54870cb",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "from statsmodels.stats.power import zt_ind_solve_power\n",
    "\n",
    "# Current and target conversion rates\n",
    "p1 = 0.05  # 5%\n",
    "p2 = 0.055  # 5.5% (10% relative improvement)\n",
    "\n",
    "# Calculate effect size for proportions\n",
    "effect = proportion_effectsize(p1, p2)\n",
    "print(f\"Effect size: {effect:.3f}\")\n",
    "\n",
    "# Calculate required sample size\n",
    "n_required = zt_ind_solve_power(\n",
    "    effect_size=effect,\n",
    "    alpha=0.05,\n",
    "    power=0.8,\n",
    "    ratio=1\n",
    ")\n",
    "\n",
    "print(f\"\\nRequired sample size per group: {n_required:.0f}\")\n",
    "print(f\"Total visitors needed: {2*n_required:.0f}\")\n",
    "\n",
    "# Reality check\n",
    "if n_required > 10000:\n",
    "    print(\"\\n⚠️ Large sample needed! Consider:\")\n",
    "    print(\"- Running test longer\")\n",
    "    print(\"- Accepting lower power\")\n",
    "    print(\"- Looking for larger effects\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93813ac",
   "metadata": {},
   "source": [
    "## 5. Multiple Testing Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The multiple comparisons problem\n",
    "np.random.seed(42)\n",
    "\n",
    "# Test 20 different metrics (all from same distribution)\n",
    "n_tests = 20\n",
    "p_values = []\n",
    "\n",
    "for i in range(n_tests):\n",
    "    control = np.random.normal(100, 15, 100)\n",
    "    treatment = np.random.normal(100, 15, 100)  # No real difference!\n",
    "    _, p = stats.ttest_ind(control, treatment)\n",
    "    p_values.append(p)\n",
    "\n",
    "# Without correction\n",
    "significant_raw = sum(p < 0.05 for p in p_values)\n",
    "print(f\"Without correction: {significant_raw}/{n_tests} 'significant' results\")\n",
    "print(f\"That's {significant_raw/n_tests:.0%} false positives!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de98963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonferroni correction\n",
    "bonferroni_alpha = 0.05 / n_tests\n",
    "significant_bonferroni = sum(p < bonferroni_alpha for p in p_values)\n",
    "\n",
    "print(f\"Bonferroni correction:\")\n",
    "print(f\"Adjusted α = {bonferroni_alpha:.4f}\")\n",
    "print(f\"Significant results: {significant_bonferroni}/{n_tests}\")\n",
    "\n",
    "# Benjamini-Hochberg (less conservative)\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "reject, p_adjusted, _, _ = multipletests(p_values, method='fdr_bh')\n",
    "print(f\"\\nBenjamini-Hochberg (FDR):\")\n",
    "print(f\"Significant results: {sum(reject)}/{n_tests}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de98963a1",
   "metadata": {},
   "source": [
    "**Exercise 4 – A/B Test Analysis (hard)**  \n",
    "Analyze results from an A/B test with multiple metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n",
    "# Metrics: clicks, time_on_site, bounce_rate, conversion\n",
    "# control_data = {\n",
    "#     'clicks': [120, 115, 125, 110, 130, 122, 118, 124],\n",
    "#     'time_on_site': [45, 52, 48, 51, 47, 50, 49, 46],\n",
    "#     'bounce_rate': [0.35, 0.32, 0.38, 0.33, 0.36, 0.34, 0.37, 0.35],\n",
    "#     'conversion': [0.045, 0.042, 0.048, 0.041, 0.046, 0.044, 0.043, 0.045]\n",
    "# }\n",
    "# treatment_data = {\n",
    "#     'clicks': [135, 128, 140, 132, 138, 136, 134, 137],\n",
    "#     'time_on_site': [52, 58, 55, 57, 54, 56, 53, 55],\n",
    "#     'bounce_rate': [0.28, 0.25, 0.30, 0.27, 0.29, 0.26, 0.28, 0.27],\n",
    "#     'conversion': [0.052, 0.055, 0.058, 0.054, 0.056, 0.053, 0.057, 0.055]\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cecfa82",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "control_data = {\n",
    "    'clicks': [120, 115, 125, 110, 130, 122, 118, 124],\n",
    "    'time_on_site': [45, 52, 48, 51, 47, 50, 49, 46],\n",
    "    'bounce_rate': [0.35, 0.32, 0.38, 0.33, 0.36, 0.34, 0.37, 0.35],\n",
    "    'conversion': [0.045, 0.042, 0.048, 0.041, 0.046, 0.044, 0.043, 0.045]\n",
    "}\n",
    "treatment_data = {\n",
    "    'clicks': [135, 128, 140, 132, 138, 136, 134, 137],\n",
    "    'time_on_site': [52, 58, 55, 57, 54, 56, 53, 55],\n",
    "    'bounce_rate': [0.28, 0.25, 0.30, 0.27, 0.29, 0.26, 0.28, 0.27],\n",
    "    'conversion': [0.052, 0.055, 0.058, 0.054, 0.056, 0.053, 0.057, 0.055]\n",
    "}\n",
    "\n",
    "# Run tests for each metric\n",
    "results = []\n",
    "for metric in control_data.keys():\n",
    "    control = np.array(control_data[metric])\n",
    "    treatment = np.array(treatment_data[metric])\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_ind(control, treatment)\n",
    "    \n",
    "    # Calculate percentage change\n",
    "    pct_change = (treatment.mean() - control.mean()) / control.mean() * 100\n",
    "    \n",
    "    results.append({\n",
    "        'metric': metric,\n",
    "        'control_mean': control.mean(),\n",
    "        'treatment_mean': treatment.mean(),\n",
    "        'pct_change': pct_change,\n",
    "        'p_value': p_value\n",
    "    })\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "results_df['p_value_adjusted'] = results_df['p_value'] * len(results_df)\n",
    "results_df['significant'] = results_df['p_value_adjusted'] < 0.05\n",
    "\n",
    "print(\"A/B Test Results Summary:\")\n",
    "print(\"=\"*60)\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"\\n{row['metric'].upper()}:\")\n",
    "    print(f\"  Control: {row['control_mean']:.3f}\")\n",
    "    print(f\"  Treatment: {row['treatment_mean']:.3f}\")\n",
    "    print(f\"  Change: {row['pct_change']:+.1f}%\")\n",
    "    print(f\"  p-value: {row['p_value']:.4f}\")\n",
    "    print(f\"  Adjusted p-value: {row['p_value_adjusted']:.4f}\")\n",
    "    print(f\"  Significant: {'✅ Yes' if row['significant'] else '❌ No'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATION:\")\n",
    "if results_df['significant'].any():\n",
    "    print(\"✅ Implement the treatment! Significant improvements found.\")\n",
    "else:\n",
    "    print(\"⚠️ No significant improvements after correction. Need more data.\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88bba8",
   "metadata": {},
   "source": [
    "## 6. Mini-Challenges\n",
    "- **M1 (easy):** Test if coin is fair (60 heads in 100 flips)\n",
    "- **M2 (medium):** Paired t-test for before/after weight loss program\n",
    "- **M3 (hard):** Design an A/B test for 3 groups (ANOVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f834bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn - try the challenges!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2e9d0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solutions</b></summary>\n",
    "\n",
    "```python\n",
    "# M1 - Coin fairness test\n",
    "from scipy.stats import binom_test\n",
    "heads = 60\n",
    "flips = 100\n",
    "p_value = binom_test(heads, flips, p=0.5)\n",
    "print(f\"Observed: {heads}/{flips} heads ({heads/flips:.0%})\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"✅ Coin appears biased!\")\n",
    "else:\n",
    "    print(\"❌ Cannot conclude coin is biased\")\n",
    "\n",
    "# M2 - Paired t-test\n",
    "before = [85, 90, 78, 92, 87, 83, 88, 91, 79, 86]\n",
    "after = [82, 87, 76, 88, 85, 80, 85, 87, 77, 83]\n",
    "t_stat, p_value = stats.ttest_rel(before, after)\n",
    "weight_loss = np.mean(np.array(before) - np.array(after))\n",
    "print(f\"\\nAverage weight loss: {weight_loss:.1f} kg\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"✅ Significant weight loss!\")\n",
    "\n",
    "# M3 - ANOVA for 3 groups\n",
    "control = [100, 102, 98, 105, 103, 99, 101, 104]\n",
    "variant_a = [108, 110, 107, 112, 109, 111, 108, 110]\n",
    "variant_b = [115, 118, 116, 120, 117, 119, 115, 118]\n",
    "\n",
    "f_stat, p_value = stats.f_oneway(control, variant_a, variant_b)\n",
    "print(f\"\\nF-statistic: {f_stat:.3f}\")\n",
    "print(f\"p-value: {p_value:.6f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"✅ Significant difference between groups!\")\n",
    "    print(\"Run post-hoc tests to find which groups differ\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba28e65",
   "metadata": {},
   "source": [
    "## Wrap-Up & Next Steps\n",
    "✅ You can formulate null and alternative hypotheses  \n",
    "✅ You understand p-values and statistical significance  \n",
    "✅ You know about Type I/II errors and power analysis  \n",
    "✅ You can handle multiple comparisons  \n",
    "\n",
    "**Next:** A/B Testing Framework - Putting it all together for real experiments!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
