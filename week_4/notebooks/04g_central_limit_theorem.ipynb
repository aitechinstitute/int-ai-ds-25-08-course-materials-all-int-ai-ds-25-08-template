{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 04 ¬∑ Notebook 07 ‚Äî Central Limit Theorem and Statistical Foundations\n",
    "**Instructor:** Amir Charkhi  |  **Goal:** Master the most beautiful theorem in statistics and unlock the power of inference.\n",
    "\n",
    "> Format: short theory ‚Üí quick practice ‚Üí build understanding ‚Üí mini-challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Objectives\n",
    "- Understand and apply the Central Limit Theorem (the magic behind statistics!)\n",
    "- Explain the Law of Large Numbers and why bigger is better\n",
    "- Use normal approximation to solve real business problems\n",
    "- Understand confidence intervals and what they really mean\n",
    "- Apply these concepts to make data-driven business decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Central Limit Theorem: Pure Magic\n",
    "**The most important theorem in statistics**: No matter what your data looks like, sample means become beautifully normal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set style for nice plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (8, 4)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Central Limit Theorem in plain English\n",
    "clt_facts = {\n",
    "    \"üéØ The Promise\": \"Sample means become normal, regardless of original distribution\",\n",
    "    \"üìä What You Need\": \"Many samples of the same size (usually n ‚â• 30)\",\n",
    "    \"üîÆ The Magic\": \"Works for ANY distribution - uniform, skewed, bimodal, anything!\",\n",
    "    \"üìê The Math\": \"Mean of sample means = population mean (Œº)\",\n",
    "    \"üìè The Spread\": \"Standard error = population std / ‚àön\",\n",
    "    \"‚ö° The Power\": \"Enables statistical inference and confidence intervals\"\n",
    "}\n",
    "\n",
    "print(\"üé™ The Central Limit Theorem: Statistics' Greatest Show!\")\n",
    "for key, value in clt_facts.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "    \n",
    "print(\"\\nüí´ This theorem is why we can make big conclusions from small samples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CLT in Action: The Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wildly different population distributions\n",
    "np.random.seed(42)\n",
    "n_population = 10000\n",
    "\n",
    "# 1. Uniform (flat as a pancake)\n",
    "uniform_pop = np.random.uniform(0, 10, n_population)\n",
    "\n",
    "# 2. Exponential (extremely right-skewed)\n",
    "exponential_pop = np.random.exponential(2, n_population)\n",
    "\n",
    "# 3. Bimodal (two humps)\n",
    "bimodal_pop = np.concatenate([\n",
    "    np.random.normal(3, 0.8, n_population//2),\n",
    "    np.random.normal(8, 0.8, n_population//2)\n",
    "])\n",
    "\n",
    "populations = {\n",
    "    'Uniform': uniform_pop,\n",
    "    'Exponential': exponential_pop,\n",
    "    'Bimodal': bimodal_pop\n",
    "}\n",
    "\n",
    "print(\"üé≠ Our Test Populations:\")\n",
    "for name, pop in populations.items():\n",
    "    print(f\"   {name}: mean={np.mean(pop):.2f}, std={np.std(pop):.2f}\")\n",
    "    \n",
    "print(\"\\nThese look nothing like normal distributions - let's see the magic!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the original populations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "\n",
    "for i, (name, pop) in enumerate(populations.items()):\n",
    "    axes[i].hist(pop, bins=50, density=True, alpha=0.7, color=colors[i], edgecolor='black')\n",
    "    axes[i].set_title(f'{name} Population\\n(Definitely NOT Normal!)')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].axvline(np.mean(pop), color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Mean: {np.mean(pop):.1f}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üé® Original populations: weird shapes, different patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The Magical Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to demonstrate CLT\n",
    "def demonstrate_clt(population, sample_size, num_samples=1000):\n",
    "    \"\"\"Take many samples and return their means\"\"\"\n",
    "    sample_means = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Take a random sample\n",
    "        sample = np.random.choice(population, size=sample_size, replace=True)\n",
    "        # Calculate and store the sample mean\n",
    "        sample_means.append(np.mean(sample))\n",
    "    \n",
    "    return np.array(sample_means)\n",
    "\n",
    "# Test with exponential distribution (most skewed)\n",
    "sample_sizes = [5, 15, 30, 100, 500, 1000]\n",
    "pop = exponential_pop\n",
    "pop_mean = np.mean(pop)\n",
    "pop_std = np.std(pop)\n",
    "\n",
    "print(f\"üß™ CLT Experiment with Exponential Distribution:\")\n",
    "print(f\"   Population: mean={pop_mean:.2f}, std={pop_std:.2f}\")\n",
    "print(f\"\\nüìä Sample Size Effect:\")\n",
    "print(\"Sample Size | Mean of Means | Std of Means | Theoretical SE\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for n in sample_sizes:\n",
    "    sample_means = demonstrate_clt(pop, n)\n",
    "    mean_of_means = np.mean(sample_means)\n",
    "    std_of_means = np.std(sample_means, ddof=1)\n",
    "    theoretical_se = pop_std / np.sqrt(n)\n",
    "    \n",
    "    print(f\"    {n:2d}      |     {mean_of_means:5.2f}     |    {std_of_means:5.2f}     |     {theoretical_se:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the CLT magic\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, n in enumerate(sample_sizes):\n",
    "    sample_means = demonstrate_clt(exponential_pop, n)\n",
    "    \n",
    "    axes[i].hist(sample_means, bins=30, density=True, alpha=0.7, \n",
    "                color=f'C{i}', edgecolor='black')\n",
    "    \n",
    "    # Overlay normal distribution for comparison\n",
    "    x = np.linspace(sample_means.min(), sample_means.max(), 100)\n",
    "    normal_curve = stats.norm.pdf(x, np.mean(sample_means), np.std(sample_means, ddof=1))\n",
    "    axes[i].plot(x, normal_curve, 'red', linewidth=2, label='Normal Fit')\n",
    "    \n",
    "    axes[i].set_title(f'Sample Size = {n}\\n(Getting More Normal!)')\n",
    "    axes[i].set_xlabel('Sample Mean')\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('üé™ Central Limit Theorem: The Magic Happens!', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚ú® Notice: As sample size increases, the distribution becomes more normal!\")\n",
    "print(\"üéØ By n=30, it's beautifully normal - regardless of the original shape!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 ‚Äî CLT Verification (easy)**  \n",
    "Test the CLT with a bimodal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn - test CLT with the bimodal population\n",
    "# Use sample size of 50 and take 1000 samples\n",
    "# Check if the sample means are normally distributed\n",
    "\n",
    "sample_size = 50\n",
    "num_samples = 1000\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Test CLT with bimodal distribution\n",
    "bimodal_sample_means = demonstrate_clt(bimodal_pop, sample_size, num_samples)\n",
    "\n",
    "print(f\"üé≠ CLT Test with Bimodal Distribution:\")\n",
    "print(f\"   Original population mean: {np.mean(bimodal_pop):.2f}\")\n",
    "print(f\"   Mean of sample means: {np.mean(bimodal_sample_means):.2f}\")\n",
    "print(f\"   Standard error: {np.std(bimodal_sample_means, ddof=1):.2f}\")\n",
    "print(f\"   Theoretical SE: {np.std(bimodal_pop)/np.sqrt(sample_size):.2f}\")\n",
    "\n",
    "# Test for normality\n",
    "stat, p_value = stats.shapiro(bimodal_sample_means[:5000])  # Shapiro-Wilk test\n",
    "print(f\"\\nüî¨ Normality test p-value: {p_value:.6f}\")\n",
    "if p_value > 0.05:\n",
    "    print(\"‚úÖ Sample means are normally distributed!\")\n",
    "else:\n",
    "    print(\"‚ùå Sample means are not quite normal yet\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(bimodal_pop, bins=50, density=True, alpha=0.7, color='lightgreen')\n",
    "plt.title('Original Bimodal Population\\n(Two Humps)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(bimodal_sample_means, bins=30, density=True, alpha=0.7, color='orange')\n",
    "# Add normal curve\n",
    "x = np.linspace(bimodal_sample_means.min(), bimodal_sample_means.max(), 100)\n",
    "normal_curve = stats.norm.pdf(x, np.mean(bimodal_sample_means), \n",
    "                             np.std(bimodal_sample_means, ddof=1))\n",
    "plt.plot(x, normal_curve, 'red', linewidth=2, label='Normal Fit')\n",
    "plt.title('Sample Means Distribution\\n(Beautiful Bell Curve!)')\n",
    "plt.xlabel('Sample Mean')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üé™ Even with a weird bimodal population, CLT creates normality!\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Law of Large Numbers: The Convergence Story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The Coin Flip Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Law of Large Numbers with coin flips\n",
    "def coin_flip_experiment(max_flips=10000):\n",
    "    \"\"\"Show how proportion of heads converges to 0.5\"\"\"\n",
    "    # Simulate coin flips (1 = heads, 0 = tails)\n",
    "    flips = np.random.binomial(1, 0.5, max_flips)\n",
    "    \n",
    "    # Calculate running proportion of heads\n",
    "    cumulative_heads = np.cumsum(flips)\n",
    "    flip_numbers = np.arange(1, max_flips + 1)\n",
    "    running_proportion = cumulative_heads / flip_numbers\n",
    "    \n",
    "    return flip_numbers, running_proportion, flips\n",
    "\n",
    "# Run the experiment\n",
    "flip_numbers, proportions, flips = coin_flip_experiment()\n",
    "\n",
    "# Show convergence at different points\n",
    "checkpoints = [10, 100, 1000, 10000]\n",
    "print(\"ü™ô Law of Large Numbers - Coin Flip Experiment:\")\n",
    "print(\"Number of Flips | Proportion of Heads | Distance from 0.5\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for n in checkpoints:\n",
    "    prop = proportions[n-1]\n",
    "    distance = abs(prop - 0.5)\n",
    "    print(f\"     {n:5d}      |       {prop:.4f}       |     {distance:.4f}\")\n",
    "    \n",
    "print(f\"\\nüéØ As flips increase, proportion gets closer to true probability (0.5)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Law of Large Numbers\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(flip_numbers, proportions, 'b-', alpha=0.7, linewidth=1)\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', linewidth=2, label='True Probability (0.5)')\n",
    "plt.xlabel('Number of Flips')\n",
    "plt.ylabel('Proportion of Heads')\n",
    "plt.title('ü™ô Law of Large Numbers:\\nConvergence to Truth')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0.3, 0.7)\n",
    "\n",
    "# Zoom in on later part\n",
    "plt.subplot(1, 2, 2)\n",
    "start_idx = 1000\n",
    "plt.plot(flip_numbers[start_idx:], proportions[start_idx:], 'b-', alpha=0.7, linewidth=1)\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', linewidth=2, label='True Probability')\n",
    "plt.xlabel('Number of Flips')\n",
    "plt.ylabel('Proportion of Heads')\n",
    "plt.title('üîç Zoomed View:\\nVery Close to 0.5!')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0.45, 0.55)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà The more data we collect, the closer we get to the truth!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Business Applications: Where the Magic Pays Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Customer Service Response Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real business scenario: call center response times\n",
    "np.random.seed(123)\n",
    "\n",
    "# Population: actual response times (right-skewed)\n",
    "true_mean_response = 4.8  # minutes \n",
    "population_response_times = np.random.exponential(true_mean_response, 50000)\n",
    "\n",
    "print(f\"üìû Call Center Response Time Analysis:\")\n",
    "print(f\"   True population mean: {np.mean(population_response_times):.2f} minutes\")\n",
    "print(f\"   Population std: {np.std(population_response_times):.2f} minutes\")\n",
    "print(f\"   Distribution: Right-skewed (exponential)\")\n",
    "\n",
    "# Daily sampling: measure 50 calls per day for 100 days\n",
    "daily_sample_size = 50\n",
    "num_days = 100\n",
    "\n",
    "daily_averages = []\n",
    "for day in range(num_days):\n",
    "    daily_sample = np.random.choice(population_response_times, \n",
    "                                  size=daily_sample_size, replace=True)\n",
    "    daily_averages.append(np.mean(daily_sample))\n",
    "\n",
    "daily_averages = np.array(daily_averages)\n",
    "\n",
    "print(f\"\\nüìä Daily Sampling Results ({num_days} days, {daily_sample_size} calls/day):\")\n",
    "print(f\"   Mean of daily averages: {np.mean(daily_averages):.2f} minutes\")\n",
    "print(f\"   Std of daily averages: {np.std(daily_averages, ddof=1):.2f} minutes\")\n",
    "print(f\"   Theoretical SE: {np.std(population_response_times)/np.sqrt(daily_sample_size):.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the business transformation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original population (messy reality)\n",
    "axes[0].hist(population_response_times, bins=50, density=True, alpha=0.7, \n",
    "            color='lightcoral', edgecolor='black')\n",
    "axes[0].set_title('üìû Individual Response Times\\n(Highly Variable & Skewed)')\n",
    "axes[0].set_xlabel('Response Time (minutes)')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].axvline(np.mean(population_response_times), color='red', linestyle='--', \n",
    "               label=f'Mean: {np.mean(population_response_times):.1f} min')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0, 20)\n",
    "\n",
    "# Daily averages (manageable insights)\n",
    "axes[1].hist(daily_averages, bins=20, density=True, alpha=0.7, \n",
    "            color='lightblue', edgecolor='black')\n",
    "axes[1].set_title('üìä Daily Average Response Times\\n(Predictable & Normal!)')\n",
    "axes[1].set_xlabel('Daily Average Response Time (minutes)')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].axvline(np.mean(daily_averages), color='red', linestyle='--', \n",
    "               label=f'Mean: {np.mean(daily_averages):.1f} min')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üíº Business Impact: CLT transforms chaotic individual data into predictable patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Quality Control Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manufacturing example: product weights\n",
    "target_weight = 500  # grams\n",
    "process_std = 12     # grams\n",
    "batch_size = 25      # items tested per batch\n",
    "\n",
    "# CLT tells us what to expect for batch averages\n",
    "batch_mean_std = process_std / np.sqrt(batch_size)\n",
    "\n",
    "print(f\"‚öñÔ∏è Quality Control Using CLT:\")\n",
    "print(f\"   Target weight: {target_weight}g\")\n",
    "print(f\"   Individual item std: {process_std}g\")\n",
    "print(f\"   Batch size: {batch_size} items\")\n",
    "print(f\"   Standard error of batch means: {batch_mean_std:.2f}g\")\n",
    "\n",
    "# Control limits (99.7% of batch means should fall within ¬±3 SE)\n",
    "lower_limit = target_weight - 3 * batch_mean_std\n",
    "upper_limit = target_weight + 3 * batch_mean_std\n",
    "\n",
    "print(f\"\\nüö® Control Limits for Batch Averages:\")\n",
    "print(f\"   Lower limit: {lower_limit:.1f}g\")\n",
    "print(f\"   Upper limit: {upper_limit:.1f}g\")\n",
    "print(f\"   Any batch average outside these limits signals a problem!\")\n",
    "\n",
    "# Probability calculations\n",
    "prob_in_control = 0.997  # 99.7% within 3 standard errors\n",
    "prob_false_alarm = 1 - prob_in_control\n",
    "\n",
    "print(f\"\\nüìä Expected Performance:\")\n",
    "print(f\"   Batches within limits: {prob_in_control:.1%}\")\n",
    "print(f\"   False alarm rate: {prob_false_alarm:.1%}\")\n",
    "print(f\"   Out of 1000 batches, expect ~{prob_false_alarm * 1000:.0f} false alarms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 ‚Äî A/B Testing Power (medium)**  \n",
    "Use CLT to design an A/B test for website conversion rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B test scenario: website conversion optimization\n",
    "# Version A: current site (5% conversion rate)\n",
    "# Version B: new design (hoping for improvement)\n",
    "\n",
    "current_conversion_rate = 0.05  # 5%\n",
    "visitors_per_version = 1000\n",
    "\n",
    "# Your tasks:\n",
    "# 1. Simulate conversions for both versions (assume B is actually 7%)\n",
    "# 2. Calculate the difference in conversion rates\n",
    "# 3. Use CLT to estimate if the difference is significant\n",
    "# 4. Determine if we can detect a 2% improvement\n",
    "\n",
    "true_rate_a = 0.05\n",
    "true_rate_b = 0.07  # Unknown to us in real scenario\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Task 1: Simulate A/B test results\n",
    "np.random.seed(456)\n",
    "conversions_a = np.random.binomial(visitors_per_version, true_rate_a)\n",
    "conversions_b = np.random.binomial(visitors_per_version, true_rate_b)\n",
    "\n",
    "rate_a = conversions_a / visitors_per_version\n",
    "rate_b = conversions_b / visitors_per_version\n",
    "\n",
    "print(f\"üåê A/B Test Results:\")\n",
    "print(f\"   Version A: {conversions_a}/{visitors_per_version} = {rate_a:.3f} ({rate_a:.1%})\")\n",
    "print(f\"   Version B: {conversions_b}/{visitors_per_version} = {rate_b:.3f} ({rate_b:.1%})\")\n",
    "\n",
    "# Task 2: Calculate difference\n",
    "observed_difference = rate_b - rate_a\n",
    "true_difference = true_rate_b - true_rate_a\n",
    "print(f\"   Observed difference: {observed_difference:.3f} ({observed_difference:.1%})\")\n",
    "print(f\"   True difference: {true_difference:.3f} ({true_difference:.1%})\")\n",
    "\n",
    "# Task 3: Use CLT for significance testing\n",
    "# Standard error for proportion difference\n",
    "se_a = np.sqrt(rate_a * (1 - rate_a) / visitors_per_version)\n",
    "se_b = np.sqrt(rate_b * (1 - rate_b) / visitors_per_version)\n",
    "se_difference = np.sqrt(se_a**2 + se_b**2)\n",
    "\n",
    "# Z-score for the observed difference\n",
    "z_score = observed_difference / se_difference\n",
    "p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))  # Two-tailed test\n",
    "\n",
    "print(f\"\\nüìä Statistical Analysis:\")\n",
    "print(f\"   Standard error of difference: {se_difference:.4f}\")\n",
    "print(f\"   Z-score: {z_score:.2f}\")\n",
    "print(f\"   P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"   Result: üéâ Statistically significant! Version B is better.\")\n",
    "else:\n",
    "    print(f\"   Result: üòê Not statistically significant. Need more data.\")\n",
    "\n",
    "# Task 4: Power analysis - can we detect 2% improvement?\n",
    "effect_size = 0.02  # 2% improvement\n",
    "pooled_rate = (true_rate_a + true_rate_b) / 2\n",
    "pooled_se = np.sqrt(2 * pooled_rate * (1 - pooled_rate) / visitors_per_version)\n",
    "\n",
    "# Minimum Z-score needed for significance\n",
    "critical_z = 1.96  # 95% confidence\n",
    "minimum_detectable_effect = critical_z * pooled_se\n",
    "\n",
    "print(f\"\\nüîç Power Analysis:\")\n",
    "print(f\"   Minimum detectable effect: {minimum_detectable_effect:.3f} ({minimum_detectable_effect:.1%})\")\n",
    "print(f\"   Target effect size: {effect_size:.3f} ({effect_size:.1%})\")\n",
    "\n",
    "if effect_size > minimum_detectable_effect:\n",
    "    print(f\"   Power: ‚úÖ Test can detect 2% improvement\")\n",
    "else:\n",
    "    needed_sample = 2 * pooled_rate * (1 - pooled_rate) * (critical_z / effect_size)**2\n",
    "    print(f\"   Power: ‚ùå Test underpowered. Need ~{needed_sample:.0f} visitors per version\")\n",
    "\n",
    "print(f\"\\nüí° CLT enables us to quantify uncertainty and make data-driven decisions!\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confidence Intervals: Quantifying Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Understanding Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence interval demonstration\n",
    "population_data = np.random.normal(100, 15, 10000)  # IQ scores\n",
    "true_mean = np.mean(population_data)\n",
    "sample_size = 50\n",
    "num_intervals = 100\n",
    "\n",
    "# Generate many confidence intervals\n",
    "confidence_intervals = []\n",
    "contains_true_mean = []\n",
    "\n",
    "for i in range(num_intervals):\n",
    "    # Take a sample\n",
    "    sample = np.random.choice(population_data, size=sample_size, replace=True)\n",
    "    sample_mean = np.mean(sample)\n",
    "    \n",
    "    # Calculate 95% confidence interval\n",
    "    standard_error = np.std(sample, ddof=1) / np.sqrt(sample_size)\n",
    "    margin_of_error = 1.96 * standard_error  # 95% CI\n",
    "    \n",
    "    ci_lower = sample_mean - margin_of_error\n",
    "    ci_upper = sample_mean + margin_of_error\n",
    "    \n",
    "    confidence_intervals.append((ci_lower, ci_upper))\n",
    "    contains_true_mean.append(ci_lower <= true_mean <= ci_upper)\n",
    "\n",
    "# Check coverage\n",
    "coverage_rate = np.mean(contains_true_mean)\n",
    "\n",
    "print(f\"üéØ Confidence Interval Experiment:\")\n",
    "print(f\"   True population mean: {true_mean:.2f}\")\n",
    "print(f\"   Number of 95% confidence intervals: {num_intervals}\")\n",
    "print(f\"   Intervals containing true mean: {sum(contains_true_mean)}\")\n",
    "print(f\"   Coverage rate: {coverage_rate:.1%} (should be ~95%)\")\n",
    "print(f\"\\nüí° 95% confidence means 95% of intervals capture the truth!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confidence intervals\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot first 25 confidence intervals\n",
    "display_count = 25\n",
    "for i in range(display_count):\n",
    "    ci_lower, ci_upper = confidence_intervals[i]\n",
    "    color = 'blue' if contains_true_mean[i] else 'red'\n",
    "    alpha = 0.7 if contains_true_mean[i] else 1.0\n",
    "    \n",
    "    plt.plot([ci_lower, ci_upper], [i, i], color=color, linewidth=3, alpha=alpha)\n",
    "    \n",
    "    # Add sample mean point\n",
    "    sample_mean = (ci_lower + ci_upper) / 2\n",
    "    plt.plot(sample_mean, i, 'o', color=color, markersize=6)\n",
    "\n",
    "plt.axvline(true_mean, color='green', linestyle='--', linewidth=3, \n",
    "           label=f'True Population Mean: {true_mean:.1f}')\n",
    "plt.xlabel('IQ Score')\n",
    "plt.ylabel('Sample Number')\n",
    "plt.title('üéØ 95% Confidence Intervals\\n(Blue = captures truth, Red = misses)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(80, 120)\n",
    "plt.show()\n",
    "\n",
    "hit_rate = sum(contains_true_mean[:display_count])\n",
    "print(f\"üìä In this visualization: {hit_rate}/{display_count} intervals capture the truth\")\n",
    "print(f\"üéØ Expected: ~{0.95 * display_count:.0f} out of {display_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Confidence Interval Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical confidence interval calculator\n",
    "def calculate_confidence_interval(data, confidence_level=0.95):\n",
    "    \"\"\"Calculate confidence interval for sample mean\"\"\"\n",
    "    n = len(data)\n",
    "    sample_mean = np.mean(data)\n",
    "    sample_std = np.std(data, ddof=1)\n",
    "    standard_error = sample_std / np.sqrt(n)\n",
    "    \n",
    "    # Z-score for confidence level\n",
    "    z_scores = {0.90: 1.645, 0.95: 1.96, 0.99: 2.576}\n",
    "    z_score = z_scores[confidence_level]\n",
    "    \n",
    "    margin_of_error = z_score * standard_error\n",
    "    ci_lower = sample_mean - margin_of_error\n",
    "    ci_upper = sample_mean + margin_of_error\n",
    "    \n",
    "    return {\n",
    "        'sample_mean': sample_mean,\n",
    "        'standard_error': standard_error,\n",
    "        'margin_of_error': margin_of_error,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'confidence_level': confidence_level\n",
    "    }\n",
    "\n",
    "# Example: Customer satisfaction scores\n",
    "satisfaction_scores = np.random.normal(7.5, 1.2, 150)  # Scale 1-10\n",
    "results = calculate_confidence_interval(satisfaction_scores, 0.95)\n",
    "\n",
    "print(f\"üìä Customer Satisfaction Analysis:\")\n",
    "print(f\"   Sample size: {len(satisfaction_scores)}\")\n",
    "print(f\"   Sample mean: {results['sample_mean']:.2f}\")\n",
    "print(f\"   Standard error: {results['standard_error']:.3f}\")\n",
    "print(f\"   95% Confidence Interval: [{results['ci_lower']:.2f}, {results['ci_upper']:.2f}]\")\n",
    "print(f\"   Margin of Error: ¬±{results['margin_of_error']:.2f}\")\n",
    "\n",
    "print(f\"\\nüíº Business Interpretation:\")\n",
    "print(f\"   We are 95% confident that the true average satisfaction\")\n",
    "print(f\"   is between {results['ci_lower']:.2f} and {results['ci_upper']:.2f}\")\n",
    "print(f\"   This helps us set realistic targets and measure progress!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3 ‚Äî Marketing Campaign ROI (hard)**  \n",
    "Build a complete analysis using CLT and confidence intervals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marketing campaign effectiveness analysis\n",
    "np.random.seed(789)\n",
    "\n",
    "# Before campaign: baseline sales data (30 days)\n",
    "before_mean = 1500  # daily sales\n",
    "before_std = 200\n",
    "before_sales = np.random.normal(before_mean, before_std, 30)\n",
    "\n",
    "# After campaign: improved sales (30 days)\n",
    "after_mean = 1650  # +150 improvement  \n",
    "after_sales = np.random.normal(after_mean, before_std, 30)\n",
    "\n",
    "print(f\"üí∞ Marketing Campaign Analysis:\")\n",
    "print(f\"   Before campaign: {len(before_sales)} days of data\")\n",
    "print(f\"   After campaign: {len(after_sales)} days of data\")\n",
    "print(f\"   Campaign cost: $50,000\")\n",
    "\n",
    "# Your tasks:\n",
    "# 1. Calculate confidence intervals for before and after periods\n",
    "# 2. Estimate the campaign effect with confidence interval\n",
    "# 3. Calculate ROI and its confidence interval\n",
    "# 4. Make a business recommendation\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Task 1: Calculate confidence intervals for both periods\n",
    "before_ci = calculate_confidence_interval(before_sales, 0.95)\n",
    "after_ci = calculate_confidence_interval(after_sales, 0.95)\n",
    "\n",
    "print(f\"\\nüìä Task 1 - Period Analysis:\")\n",
    "print(f\"Before campaign:\")\n",
    "print(f\"   Mean: ${before_ci['sample_mean']:.0f}/day\")\n",
    "print(f\"   95% CI: [${before_ci['ci_lower']:.0f}, ${before_ci['ci_upper']:.0f}]\")\n",
    "print(f\"After campaign:\")\n",
    "print(f\"   Mean: ${after_ci['sample_mean']:.0f}/day\")\n",
    "print(f\"   95% CI: [${after_ci['ci_lower']:.0f}, ${after_ci['ci_upper']:.0f}]\")\n",
    "\n",
    "# Task 2: Campaign effect with confidence interval\n",
    "effect = after_ci['sample_mean'] - before_ci['sample_mean']\n",
    "# Standard error of difference\n",
    "se_difference = np.sqrt(before_ci['standard_error']**2 + after_ci['standard_error']**2)\n",
    "effect_margin = 1.96 * se_difference\n",
    "effect_ci_lower = effect - effect_margin\n",
    "effect_ci_upper = effect + effect_margin\n",
    "\n",
    "print(f\"\\nüéØ Task 2 - Campaign Effect:\")\n",
    "print(f\"   Observed effect: ${effect:.0f}/day increase\")\n",
    "print(f\"   95% CI for effect: [${effect_ci_lower:.0f}, ${effect_ci_upper:.0f}]/day\")\n",
    "\n",
    "if effect_ci_lower > 0:\n",
    "    print(f\"   Result: ‚úÖ Statistically significant improvement!\")\n",
    "else:\n",
    "    print(f\"   Result: ‚ùå Not statistically significant\")\n",
    "\n",
    "# Task 3: ROI analysis\n",
    "campaign_cost = 50000\n",
    "days_in_month = 30\n",
    "monthly_effect = effect * days_in_month\n",
    "monthly_effect_lower = effect_ci_lower * days_in_month\n",
    "monthly_effect_upper = effect_ci_upper * days_in_month\n",
    "\n",
    "# ROI calculation\n",
    "roi = (monthly_effect - campaign_cost) / campaign_cost\n",
    "roi_lower = (monthly_effect_lower - campaign_cost) / campaign_cost\n",
    "roi_upper = (monthly_effect_upper - campaign_cost) / campaign_cost\n",
    "\n",
    "print(f\"\\nüíµ Task 3 - ROI Analysis:\")\n",
    "print(f\"   Monthly revenue increase: ${monthly_effect:,.0f}\")\n",
    "print(f\"   95% CI: [${monthly_effect_lower:,.0f}, ${monthly_effect_upper:,.0f}]\")\n",
    "print(f\"   Campaign cost: ${campaign_cost:,}\")\n",
    "print(f\"   ROI: {roi:.1%}\")\n",
    "print(f\"   95% CI for ROI: [{roi_lower:.1%}, {roi_upper:.1%}]\")\n",
    "\n",
    "# Task 4: Business recommendation\n",
    "print(f\"\\nüè¢ Task 4 - Business Recommendation:\")\n",
    "\n",
    "if roi_lower > 0:\n",
    "    print(f\"   ‚úÖ STRONG RECOMMENDATION: Continue campaign\")\n",
    "    print(f\"   ‚Ä¢ Campaign shows positive ROI with 95% confidence\")\n",
    "    print(f\"   ‚Ä¢ Expected monthly profit: ${monthly_effect - campaign_cost:,.0f}\")\n",
    "    print(f\"   ‚Ä¢ Worst-case scenario still profitable\")\n",
    "elif roi > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  CAUTIOUS RECOMMENDATION: Monitor closely\")\n",
    "    print(f\"   ‚Ä¢ Campaign likely profitable but with uncertainty\")\n",
    "    print(f\"   ‚Ä¢ Consider extending test period for more data\")\n",
    "else:\n",
    "    print(f\"   ‚ùå RECOMMENDATION: Discontinue campaign\")\n",
    "    print(f\"   ‚Ä¢ ROI not convincingly positive\")\n",
    "    print(f\"   ‚Ä¢ Risk of losing money\")\n",
    "\n",
    "print(f\"\\nüí° CLT enabled us to quantify uncertainty and make informed decisions!\")\n",
    "print(f\"üìä Without statistics, we'd just be guessing about campaign effectiveness.\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Common Misconceptions and Pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 The Gambler's Fallacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate why \"balancing out\" is a myth\n",
    "def analyze_streaks(num_sequences=10000):\n",
    "    \"\"\"Analyze what happens after streaks\"\"\"\n",
    "    after_5_heads = []\n",
    "    \n",
    "    for _ in range(num_sequences):\n",
    "        flips = np.random.binomial(1, 0.5, 10)\n",
    "        \n",
    "        # Look for 5 heads in a row\n",
    "        for i in range(5):\n",
    "            if np.sum(flips[i:i+5]) == 5 and i+5 < len(flips):\n",
    "                after_5_heads.append(flips[i+5])\n",
    "    \n",
    "    if after_5_heads:\n",
    "        proportion_heads = np.mean(after_5_heads)\n",
    "        print(f\"üé∞ Gambler's Fallacy Test:\")\n",
    "        print(f\"   After 5 heads in a row, next flip is heads: {proportion_heads:.3f}\")\n",
    "        print(f\"   Expected if fair: 0.500\")\n",
    "        print(f\"   Sample size: {len(after_5_heads)}\")\n",
    "        \n",
    "        if abs(proportion_heads - 0.5) < 0.05:\n",
    "            print(f\"   ‚úÖ Confirmed: Each flip is independent!\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Unusual result - might need more data\")\n",
    "    else:\n",
    "        print(f\"   No 5-head sequences found in {num_sequences} trials\")\n",
    "\n",
    "analyze_streaks()\n",
    "\n",
    "print(f\"\\nüí° Key insight: Past results don't influence future independent events!\")\n",
    "print(f\"üìä Law of Large Numbers works over MANY trials, not through 'balancing'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Confidence Interval Misinterpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common confidence interval misconceptions\n",
    "print(\"‚ö†Ô∏è Common Confidence Interval Mistakes:\")\n",
    "print()\n",
    "\n",
    "misconceptions = [\n",
    "    {\n",
    "        'wrong': \"There's a 95% chance the true parameter is in this interval\",\n",
    "        'right': \"If we repeated this process many times, 95% of intervals would contain the true parameter\",\n",
    "        'explanation': \"The parameter is fixed; the interval is random\"\n",
    "    },\n",
    "    {\n",
    "        'wrong': \"95% of the data falls within the confidence interval\",\n",
    "        'right': \"The confidence interval is for the population mean, not individual data points\",\n",
    "        'explanation': \"CIs are about parameter estimates, not data spread\"\n",
    "    },\n",
    "    {\n",
    "        'wrong': \"A wider interval means we're less confident\",\n",
    "        'right': \"A wider interval reflects more uncertainty in our estimate\",\n",
    "        'explanation': \"Confidence level stays the same; precision changes\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, item in enumerate(misconceptions, 1):\n",
    "    print(f\"{i}. ‚ùå WRONG: {item['wrong']}\")\n",
    "    print(f\"   ‚úÖ RIGHT: {item['right']}\")\n",
    "    print(f\"   üí° Why: {item['explanation']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Mini-Challenges\n",
    "- **M1 (easy):** Build a sample size calculator for different precision requirements\n",
    "- **M2 (medium):** Create a CLT demonstration with multiple weird distributions\n",
    "- **M3 (hard):** Design a complete A/B testing framework with power analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn - try the challenges!\n",
    "# M1: Calculate sample sizes for customer surveys\n",
    "# M2: Test CLT with uniform, exponential, and custom distributions  \n",
    "# M3: Full A/B testing system with statistical power\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solutions</b></summary>\n",
    "\n",
    "```python\n",
    "# M1 - Sample Size Calculator\n",
    "def sample_size_calculator(population_std, desired_margin, confidence_level=0.95):\n",
    "    \"\"\"Calculate required sample size for desired precision\"\"\"\n",
    "    z_scores = {0.90: 1.645, 0.95: 1.96, 0.99: 2.576}\n",
    "    z = z_scores[confidence_level]\n",
    "    \n",
    "    # Formula: n = (z * œÉ / E)¬≤\n",
    "    n = (z * population_std / desired_margin) ** 2\n",
    "    return int(np.ceil(n))\n",
    "\n",
    "print(\"üßÆ M1 - Sample Size Calculator:\")\n",
    "scenarios = [\n",
    "    (\"Customer satisfaction (1-10)\", 2.0, 0.2),\n",
    "    (\"Website load time (seconds)\", 0.5, 0.05),\n",
    "    (\"Order value ($)\", 50, 5),\n",
    "    (\"Employee productivity (0-100)\", 15, 2)\n",
    "]\n",
    "\n",
    "for scenario, std, margin in scenarios:\n",
    "    n_90 = sample_size_calculator(std, margin, 0.90)\n",
    "    n_95 = sample_size_calculator(std, margin, 0.95)\n",
    "    n_99 = sample_size_calculator(std, margin, 0.99)\n",
    "    \n",
    "    print(f\"\\n{scenario}:\")\n",
    "    print(f\"   For ¬±{margin} precision: {n_90} (90%), {n_95} (95%), {n_99} (99%)\")\n",
    "\n",
    "# M2 - CLT Demonstration with Multiple Distributions\n",
    "def test_clt_convergence(distributions, sample_sizes=[5, 15, 30, 100]):\n",
    "    \"\"\"Test CLT with multiple distributions\"\"\"\n",
    "    print(f\"\\nüé™ M2 - CLT Universal Test:\")\n",
    "    \n",
    "    fig, axes = plt.subplots(len(distributions), len(sample_sizes), \n",
    "                            figsize=(16, 4*len(distributions)))\n",
    "    \n",
    "    for i, (name, dist) in enumerate(distributions.items()):\n",
    "        print(f\"\\nTesting {name} distribution:\")\n",
    "        \n",
    "        for j, n in enumerate(sample_sizes):\n",
    "            sample_means = demonstrate_clt(dist, n, 1000)\n",
    "            \n",
    "            # Plot\n",
    "            ax = axes[i, j] if len(distributions) > 1 else axes[j]\n",
    "            ax.hist(sample_means, bins=30, density=True, alpha=0.7, color=f'C{j}')\n",
    "            \n",
    "            # Normal overlay\n",
    "            x = np.linspace(sample_means.min(), sample_means.max(), 100)\n",
    "            normal_fit = stats.norm.pdf(x, np.mean(sample_means), np.std(sample_means))\n",
    "            ax.plot(x, normal_fit, 'red', linewidth=2)\n",
    "            \n",
    "            ax.set_title(f'{name}\\nn={n}')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Normality test\n",
    "            _, p_val = stats.shapiro(sample_means[:5000])\n",
    "            normal_enough = \"‚úÖ\" if p_val > 0.05 else \"‚ùå\"\n",
    "            print(f\"   n={n}: {normal_enough} (p={p_val:.4f})\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test with different distributions\n",
    "test_distributions = {\n",
    "    'Uniform': np.random.uniform(0, 10, 10000),\n",
    "    'Exponential': np.random.exponential(2, 10000)\n",
    "}\n",
    "test_clt_convergence(test_distributions)\n",
    "\n",
    "# M3 - Complete A/B Testing Framework\n",
    "class ABTestFramework:\n",
    "    def __init__(self, alpha=0.05, power=0.8):\n",
    "        self.alpha = alpha\n",
    "        self.power = power\n",
    "        \n",
    "    def sample_size_for_proportions(self, p1, p2, alpha=None, power=None):\n",
    "        \"\"\"Calculate sample size for proportion A/B test\"\"\"\n",
    "        alpha = alpha or self.alpha\n",
    "        power = power or self.power\n",
    "        \n",
    "        # Effect size\n",
    "        effect = abs(p2 - p1)\n",
    "        pooled_p = (p1 + p2) / 2\n",
    "        \n",
    "        # Z-scores\n",
    "        z_alpha = stats.norm.ppf(1 - alpha/2)\n",
    "        z_beta = stats.norm.ppf(power)\n",
    "        \n",
    "        # Sample size calculation\n",
    "        n = (2 * pooled_p * (1 - pooled_p) * (z_alpha + z_beta)**2) / effect**2\n",
    "        return int(np.ceil(n))\n",
    "    \n",
    "    def analyze_test(self, conversions_a, visitors_a, conversions_b, visitors_b):\n",
    "        \"\"\"Analyze A/B test results\"\"\"\n",
    "        rate_a = conversions_a / visitors_a\n",
    "        rate_b = conversions_b / visitors_b\n",
    "        \n",
    "        # Standard errors\n",
    "        se_a = np.sqrt(rate_a * (1 - rate_a) / visitors_a)\n",
    "        se_b = np.sqrt(rate_b * (1 - rate_b) / visitors_b)\n",
    "        se_diff = np.sqrt(se_a**2 + se_b**2)\n",
    "        \n",
    "        # Test statistic\n",
    "        diff = rate_b - rate_a\n",
    "        z_stat = diff / se_diff\n",
    "        p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "        \n",
    "        # Confidence interval for difference\n",
    "        margin = 1.96 * se_diff\n",
    "        ci_lower = diff - margin\n",
    "        ci_upper = diff + margin\n",
    "        \n",
    "        return {\n",
    "            'rate_a': rate_a,\n",
    "            'rate_b': rate_b,\n",
    "            'difference': diff,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'z_stat': z_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < self.alpha\n",
    "        }\n",
    "\n",
    "print(f\"\\nüß™ M3 - A/B Testing Framework:\")\n",
    "ab_test = ABTestFramework()\n",
    "\n",
    "# Power analysis\n",
    "baseline_rate = 0.05\n",
    "improvement = 0.02\n",
    "new_rate = baseline_rate + improvement\n",
    "\n",
    "required_n = ab_test.sample_size_for_proportions(baseline_rate, new_rate)\n",
    "print(f\"Required sample size per group: {required_n:,}\")\n",
    "\n",
    "# Simulate test results\n",
    "np.random.seed(999)\n",
    "conversions_a = np.random.binomial(required_n, baseline_rate)\n",
    "conversions_b = np.random.binomial(required_n, new_rate)\n",
    "\n",
    "results = ab_test.analyze_test(conversions_a, required_n, conversions_b, required_n)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"   Version A: {conversions_a}/{required_n} = {results['rate_a']:.3f}\")\n",
    "print(f\"   Version B: {conversions_b}/{required_n} = {results['rate_b']:.3f}\")\n",
    "print(f\"   Difference: {results['difference']:.3f} ({results['difference']:.1%})\")\n",
    "print(f\"   95% CI: [{results['ci_lower']:.3f}, {results['ci_upper']:.3f}]\")\n",
    "print(f\"   P-value: {results['p_value']:.4f}\")\n",
    "print(f\"   Significant: {'‚úÖ Yes' if results['significant'] else '‚ùå No'}\")\n",
    "\n",
    "print(f\"\\nüí° Complete A/B testing framework with statistical rigor!\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up & Course Conclusion\n",
    "‚úÖ You've mastered the Central Limit Theorem - the foundation of statistical inference  \n",
    "‚úÖ You understand the Law of Large Numbers and why sample size matters  \n",
    "‚úÖ You can use normal approximations to solve business problems  \n",
    "‚úÖ You know what confidence intervals really mean and how to use them  \n",
    "‚úÖ You can avoid common statistical misconceptions and pitfalls  \n",
    "\n",
    "**Quick Reference Card:**\n",
    "- üé™ **CLT Magic**: Sample means become normal (n ‚â• 30)\n",
    "- üìè **Standard Error**: œÉ/‚àön (bigger samples = smaller error)\n",
    "- üéØ **Confidence Intervals**: Quantify uncertainty in estimates\n",
    "- ‚öñÔ∏è **95% Confidence**: 95% of intervals contain the true parameter\n",
    "- üö´ **Remember**: Each trial is independent (no gambler's fallacy!)\n",
    "\n",
    "## üéì Statistics Foundations Complete!\n",
    "\n",
    "Congratulations! You've completed an incredible journey through statistics fundamentals:\n",
    "\n",
    "### What You've Mastered:\n",
    "1. **üìä Data Types & Visualization** - Choose the right chart for your story\n",
    "2. **üìà Descriptive Statistics** - Summarize data with mean, median, std dev, and z-scores\n",
    "3. **üîî Probability Distributions** - Model real-world phenomena with normal, binomial, uniform\n",
    "4. **üîó Correlation & Relationships** - Find connections while avoiding causation traps\n",
    "5. **üë• Sampling & Populations** - Make big conclusions from small samples\n",
    "6. **üé™ Central Limit Theorem** - The magic that makes statistical inference possible\n",
    "\n",
    "### Your Statistical Superpowers:\n",
    "- Design surveys and experiments that avoid bias\n",
    "- Calculate confidence intervals for business metrics\n",
    "- Detect outliers and unusual patterns in data\n",
    "- Make data-driven decisions with quantified uncertainty\n",
    "- Communicate statistical findings to non-technical stakeholders\n",
    "\n",
    "### Next Steps in Your Data Science Journey:\n",
    "1. **Hypothesis Testing** - Formal statistical tests and p-values\n",
    "2. **Regression Analysis** - Predict outcomes and understand relationships\n",
    "3. **Machine Learning** - Algorithms that learn from data\n",
    "4. **Advanced Statistics** - ANOVA, time series, Bayesian methods\n",
    "\n",
    "### Final Challenge:\n",
    "Apply everything you've learned to a real dataset:\n",
    "1. Find an interesting dataset (Kaggle, government data, company data)\n",
    "2. Perform complete exploratory data analysis\n",
    "3. Calculate descriptive statistics and create visualizations\n",
    "4. Find correlations and test for statistical significance\n",
    "5. Build confidence intervals for key metrics\n",
    "6. Write a professional report with business recommendations\n",
    "\n",
    "**Remember**: Statistics is your superpower for making better decisions with data. Keep practicing, stay curious, and never stop asking \"What does the data really tell us?\"\n",
    "\n",
    "üöÄ You're now ready to tackle real-world data science challenges with confidence!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
