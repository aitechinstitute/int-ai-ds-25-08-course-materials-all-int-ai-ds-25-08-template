{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 04 ¬∑ Notebook 05 ‚Äî Correlation and Relationships\n",
    "**Instructor:** Amir Charkhi  |  **Goal:** Discover hidden relationships in your data and avoid the correlation-causation trap.\n",
    "\n",
    "> Format: short theory ‚Üí quick practice ‚Üí build understanding ‚Üí mini-challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Objectives\n",
    "- Master correlation vs causation (the eternal data science battle!)\n",
    "- Calculate and interpret correlation coefficients like a pro\n",
    "- Create stunning correlation matrices and heatmaps\n",
    "- Understand Anscombe's Quartet and why visualization saves lives\n",
    "- Identify different types of relationships in your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Correlation Fundamentals\n",
    "**Your relationship detector**: Correlation tells you how strongly two variables dance together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set style for nice plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (8, 4)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation cheat sheet\n",
    "correlation_guide = {\n",
    "    \"+1.0\": \"üéØ Perfect positive - as X goes up, Y goes up perfectly\",\n",
    "    \"+0.8\": \"üí™ Strong positive - X and Y move together strongly\",\n",
    "    \"+0.5\": \"üëç Moderate positive - noticeable relationship\",\n",
    "    \"+0.2\": \"ü§è Weak positive - barely there\",\n",
    "    \"0.0\": \"ü§∑ No linear relationship - completely independent\",\n",
    "    \"-0.2\": \"ü§è Weak negative - barely opposite\",\n",
    "    \"-0.5\": \"üëé Moderate negative - one goes up, other goes down\",\n",
    "    \"-0.8\": \"üí• Strong negative - strong opposite relationship\",\n",
    "    \"-1.0\": \"üéØ Perfect negative - exact opposites\"\n",
    "}\n",
    "\n",
    "print(\"üìä Correlation Strength Guide:\")\n",
    "for corr, meaning in correlation_guide.items():\n",
    "    print(f\"   {corr:>4}: {meaning}\")\n",
    "    \n",
    "print(\"\\n‚ö†Ô∏è  REMEMBER: Correlation ‚â† Causation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Perfect Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with different correlation strengths\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "x = np.random.normal(0, 1, n)\n",
    "\n",
    "# Create different relationships\n",
    "perfect_pos = x + 0.01 * np.random.normal(0, 0.1, n)  # Almost perfect\n",
    "strong_pos = x + 0.5 * np.random.normal(0, 1, n)      # Strong positive  \n",
    "moderate_pos = x + 1.2 * np.random.normal(0, 1, n)    # Moderate positive\n",
    "no_corr = np.random.normal(0, 1, n)                   # No correlation\n",
    "strong_neg = -x + 0.5 * np.random.normal(0, 1, n)     # Strong negative\n",
    "\n",
    "# Calculate actual correlations\n",
    "correlations = {\n",
    "    'Perfect Positive': np.corrcoef(x, perfect_pos)[0,1],\n",
    "    'Strong Positive': np.corrcoef(x, strong_pos)[0,1],\n",
    "    'Moderate Positive': np.corrcoef(x, moderate_pos)[0,1],\n",
    "    'No Correlation': np.corrcoef(x, no_corr)[0,1],\n",
    "    'Strong Negative': np.corrcoef(x, strong_neg)[0,1]\n",
    "}\n",
    "\n",
    "print(\"üé≤ Generated Correlation Examples:\")\n",
    "for name, corr in correlations.items():\n",
    "    print(f\"   {name:<18}: r = {corr:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation spectrum\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "datasets = [\n",
    "    (x, perfect_pos, 'Perfect Positive', 'green'),\n",
    "    (x, strong_pos, 'Strong Positive', 'blue'),\n",
    "    (x, moderate_pos, 'Moderate Positive', 'lightblue'),\n",
    "    (x, no_corr, 'No Correlation', 'gray'),\n",
    "    (x, strong_neg, 'Strong Negative', 'red'),\n",
    "    (x, -x, 'Perfect Negative', 'darkred')\n",
    "]\n",
    "\n",
    "for i, (x_data, y_data, title, color) in enumerate(datasets):\n",
    "    axes[i].scatter(x_data, y_data, alpha=0.6, color=color, s=30)\n",
    "    axes[i].set_title(title, fontsize=12, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    r = np.corrcoef(x_data, y_data)[0,1]\n",
    "    axes[i].text(0.05, 0.95, f'r = {r:.3f}', transform=axes[i].transAxes, \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle('üéØ The Correlation Spectrum: From Perfect to None', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Real Business Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic employee performance dataset\n",
    "np.random.seed(123)\n",
    "n_employees = 200\n",
    "\n",
    "# Base performance factor (hidden variable)\n",
    "performance_factor = np.random.normal(75, 15, n_employees)\n",
    "\n",
    "employee_data = pd.DataFrame({\n",
    "    'hours_worked': np.maximum(35, performance_factor/2 + np.random.normal(40, 5, n_employees)),\n",
    "    'productivity_score': np.clip(performance_factor + np.random.normal(0, 8, n_employees), 0, 100),\n",
    "    'training_hours': np.maximum(0, performance_factor/8 + np.random.normal(10, 3, n_employees)),\n",
    "    'job_satisfaction': np.clip(performance_factor/10 + 3 + np.random.normal(0, 1.5, n_employees), 1, 10),\n",
    "    'years_experience': np.maximum(0, performance_factor/15 + np.random.normal(5, 3, n_employees)),\n",
    "    'salary': np.maximum(30000, performance_factor * 800 + np.random.normal(50000, 8000, n_employees))\n",
    "})\n",
    "\n",
    "print(f\"üë• Employee Dataset: {employee_data.shape[0]} employees, {employee_data.shape[1]} variables\")\n",
    "print(\"\\nüìä First look at the data:\")\n",
    "print(employee_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data summary\n",
    "print(\"üìà Employee Data Summary:\")\n",
    "print(employee_data.describe().round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 ‚Äî Quick Correlation Check (easy)**  \n",
    "Calculate the correlation between hours worked and productivity score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn - calculate correlation between hours_worked and productivity_score\n",
    "# Try multiple methods: pandas .corr(), numpy, and scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "print(\"üîç Hours Worked vs Productivity Analysis:\")\n",
    "\n",
    "# Method 1: Pandas (easiest)\n",
    "pandas_corr = employee_data['hours_worked'].corr(employee_data['productivity_score'])\n",
    "print(f\"   Pandas method: r = {pandas_corr:.3f}\")\n",
    "\n",
    "# Method 2: NumPy \n",
    "numpy_corr = np.corrcoef(employee_data['hours_worked'], employee_data['productivity_score'])[0,1]\n",
    "print(f\"   NumPy method:  r = {numpy_corr:.3f}\")\n",
    "\n",
    "# Method 3: SciPy (gives p-value too)\n",
    "scipy_corr, p_value = stats.pearsonr(employee_data['hours_worked'], employee_data['productivity_score'])\n",
    "print(f\"   SciPy method:  r = {scipy_corr:.3f}, p-value = {p_value:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "if abs(pandas_corr) > 0.7:\n",
    "    strength = \"strong\"\n",
    "elif abs(pandas_corr) > 0.5:\n",
    "    strength = \"moderate\"\n",
    "elif abs(pandas_corr) > 0.3:\n",
    "    strength = \"weak\"\n",
    "else:\n",
    "    strength = \"very weak\"\n",
    "\n",
    "direction = \"positive\" if pandas_corr > 0 else \"negative\"\n",
    "print(f\"\\nüí° Interpretation: {strength} {direction} relationship\")\n",
    "print(f\"   More hours worked tends to relate to {'higher' if pandas_corr > 0 else 'lower'} productivity\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Mighty Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate full correlation matrix\n",
    "correlation_matrix = employee_data.corr()\n",
    "\n",
    "print(\"üéØ Complete Correlation Matrix:\")\n",
    "print(correlation_matrix.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a beautiful correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Hide upper triangle\n",
    "\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            square=True, \n",
    "            fmt='.3f', \n",
    "            cbar_kws={\"shrink\": .8},\n",
    "            linewidths=0.5)\n",
    "\n",
    "plt.title('üë• Employee Performance Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the strongest relationships\n",
    "# Get upper triangle of correlation matrix (avoid duplicates)\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "correlations_list = upper_triangle.stack().sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(\"üèÜ Strongest Relationships (Top 5):\")\n",
    "for i, (variables, corr) in enumerate(correlations_list.head().items()):\n",
    "    var1, var2 = variables\n",
    "    strength = \"üí™ Strong\" if abs(corr) > 0.7 else \"üëç Moderate\" if abs(corr) > 0.5 else \"ü§è Weak\"\n",
    "    direction = \"positive\" if corr > 0 else \"negative\"\n",
    "    print(f\"   {i+1}. {var1} ‚Üî {var2}\")\n",
    "    print(f\"      r = {corr:+.3f} ({strength} {direction})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Anscombe's Quartet: The Plot Twist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 The Most Important Lesson in Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The famous Anscombe's Quartet\n",
    "anscombe_data = {\n",
    "    'x1': [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5],\n",
    "    'y1': [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68],\n",
    "    'x2': [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5],\n",
    "    'y2': [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74],\n",
    "    'x3': [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5],\n",
    "    'y3': [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73],\n",
    "    'x4': [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8],\n",
    "    'y4': [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n",
    "}\n",
    "\n",
    "anscombe_df = pd.DataFrame(anscombe_data)\n",
    "\n",
    "print(\"üé≠ Anscombe's Quartet: The Ultimate Data Surprise\")\n",
    "print(\"\\nüìä Summary Statistics Comparison:\")\n",
    "print(\"Dataset | Mean X | Mean Y | Std X | Std Y | Correlation\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "datasets = [('x1', 'y1'), ('x2', 'y2'), ('x3', 'y3'), ('x4', 'y4')]\n",
    "\n",
    "for i, (x_col, y_col) in enumerate(datasets, 1):\n",
    "    x_data = anscombe_df[x_col]\n",
    "    y_data = anscombe_df[y_col]\n",
    "    \n",
    "    mean_x = x_data.mean()\n",
    "    mean_y = y_data.mean()\n",
    "    std_x = x_data.std()\n",
    "    std_y = y_data.std()\n",
    "    corr = x_data.corr(y_data)\n",
    "    \n",
    "    print(f\"   {i}    |  {mean_x:.2f}  |  {mean_y:.2f}  | {std_x:.2f}  | {std_y:.2f}  |   {corr:.3f}\")\n",
    "\n",
    "print(\"\\nü§Ø SHOCKING: All four datasets have nearly identical statistics!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the shocking truth\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "titles = ['Dataset I: Linear', 'Dataset II: Curved', 'Dataset III: Outlier', 'Dataset IV: Extreme']\n",
    "\n",
    "for i, (x_col, y_col) in enumerate(datasets):\n",
    "    x_data = anscombe_df[x_col]\n",
    "    y_data = anscombe_df[y_col]\n",
    "    \n",
    "    axes[i].scatter(x_data, y_data, s=80, alpha=0.8, color=colors[i])\n",
    "    \n",
    "    # Add best fit line\n",
    "    z = np.polyfit(x_data, y_data, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(x_data.min(), x_data.max(), 100)\n",
    "    axes[i].plot(x_line, p(x_line), \"--\", color='black', alpha=0.8, linewidth=2)\n",
    "    \n",
    "    axes[i].set_title(titles[i], fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('X')\n",
    "    axes[i].set_ylabel('Y')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation\n",
    "    corr = x_data.corr(y_data)\n",
    "    axes[i].text(0.05, 0.95, f'r = {corr:.3f}', transform=axes[i].transAxes,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle(\"üé≠ Anscombe's Quartet: Same Stats, VERY Different Stories!\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° LESSON: ALWAYS visualize your data before making conclusions!\")\n",
    "print(\"üìä Statistics alone can be misleading - plots reveal the truth!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 ‚Äî Correlation Traps (medium)**  \n",
    "Identify which scenarios show spurious correlation (correlation without causation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn - analyze these scenarios\n",
    "scenarios = [\n",
    "    \"Ice cream sales and drowning incidents (both increase in summer)\",\n",
    "    \"Hours studied and exam scores\", \n",
    "    \"Shoe size and reading ability in children\",\n",
    "    \"Employee training hours and job performance\",\n",
    "    \"Number of firefighters and property damage\"\n",
    "]\n",
    "\n",
    "# For each scenario, determine:\n",
    "# 1. Is there likely correlation?\n",
    "# 2. Is there causation?\n",
    "# 3. What might be the hidden variable?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "print(\"üïµÔ∏è Correlation vs Causation Analysis:\")\n",
    "print()\n",
    "\n",
    "analyses = [\n",
    "    {\n",
    "        'scenario': \"Ice cream sales and drowning incidents\",\n",
    "        'correlation': \"YES - both increase together\",\n",
    "        'causation': \"NO - ice cream doesn't cause drowning!\",\n",
    "        'hidden_variable': \"Temperature/Season (hot weather increases both)\",\n",
    "        'type': \"üö® SPURIOUS\"\n",
    "    },\n",
    "    {\n",
    "        'scenario': \"Hours studied and exam scores\",\n",
    "        'correlation': \"YES - more study, higher scores\",\n",
    "        'causation': \"LIKELY - studying does improve performance\",\n",
    "        'hidden_variable': \"Student motivation, prior knowledge\",\n",
    "        'type': \"‚úÖ REAL\"\n",
    "    },\n",
    "    {\n",
    "        'scenario': \"Shoe size and reading ability in children\",\n",
    "        'correlation': \"YES - bigger feet, better reading\",\n",
    "        'causation': \"NO - feet don't help you read!\",\n",
    "        'hidden_variable': \"Age (older kids have bigger feet AND read better)\",\n",
    "        'type': \"üö® SPURIOUS\"\n",
    "    },\n",
    "    {\n",
    "        'scenario': \"Employee training and job performance\",\n",
    "        'correlation': \"YES - more training, better performance\",\n",
    "        'causation': \"LIKELY - training develops skills\",\n",
    "        'hidden_variable': \"Employee motivation, initial ability\",\n",
    "        'type': \"‚úÖ REAL\"\n",
    "    },\n",
    "    {\n",
    "        'scenario': \"Number of firefighters and property damage\",\n",
    "        'correlation': \"YES - more firefighters, more damage\",\n",
    "        'causation': \"NO - firefighters don't cause damage!\",\n",
    "        'hidden_variable': \"Fire severity (bigger fires need more firefighters AND cause more damage)\",\n",
    "        'type': \"üö® SPURIOUS\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, analysis in enumerate(analyses, 1):\n",
    "    print(f\"{i}. {analysis['scenario']} {analysis['type']}\")\n",
    "    print(f\"   Correlation: {analysis['correlation']}\")\n",
    "    print(f\"   Causation: {analysis['causation']}\")\n",
    "    print(f\"   Hidden factor: {analysis['hidden_variable']}\")\n",
    "    print()\n",
    "\n",
    "print(\"üí° Key insight: Always look for hidden variables that might explain both!\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 How Outliers Can Fool You"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate outlier impact\n",
    "np.random.seed(42)\n",
    "x_clean = np.random.normal(0, 1, 50)\n",
    "y_clean = 0.2 * x_clean + np.random.normal(0, 1, 50)\n",
    "\n",
    "# Add a powerful outlier\n",
    "x_with_outlier = np.append(x_clean, 5)\n",
    "y_with_outlier = np.append(y_clean, 5)\n",
    "\n",
    "corr_clean = np.corrcoef(x_clean, y_clean)[0,1]\n",
    "corr_with_outlier = np.corrcoef(x_with_outlier, y_with_outlier)[0,1]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Clean data\n",
    "ax1.scatter(x_clean, y_clean, alpha=0.7, color='blue')\n",
    "ax1.set_title(f'Clean Data: r = {corr_clean:.3f}\\n(Weak positive relationship)')\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# With outlier\n",
    "ax2.scatter(x_clean, y_clean, alpha=0.7, color='blue', label='Normal data')\n",
    "ax2.scatter([5], [5], color='red', s=200, label='Outlier', marker='*')\n",
    "ax2.set_title(f'With Outlier: r = {corr_with_outlier:.3f}\\n(Appears much stronger!)')\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('Y')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üéØ Outlier Impact Analysis:\")\n",
    "print(f\"   Without outlier: r = {corr_clean:+.3f}\")\n",
    "print(f\"   With outlier:    r = {corr_with_outlier:+.3f}\")\n",
    "print(f\"   Change:          {corr_with_outlier - corr_clean:+.3f}\")\n",
    "print(f\"\\n‚ö†Ô∏è  One outlier completely changed our conclusion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Non-Linear Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show when correlation fails\n",
    "x = np.linspace(-3, 3, 100)\n",
    "\n",
    "# Create non-linear relationships\n",
    "y_quadratic = x**2 + np.random.normal(0, 0.5, 100)\n",
    "y_sine = 3 * np.sin(2*x) + np.random.normal(0, 0.3, 100)\n",
    "y_circle = np.sqrt(4 - x**2) + np.random.normal(0, 0.2, 100)\n",
    "y_circle = np.where(np.isnan(y_circle), 0, y_circle)  # Handle NaN values\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Quadratic\n",
    "axes[0].scatter(x, y_quadratic, alpha=0.7, color='green')\n",
    "axes[0].set_title(f'Quadratic: r = {np.corrcoef(x, y_quadratic)[0,1]:.3f}\\n(Strong relationship, weak correlation!)')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('Y')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sine wave\n",
    "axes[1].scatter(x, y_sine, alpha=0.7, color='purple')\n",
    "axes[1].set_title(f'Sine Wave: r = {np.corrcoef(x, y_sine)[0,1]:.3f}\\n(Perfect pattern, zero correlation!)')\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('Y')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Circular\n",
    "x_circle = x[abs(x) <= 2]  # Limit to valid range\n",
    "y_circle = y_circle[:len(x_circle)]\n",
    "axes[2].scatter(x_circle, y_circle, alpha=0.7, color='orange')\n",
    "axes[2].set_title(f'Curved: r = {np.corrcoef(x_circle, y_circle)[0,1]:.3f}\\n(Clear relationship, low correlation!)')\n",
    "axes[2].set_xlabel('X')\n",
    "axes[2].set_ylabel('Y')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('üåÄ When Correlation Fails: Non-Linear Relationships', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° LESSON: Correlation only measures LINEAR relationships!\")\n",
    "print(\"üìä Always plot your data to see the full picture!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3 ‚Äî Real Business Analysis (hard)**  \n",
    "Analyze marketing campaign effectiveness using correlation analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marketing campaign dataset\n",
    "np.random.seed(789)\n",
    "n_campaigns = 100\n",
    "\n",
    "marketing_data = pd.DataFrame({\n",
    "    'ad_spend': np.random.exponential(5000, n_campaigns),\n",
    "    'social_media_followers': np.random.poisson(10000, n_campaigns),\n",
    "    'email_opens': np.random.poisson(2000, n_campaigns),\n",
    "    'website_traffic': np.random.poisson(5000, n_campaigns),\n",
    "    'conversions': np.random.poisson(100, n_campaigns),\n",
    "    'revenue': np.random.exponential(15000, n_campaigns)\n",
    "})\n",
    "\n",
    "# Add realistic relationships\n",
    "marketing_data['website_traffic'] += (marketing_data['ad_spend'] / 2).astype(int)\n",
    "marketing_data['conversions'] += (marketing_data['website_traffic'] / 80).astype(int)\n",
    "marketing_data['revenue'] += marketing_data['conversions'] * 50\n",
    "\n",
    "print(f\"üìä Marketing Dataset: {marketing_data.shape}\")\n",
    "print(marketing_data.head())\n",
    "\n",
    "# Your tasks:\n",
    "# 1. Create correlation matrix and heatmap\n",
    "# 2. Find the top 3 strongest correlations\n",
    "# 3. Create scatter plots for strongest relationships\n",
    "# 4. Identify which correlations likely represent causation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Task 1: Correlation matrix and heatmap\n",
    "marketing_corr = marketing_data.corr()\n",
    "print(\"üìä Task 1 - Marketing Correlation Matrix:\")\n",
    "print(marketing_corr.round(3))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(marketing_corr, annot=True, cmap='RdBu_r', center=0,\n",
    "            square=True, fmt='.3f', cbar_kws={\"shrink\": .8})\n",
    "plt.title('üìà Marketing Campaign Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Task 2: Find strongest correlations\n",
    "upper_tri = marketing_corr.where(np.triu(np.ones(marketing_corr.shape), k=1).astype(bool))\n",
    "strong_corrs = upper_tri.stack().sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ Task 2 - Top 3 Strongest Relationships:\")\n",
    "for i, (variables, corr) in enumerate(strong_corrs.head(3).items()):\n",
    "    var1, var2 = variables\n",
    "    print(f\"   {i+1}. {var1} ‚Üî {var2}: r = {corr:+.3f}\")\n",
    "\n",
    "# Task 3: Scatter plots\n",
    "print(\"\\nüìä Task 3 - Scatter Plots for Strongest Relationships:\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, (variables, corr) in enumerate(strong_corrs.head(3).items()):\n",
    "    var1, var2 = variables\n",
    "    axes[i].scatter(marketing_data[var1], marketing_data[var2], alpha=0.6)\n",
    "    axes[i].set_xlabel(var1.replace('_', ' ').title())\n",
    "    axes[i].set_ylabel(var2.replace('_', ' ').title())\n",
    "    axes[i].set_title(f'{var1} vs {var2}\\nr = {corr:.3f}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Task 4: Causation analysis\n",
    "print(\"\\nüîç Task 4 - Causation Analysis:\")\n",
    "causation_likely = [\n",
    "    (\"ad_spend\", \"website_traffic\", \"‚úÖ LIKELY - ads drive traffic\"),\n",
    "    (\"website_traffic\", \"conversions\", \"‚úÖ LIKELY - more visitors ‚Üí more conversions\"),\n",
    "    (\"conversions\", \"revenue\", \"‚úÖ LIKELY - conversions directly generate revenue\")\n",
    "]\n",
    "\n",
    "for var1, var2, analysis in causation_likely:\n",
    "    if var1 in marketing_data.columns and var2 in marketing_data.columns:\n",
    "        corr = marketing_data[var1].corr(marketing_data[var2])\n",
    "        print(f\"   {var1} ‚Üí {var2}: r = {corr:.3f} {analysis}\")\n",
    "\n",
    "print(\"\\nüí° Business Insights:\")\n",
    "print(\"   ‚Ä¢ Focus ad spend on channels that drive website traffic\")\n",
    "print(\"   ‚Ä¢ Optimize website to improve conversion rates\")\n",
    "print(\"   ‚Ä¢ Monitor the traffic ‚Üí conversion ‚Üí revenue funnel\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Mini-Challenges\n",
    "- **M1 (easy):** Create a correlation analysis dashboard for customer satisfaction data\n",
    "- **M2 (medium):** Build an outlier detection system using correlation analysis\n",
    "- **M3 (hard):** Design a causation investigation framework for business decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn - try the challenges!\n",
    "# M1 Data: Customer satisfaction scores, service quality, price, delivery time\n",
    "# M2 Data: Detect unusual patterns in employee performance data\n",
    "# M3 Data: Investigate whether training programs actually improve performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solutions</b></summary>\n",
    "\n",
    "```python\n",
    "# M1 - Customer Satisfaction Dashboard\n",
    "np.random.seed(111)\n",
    "customer_data = pd.DataFrame({\n",
    "    'satisfaction': np.random.normal(7, 1.5, 200),\n",
    "    'service_quality': np.random.normal(8, 1, 200),\n",
    "    'price_rating': np.random.normal(6, 2, 200),\n",
    "    'delivery_speed': np.random.normal(7.5, 1.2, 200)\n",
    "})\n",
    "\n",
    "# Add realistic relationships\n",
    "customer_data['satisfaction'] += 0.5 * customer_data['service_quality'] + 0.3 * customer_data['delivery_speed']\n",
    "customer_data = customer_data.clip(1, 10)  # Keep ratings in 1-10 range\n",
    "\n",
    "print(\"üòä M1 - Customer Satisfaction Analysis:\")\n",
    "corr_matrix = customer_data.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdYlBu_r', center=0, square=True)\n",
    "plt.title('Customer Satisfaction Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "strongest_factor = corr_matrix['satisfaction'].drop('satisfaction').abs().idxmax()\n",
    "strongest_corr = corr_matrix['satisfaction'][strongest_factor]\n",
    "print(f\"Strongest driver of satisfaction: {strongest_factor} (r = {strongest_corr:.3f})\")\n",
    "\n",
    "# M2 - Outlier Detection using Correlation\n",
    "def detect_correlation_outliers(data, threshold=0.1):\n",
    "    \"\"\"Find data points that don't follow expected correlations\"\"\"\n",
    "    outliers = []\n",
    "    \n",
    "    for idx in data.index:\n",
    "        # Calculate correlation with and without this point\n",
    "        full_corr = data.iloc[:, 0].corr(data.iloc[:, 1])\n",
    "        without_point = data.drop(idx)\n",
    "        reduced_corr = without_point.iloc[:, 0].corr(without_point.iloc[:, 1])\n",
    "        \n",
    "        if abs(full_corr - reduced_corr) > threshold:\n",
    "            outliers.append((idx, abs(full_corr - reduced_corr)))\n",
    "    \n",
    "    return sorted(outliers, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nüîç M2 - Outlier Detection:\")\n",
    "test_data = employee_data[['hours_worked', 'productivity_score']].copy()\n",
    "outliers = detect_correlation_outliers(test_data)\n",
    "print(f\"Found {len(outliers)} potential outliers\")\n",
    "if outliers:\n",
    "    print(f\"Most influential outlier: Employee {outliers[0][0]} (impact: {outliers[0][1]:.3f})\")\n",
    "\n",
    "# M3 - Causation Investigation Framework\n",
    "def investigate_causation(data, cause_var, effect_var, confounders=None):\n",
    "    \"\"\"Framework for investigating potential causation\"\"\"\n",
    "    print(f\"\\nüî¨ M3 - Causation Investigation: {cause_var} ‚Üí {effect_var}\")\n",
    "    \n",
    "    # 1. Basic correlation\n",
    "    basic_corr = data[cause_var].corr(data[effect_var])\n",
    "    print(f\"1. Basic correlation: r = {basic_corr:.3f}\")\n",
    "    \n",
    "    # 2. Check for confounders\n",
    "    if confounders:\n",
    "        for confounder in confounders:\n",
    "            cause_conf = data[cause_var].corr(data[confounder])\n",
    "            effect_conf = data[effect_var].corr(data[confounder])\n",
    "            print(f\"2. Confounder {confounder}: cause r={cause_conf:.3f}, effect r={effect_conf:.3f}\")\n",
    "    \n",
    "    # 3. Temporal logic check\n",
    "    print(f\"3. Temporal logic: Does {cause_var} precede {effect_var}? [Manual check needed]\")\n",
    "    \n",
    "    # 4. Strength assessment\n",
    "    if abs(basic_corr) > 0.7:\n",
    "        strength = \"Strong evidence\"\n",
    "    elif abs(basic_corr) > 0.5:\n",
    "        strength = \"Moderate evidence\"\n",
    "    elif abs(basic_corr) > 0.3:\n",
    "        strength = \"Weak evidence\"\n",
    "    else:\n",
    "        strength = \"Insufficient evidence\"\n",
    "    \n",
    "    print(f\"4. Conclusion: {strength} for causation\")\n",
    "    return basic_corr\n",
    "\n",
    "# Test the framework\n",
    "investigate_causation(employee_data, 'training_hours', 'productivity_score', \n",
    "                     confounders=['years_experience', 'salary'])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up & Next Steps\n",
    "‚úÖ You understand correlation vs causation (the most important lesson!)  \n",
    "‚úÖ You can calculate and interpret correlation coefficients  \n",
    "‚úÖ You've mastered correlation matrices and heatmaps  \n",
    "‚úÖ You know Anscombe's Quartet and why visualization is crucial  \n",
    "‚úÖ You can spot outliers and non-linear relationships  \n",
    "\n",
    "**Quick Reference Card:**\n",
    "- üìä **Correlation range**: -1 to +1 (strength and direction)\n",
    "- ‚ö†Ô∏è **Golden rule**: Correlation ‚â† Causation!\n",
    "- üéØ **Strong**: |r| > 0.7, **Moderate**: |r| > 0.5, **Weak**: |r| > 0.3\n",
    "- üëÅÔ∏è **Always visualize**: Anscombe's Quartet proves this!\n",
    "- üîç **Watch for**: Outliers, non-linear patterns, spurious correlations\n",
    "\n",
    "**Next:** Sampling and Populations - Learn how to make big conclusions from small samples!\n"
   ]
  }
 ],
 "metadata": {
  "kernelevel": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
