{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### CLT and Law of Large Numbers ‚Äî The Mathematical Foundations\n",
    "**Instructor:** Amir Charkhi  |  **Goal:** Master the two most powerful theorems that make statistics possible.\n",
    "\n",
    "> Format: short theory ‚Üí quick practice ‚Üí build understanding ‚Üí mini-challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Objectives\n",
    "- Define and apply the Central Limit Theorem (the foundation of statistical inference)\n",
    "- Understand the Law of Large Numbers and why sample size matters\n",
    "- See these theorems work with ANY distribution (exponential, uniform, weird shapes)\n",
    "- Apply these concepts to real business scenarios\n",
    "- Avoid the Gambler's Fallacy trap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Central Limit Theorem: Statistical Magic\n",
    "**The most important theorem in statistics**: No matter how weird your data looks, sample means become beautifully normal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import math \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set style for nice plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Limit Theorem explained\n",
    "clt_facts = {\n",
    "    \"üé™ The Magic\": \"Sample means become normal, regardless of original distribution\",\n",
    "    \"üìä What You Need\": \"Repeated samples of the same size (n ‚â• 30 works great)\",\n",
    "    \"üîÆ The Power\": \"Works for ANY distribution - exponential, uniform, bimodal, anything!\",\n",
    "    \"üéØ The Result\": \"Mean of sample means = population mean (Œº)\",\n",
    "    \"üìè The Precision\": \"Standard error = œÉ/‚àön (bigger samples = more precise)\",\n",
    "    \"‚ö° Why It Matters\": \"Makes statistical inference possible!\"\n",
    "}\n",
    "\n",
    "print(\"üé™ Central Limit Theorem - The Greatest Show in Statistics!\")\n",
    "for key, value in clt_facts.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "    \n",
    "print(\"\\nüí´ This is why we can make confident conclusions from small samples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CLT Demo: From Exponential Chaos to Normal Beauty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a highly skewed exponential distribution\n",
    "beta = 2\n",
    "num_samples = 100000\n",
    "exp_data = np.random.exponential(scale=beta, size=num_samples)\n",
    "\n",
    "print(f\"üìä Exponential Population (Œ≤={beta}):\")\n",
    "print(f\"   Population size: {num_samples:,}\")\n",
    "print(f\"   Mean: {exp_data.mean():.3f} (theory: {beta})\")\n",
    "print(f\"   Variance: {exp_data.var():.3f} (theory: {beta**2})\")\n",
    "print(f\"   Shape: Extremely right-skewed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier sampling\n",
    "exponential_data_df = pd.DataFrame(exp_data, columns=[\"Population Values\"])\n",
    "\n",
    "# Visualize the original distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(exp_data, bins=50, kde=True, alpha=0.7)\n",
    "plt.title('Original Exponential Distribution\\n(Highly Skewed - NOT Normal!)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(exp_data, bins=50, alpha=0.7, color='coral')\n",
    "plt.title('Same Data (Log Scale)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency (log scale)')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üëÜ This distribution looks nothing like a normal curve!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The CLT Transformation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced sampling function\n",
    "def sample_distribution(n, num_iterations, statistic_type, data):\n",
    "    \"\"\"Sample from data and calculate specified statistic\"\"\"\n",
    "    sample_stats = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        sample_i = data.sample(n, replace=True)\n",
    "        \n",
    "        if statistic_type == \"mean\":\n",
    "            sample_stat = sample_i.mean().iloc[0]\n",
    "        elif statistic_type == \"median\":\n",
    "            sample_stat = sample_i.median().iloc[0]\n",
    "        elif statistic_type == \"sum\":\n",
    "            sample_stat = sample_i.sum().iloc[0]\n",
    "        elif statistic_type == \"range\":\n",
    "            sample_stat = (sample_i.max() - sample_i.min()).iloc[0]\n",
    "        \n",
    "        sample_stats.append(sample_stat)\n",
    "    \n",
    "    return np.array(sample_stats)\n",
    "\n",
    "print(\"üîß CLT Testing Function Ready!\")\n",
    "print(\"   We'll sample from our exponential data and watch the magic happen...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sample Size Progression: Watching CLT Emerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different sample sizes\n",
    "sample_sizes = [5, 15, 30, 100]\n",
    "num_iterations = 10000\n",
    "\n",
    "print(f\"üß™ CLT Experiment: {num_iterations:,} samples each\")\n",
    "print(\"Sample Size | Mean of Means | Std of Means | Looks Normal?\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "results = {}\n",
    "for n in sample_sizes:\n",
    "    sample_means = sample_distribution(n, num_iterations, \"mean\", exponential_data_df)\n",
    "    results[n] = sample_means\n",
    "    \n",
    "    mean_of_means = np.mean(sample_means)\n",
    "    std_of_means = np.std(sample_means, ddof=1)\n",
    "    \n",
    "    # Quick normality check\n",
    "    _, p_value = stats.shapiro(sample_means[:5000])  # Test subset for speed\n",
    "    looks_normal = \"‚úÖ Yes\" if p_value > 0.05 else \"‚ùå Not yet\"\n",
    "    \n",
    "    print(f\"    {n:2d}      |     {mean_of_means:5.3f}     |    {std_of_means:5.3f}     | {looks_normal}\")\n",
    "\n",
    "print(f\"\\nüéØ Notice: All means center around {beta} (the population mean)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the CLT transformation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "colors = ['lightcoral', 'orange', 'lightgreen', 'lightblue']\n",
    "\n",
    "for i, n in enumerate(sample_sizes):\n",
    "    sample_means = results[n]\n",
    "    \n",
    "    axes[i].hist(sample_means, bins=40, density=True, alpha=0.7, \n",
    "                color=colors[i], edgecolor='black')\n",
    "    \n",
    "    # Overlay normal distribution\n",
    "    x = np.linspace(sample_means.min(), sample_means.max(), 100)\n",
    "    normal_curve = stats.norm.pdf(x, np.mean(sample_means), np.std(sample_means, ddof=1))\n",
    "    axes[i].plot(x, normal_curve, 'red', linewidth=3, label='Normal Fit')\n",
    "    \n",
    "    axes[i].set_title(f'Sample Size = {n}\\n{\"Getting Normal!\" if n >= 30 else \"Still Transforming...\"}')\n",
    "    axes[i].set_xlabel('Sample Mean')\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('üé™ CLT Magic: Exponential ‚Üí Normal Transformation!', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚ú® Amazing! By n=30, even the most skewed data produces normal sample means!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 ‚Äî Test CLT with Sample Sums (easy)**  \n",
    "Verify that CLT works for sample sums, not just means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn - test CLT with sample sums\n",
    "# Use sample size 100 and 10,000 iterations\n",
    "sample_size = 100\n",
    "iterations = 10000\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Test CLT with sample sums\n",
    "sample_sums = sample_distribution(sample_size, iterations, \"sum\", exponential_data_df)\n",
    "\n",
    "print(f\"üìä CLT Test with Sample Sums (n={sample_size}):\")\n",
    "print(f\"   Mean of sample sums: {np.mean(sample_sums):.1f}\")\n",
    "print(f\"   Expected (n √ó Œº): {sample_size * beta:.1f}\")\n",
    "print(f\"   Standard deviation: {np.std(sample_sums, ddof=1):.2f}\")\n",
    "\n",
    "# Test normality\n",
    "_, p_value = stats.shapiro(sample_sums[:5000])\n",
    "print(f\"   Normality test p-value: {p_value:.6f}\")\n",
    "print(f\"   Normal distribution: {'‚úÖ Yes' if p_value > 0.05 else '‚ùå No'}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sample_sums, bins=40, density=True, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "\n",
    "# Normal overlay\n",
    "x = np.linspace(sample_sums.min(), sample_sums.max(), 100)\n",
    "normal_curve = stats.norm.pdf(x, np.mean(sample_sums), np.std(sample_sums, ddof=1))\n",
    "plt.plot(x, normal_curve, 'red', linewidth=3, label='Normal Fit')\n",
    "\n",
    "plt.title('Sample Sums Distribution\\n(Also Normal Thanks to CLT!)')\n",
    "plt.xlabel('Sample Sum')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ CLT works for sums too! The magic applies to any linear combination.\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Law of Large Numbers: The Convergence Story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Understanding the Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Law of Large Numbers explained\n",
    "lln_facts = {\n",
    "    \"üìè The Promise\": \"Sample averages approach the true population mean as n ‚Üí ‚àû\",\n",
    "    \"üéØ Mathematical Form\": \"XÃÑ‚Çô ‚Üí E[X] (sample mean converges to expected value)\",\n",
    "    \"üí° Intuition\": \"The more data you collect, the closer you get to the truth\",\n",
    "    \"‚ö° Business Impact\": \"Larger samples give more reliable estimates\",\n",
    "    \"üö´ NOT Magic\": \"Individual results don't 'balance out' - each trial is independent!\"\n",
    "}\n",
    "\n",
    "print(\"üìè Law of Large Numbers - Why Bigger is Better:\")\n",
    "for key, value in lln_facts.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "    \n",
    "print(\"\\nüî¨ Let's see this law in action with classic examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Classic Example: Coin Tossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced coin toss experiment\n",
    "def coin_toss_experiment(n_experiments, tosses_per_experiment):\n",
    "    \"\"\"Run multiple coin toss experiments and return average tails per experiment\"\"\"\n",
    "    experiment_results = []\n",
    "    \n",
    "    for experiment in range(n_experiments):\n",
    "        # Each experiment: toss coin 'tosses_per_experiment' times\n",
    "        tosses = [random.randint(0, 1) for _ in range(tosses_per_experiment)]\n",
    "        tails_count = sum(tosses)\n",
    "        experiment_results.append(tails_count)\n",
    "    \n",
    "    return np.mean(experiment_results)\n",
    "\n",
    "# Expected value for 1000 tosses\n",
    "tosses_per_exp = 1000\n",
    "expected_tails = tosses_per_exp * 0.5\n",
    "\n",
    "print(f\"ü™ô Coin Toss Experiment Setup:\")\n",
    "print(f\"   Tosses per experiment: {tosses_per_exp}\")\n",
    "print(f\"   Expected tails per experiment: {expected_tails}\")\n",
    "print(f\"   Let's see how sample averages converge to this value...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different numbers of experiments\n",
    "experiment_counts = [1, 5, 10, 50, 100, 500]\n",
    "\n",
    "print(\"ü™ô Law of Large Numbers - Coin Toss Results:\")\n",
    "print(\"Experiments | Average Tails | Distance from Expected\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "convergence_data = []\n",
    "for n_exp in experiment_counts:\n",
    "    avg_tails = coin_toss_experiment(n_exp, 100)  # Shorter per experiment for speed\n",
    "    expected_for_100 = 100 * 0.5\n",
    "    distance = abs(avg_tails - expected_for_100)\n",
    "    \n",
    "    convergence_data.append((n_exp, avg_tails, distance))\n",
    "    print(f\"    {n_exp:3d}     |     {avg_tails:5.2f}     |      {distance:5.2f}\")\n",
    "\n",
    "print(f\"\\nüìà Notice: As experiments increase, distance from expected value decreases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence over many trials\n",
    "def plot_convergence(max_trials=1000):\n",
    "    \"\"\"Show how sample average converges to expected value\"\"\"\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    \n",
    "    # Generate many coin tosses\n",
    "    tosses = np.random.binomial(1, 0.5, max_trials)\n",
    "    \n",
    "    # Calculate running averages\n",
    "    running_averages = np.cumsum(tosses) / np.arange(1, max_trials + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(1, max_trials + 1), running_averages, 'b-', alpha=0.8, linewidth=1)\n",
    "    plt.axhline(y=0.5, color='red', linestyle='--', linewidth=2, \n",
    "               label='Expected Value (0.5)')\n",
    "    \n",
    "    plt.xlabel('Number of Coin Tosses')\n",
    "    plt.ylabel('Proportion of Heads')\n",
    "    plt.title('ü™ô Law of Large Numbers: Convergence to Expected Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(0.3, 0.7)\n",
    "    \n",
    "    # Add annotations for key points\n",
    "    checkpoints = [10, 100, 500, 1000]\n",
    "    for checkpoint in checkpoints:\n",
    "        if checkpoint <= max_trials:\n",
    "            value = running_averages[checkpoint-1]\n",
    "            plt.annotate(f'{value:.3f}', \n",
    "                        xy=(checkpoint, value), \n",
    "                        xytext=(checkpoint, value + 0.05),\n",
    "                        arrowprops=dict(arrowstyle='->', alpha=0.7),\n",
    "                        fontsize=10, ha='center')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Final proportion after {max_trials} tosses: {running_averages[-1]:.6f}\")\n",
    "    print(f\"Distance from 0.5: {abs(running_averages[-1] - 0.5):.6f}\")\n",
    "\n",
    "plot_convergence(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Die Rolling Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die rolling function\n",
    "def die_roll_experiment(n_rolls, show_details=False):\n",
    "    \"\"\"Roll a die n times and return the average\"\"\"\n",
    "    rolls = [random.randint(1, 6) for _ in range(n_rolls)]\n",
    "    \n",
    "    if show_details:\n",
    "        print(f\"Rolls: {rolls[:20]}{'...' if len(rolls) > 20 else ''}\")\n",
    "    \n",
    "    return np.mean(rolls)\n",
    "\n",
    "# Expected value for a fair 6-sided die\n",
    "expected_die_value = (1 + 2 + 3 + 4 + 5 + 6) / 6\n",
    "\n",
    "print(f\"üé≤ Die Rolling Experiment:\")\n",
    "print(f\"   Expected value: {expected_die_value}\")\n",
    "print(f\"   Let's see convergence with different sample sizes...\")\n",
    "print()\n",
    "\n",
    "# Test with increasing sample sizes\n",
    "sample_sizes = [10, 50, 100, 500, 1000, 5000]\n",
    "\n",
    "print(\"Sample Size | Average Roll | Distance from 3.5\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for n in sample_sizes:\n",
    "    avg_roll = die_roll_experiment(n)\n",
    "    distance = abs(avg_roll - expected_die_value)\n",
    "    \n",
    "    print(f\"    {n:4d}    |     {avg_roll:5.3f}    |      {distance:5.3f}\")\n",
    "\n",
    "print(f\"\\nüéØ Pattern: Larger samples ‚Üí closer to expected value (3.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 ‚Äî Stock Price Simulation (medium)**  \n",
    "Apply the Law of Large Numbers to a business scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock price simulation scenario\n",
    "# Daily stock returns: mean = 0.05% (positive trend), std = 2%\n",
    "daily_return_mean = 0.0005  # 0.05%\n",
    "daily_return_std = 0.02     # 2%\n",
    "\n",
    "print(f\"üìà Stock Return Analysis:\")\n",
    "print(f\"   Expected daily return: {daily_return_mean:.4f} ({daily_return_mean*100:.2f}%)\")\n",
    "print(f\"   Daily volatility: {daily_return_std:.4f} ({daily_return_std*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Your tasks:\n",
    "# 1. Simulate average returns over different time periods (1 day, 1 week, 1 month, 1 year)\n",
    "# 2. Show how the Law of Large Numbers makes long-term investing more predictable\n",
    "# 3. Calculate how the average return converges to the expected value\n",
    "\n",
    "time_periods = {\n",
    "    '1 day': 1,\n",
    "    '1 week': 5,\n",
    "    '1 month': 22,\n",
    "    '3 months': 66,\n",
    "    '1 year': 252\n",
    "}\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Stock return simulation using Law of Large Numbers\n",
    "np.random.seed(123)\n",
    "\n",
    "print(\"üìä Law of Large Numbers - Stock Return Analysis:\")\n",
    "print(\"Time Period | Avg Return | Distance from Expected | Volatility\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "simulation_results = {}\n",
    "\n",
    "for period_name, days in time_periods.items():\n",
    "    # Simulate many periods of this length\n",
    "    num_simulations = 1000\n",
    "    period_returns = []\n",
    "    \n",
    "    for sim in range(num_simulations):\n",
    "        # Generate daily returns for this period\n",
    "        daily_returns = np.random.normal(daily_return_mean, daily_return_std, days)\n",
    "        period_return = np.mean(daily_returns)  # Average daily return over period\n",
    "        period_returns.append(period_return)\n",
    "    \n",
    "    avg_return = np.mean(period_returns)\n",
    "    volatility = np.std(period_returns, ddof=1)\n",
    "    distance = abs(avg_return - daily_return_mean)\n",
    "    \n",
    "    simulation_results[period_name] = {\n",
    "        'avg_return': avg_return,\n",
    "        'volatility': volatility,\n",
    "        'distance': distance\n",
    "    }\n",
    "    \n",
    "    print(f\"{period_name:>11} | {avg_return:+8.4f} | {distance:12.4f} | {volatility:8.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "print(f\"   ‚Ä¢ Longer periods ‚Üí average returns closer to expected {daily_return_mean:.4f}\")\n",
    "print(f\"   ‚Ä¢ Longer periods ‚Üí lower volatility of average returns\")\n",
    "print(f\"   ‚Ä¢ Law of Large Numbers makes long-term investing more predictable!\")\n",
    "\n",
    "# Visualize the convergence\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "periods = list(time_periods.keys())\n",
    "distances = [simulation_results[p]['distance'] for p in periods]\n",
    "volatilities = [simulation_results[p]['volatility'] for p in periods]\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(len(periods)), distances, 'bo-', linewidth=2, markersize=8)\n",
    "plt.title('Convergence to Expected Return')\n",
    "plt.xlabel('Time Period')\n",
    "plt.ylabel('Distance from Expected')\n",
    "plt.xticks(range(len(periods)), periods, rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(periods)), volatilities, 'ro-', linewidth=2, markersize=8)\n",
    "plt.title('Decreasing Volatility')\n",
    "plt.xlabel('Time Period')\n",
    "plt.ylabel('Volatility of Average Returns')\n",
    "plt.xticks(range(len(periods)), periods, rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Business Application: This is why 'time in the market beats timing the market'!\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Gambler's Fallacy: Common Misconception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate why the Gambler's Fallacy is wrong\n",
    "def test_independence_after_streaks():\n",
    "    \"\"\"Test if outcomes are truly independent after streaks\"\"\"\n",
    "    num_sequences = 50000\n",
    "    sequence_length = 10\n",
    "    \n",
    "    after_5_heads = []\n",
    "    after_5_tails = []\n",
    "    \n",
    "    for _ in range(num_sequences):\n",
    "        sequence = [random.randint(0, 1) for _ in range(sequence_length)]\n",
    "        \n",
    "        # Look for 5 heads in a row\n",
    "        for i in range(len(sequence) - 5):\n",
    "            if sequence[i:i+5] == [1, 1, 1, 1, 1]:  # 5 heads\n",
    "                if i + 5 < len(sequence):  # Next flip exists\n",
    "                    after_5_heads.append(sequence[i + 5])\n",
    "        \n",
    "        # Look for 5 tails in a row\n",
    "        for i in range(len(sequence) - 5):\n",
    "            if sequence[i:i+5] == [0, 0, 0, 0, 0]:  # 5 tails\n",
    "                if i + 5 < len(sequence):  # Next flip exists\n",
    "                    after_5_tails.append(sequence[i + 5])\n",
    "    \n",
    "    return after_5_heads, after_5_tails\n",
    "\n",
    "# Run the test\n",
    "after_heads, after_tails = test_independence_after_streaks()\n",
    "\n",
    "print(\"üé∞ Gambler's Fallacy Test:\")\n",
    "print()\n",
    "\n",
    "if len(after_heads) > 0:\n",
    "    heads_prob = np.mean(after_heads)\n",
    "    print(f\"After 5 heads in a row:\")\n",
    "    print(f\"   Next flip is heads: {heads_prob:.3f} (sample size: {len(after_heads)})\")\n",
    "    print(f\"   Expected if fair: 0.500\")\n",
    "    print(f\"   Difference: {abs(heads_prob - 0.5):.3f}\")\n",
    "\n",
    "if len(after_tails) > 0:\n",
    "    tails_prob = np.mean(after_tails)\n",
    "    print(f\"\\nAfter 5 tails in a row:\")\n",
    "    print(f\"   Next flip is heads: {tails_prob:.3f} (sample size: {len(after_tails)})\")\n",
    "    print(f\"   Expected if fair: 0.500\")\n",
    "    print(f\"   Difference: {abs(tails_prob - 0.5):.3f}\")\n",
    "\n",
    "print(f\"\\nüí° Key Points:\")\n",
    "print(f\"   ‚úÖ Each flip is independent - past results don't affect future ones\")\n",
    "print(f\"   ‚ùå WRONG: 'I'm due for heads after many tails'\")\n",
    "print(f\"   ‚úÖ RIGHT: 'Each flip has exactly 50% chance regardless of history'\")\n",
    "print(f\"   üìä Law of Large Numbers works through accumulation, not 'balancing'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Business Applications: Where These Theorems Pay Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real business applications\n",
    "applications = {\n",
    "    \"üìä Quality Control\": {\n",
    "        \"CLT Application\": \"Sample averages of product measurements follow normal distribution\",\n",
    "        \"LLN Application\": \"Larger samples give more accurate estimates of defect rates\",\n",
    "        \"Business Impact\": \"Reliable control charts and statistical process control\"\n",
    "    },\n",
    "    \"üéØ A/B Testing\": {\n",
    "        \"CLT Application\": \"Conversion rate differences follow normal distribution\",\n",
    "        \"LLN Application\": \"More visitors give more reliable test results\",\n",
    "        \"Business Impact\": \"Confident decisions about website/product changes\"\n",
    "    },\n",
    "    \"üìà Market Research\": {\n",
    "        \"CLT Application\": \"Survey sample means approximate population means normally\",\n",
    "        \"LLN Application\": \"Larger surveys provide more accurate population estimates\",\n",
    "        \"Business Impact\": \"Reliable customer insights with quantified uncertainty\"\n",
    "    },\n",
    "    \"üí∞ Financial Risk\": {\n",
    "        \"CLT Application\": \"Portfolio returns become more predictable over time\",\n",
    "        \"LLN Application\": \"Long-term averages converge to expected returns\",\n",
    "        \"Business Impact\": \"Better risk assessment and investment strategies\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üíº Real Business Applications:\")\n",
    "print()\n",
    "\n",
    "for domain, details in applications.items():\n",
    "    print(f\"{domain}:\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    print()\n",
    "\n",
    "print(\"üöÄ These theorems are the mathematical foundation for data-driven business!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mini-Challenges\n",
    "- **M1 (easy):** Create a CLT demonstration with a uniform distribution\n",
    "- **M2 (medium):** Build a sample size calculator using LLN principles\n",
    "- **M3 (hard):** Design a complete business simulation showing both theorems in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn - try the challenges!\n",
    "# M1: Test CLT with uniform distribution from 0 to 10\n",
    "# M2: Calculate required sample sizes for different precision levels\n",
    "# M3: Create a customer satisfaction monitoring system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solutions</b></summary>\n",
    "\n",
    "```python\n",
    "# M1 - CLT with Uniform Distribution\n",
    "print(\"üé≤ M1 - CLT with Uniform Distribution:\")\n",
    "\n",
    "# Create uniform distribution\n",
    "uniform_data = np.random.uniform(0, 10, 50000)\n",
    "uniform_df = pd.DataFrame(uniform_data, columns=['values'])\n",
    "\n",
    "print(f\"Uniform distribution: 0 to 10\")\n",
    "print(f\"Expected mean: 5.0\")\n",
    "print(f\"Actual mean: {uniform_data.mean():.3f}\")\n",
    "\n",
    "# Test CLT with different sample sizes\n",
    "sample_sizes = [5, 15, 30, 100]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, n in enumerate(sample_sizes):\n",
    "    sample_means = sample_distribution(n, 5000, \"mean\", uniform_df)\n",
    "    \n",
    "    axes[i].hist(sample_means, bins=30, density=True, alpha=0.7, color=f'C{i}')\n",
    "    \n",
    "    # Normal overlay\n",
    "    x = np.linspace(sample_means.min(), sample_means.max(), 100)\n",
    "    normal_fit = stats.norm.pdf(x, np.mean(sample_means), np.std(sample_means))\n",
    "    axes[i].plot(x, normal_fit, 'red', linewidth=2)\n",
    "    \n",
    "    axes[i].set_title(f'n = {n}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('CLT with Uniform Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# M2 - Sample Size Calculator\n",
    "def sample_size_calculator(population_std, desired_margin, confidence_level=0.95):\n",
    "    \"\"\"Calculate required sample size for desired precision\"\"\"\n",
    "    z_scores = {0.90: 1.645, 0.95: 1.96, 0.99: 2.576}\n",
    "    z = z_scores[confidence_level]\n",
    "    \n",
    "    # n = (z * œÉ / E)¬≤\n",
    "    n = (z * population_std / desired_margin) ** 2\n",
    "    return int(np.ceil(n))\n",
    "\n",
    "print(f\"\\nüìä M2 - Sample Size Calculator:\")\n",
    "scenarios = [\n",
    "    (\"Customer satisfaction (1-10)\", 2.0, 0.2),\n",
    "    (\"Response time (seconds)\", 1.5, 0.1),\n",
    "    (\"Revenue per customer ($)\", 100, 10)\n",
    "]\n",
    "\n",
    "for scenario, std, margin in scenarios:\n",
    "    n_95 = sample_size_calculator(std, margin, 0.95)\n",
    "    n_99 = sample_size_calculator(std, margin, 0.99)\n",
    "    \n",
    "    print(f\"\\n{scenario}:\")\n",
    "    print(f\"   For ¬±{margin} precision: {n_95} (95% conf), {n_99} (99% conf)\")\n",
    "\n",
    "# M3 - Complete Business Simulation\n",
    "class CustomerSatisfactionMonitor:\n",
    "    def __init__(self, true_satisfaction=7.5, std_dev=1.5):\n",
    "        self.true_satisfaction = true_satisfaction\n",
    "        self.std_dev = std_dev\n",
    "        self.daily_samples = []\n",
    "        \n",
    "    def collect_daily_sample(self, sample_size=50):\n",
    "        \"\"\"Collect daily customer satisfaction sample\"\"\"\n",
    "        daily_scores = np.random.normal(self.true_satisfaction, self.std_dev, sample_size)\n",
    "        daily_scores = np.clip(daily_scores, 1, 10)  # Keep in 1-10 range\n",
    "        daily_mean = np.mean(daily_scores)\n",
    "        self.daily_samples.append(daily_mean)\n",
    "        return daily_mean\n",
    "    \n",
    "    def get_running_average(self):\n",
    "        \"\"\"Get current running average (LLN in action)\"\"\"\n",
    "        if self.daily_samples:\n",
    "            return np.mean(self.daily_samples)\n",
    "        return None\n",
    "    \n",
    "    def get_confidence_interval(self, confidence=0.95):\n",
    "        \"\"\"Get confidence interval for current estimate (CLT in action)\"\"\"\n",
    "        if len(self.daily_samples) < 2:\n",
    "            return None\n",
    "        \n",
    "        mean = np.mean(self.daily_samples)\n",
    "        std_err = np.std(self.daily_samples, ddof=1) / np.sqrt(len(self.daily_samples))\n",
    "        \n",
    "        z = 1.96 if confidence == 0.95 else 2.576\n",
    "        margin = z * std_err\n",
    "        \n",
    "        return (mean - margin, mean + margin)\n",
    "\n",
    "print(f\"\\nüìà M3 - Customer Satisfaction Monitor:\")\n",
    "\n",
    "# Simulate 30 days of monitoring\n",
    "monitor = CustomerSatisfactionMonitor(true_satisfaction=7.2, std_dev=1.8)\n",
    "\n",
    "print(f\"True satisfaction: {monitor.true_satisfaction}\")\n",
    "print(f\"\\nDaily monitoring results:\")\n",
    "\n",
    "for day in range(1, 31):\n",
    "    daily_score = monitor.collect_daily_sample(50)\n",
    "    running_avg = monitor.get_running_average()\n",
    "    \n",
    "    if day % 10 == 0:  # Report every 10 days\n",
    "        ci = monitor.get_confidence_interval()\n",
    "        print(f\"Day {day:2d}: Running avg = {running_avg:.3f}, 95% CI = [{ci[0]:.3f}, {ci[1]:.3f}]\")\n",
    "\n",
    "final_avg = monitor.get_running_average()\n",
    "final_ci = monitor.get_confidence_interval()\n",
    "error = abs(final_avg - monitor.true_satisfaction)\n",
    "\n",
    "print(f\"\\nFinal Results after 30 days:\")\n",
    "print(f\"   Estimated satisfaction: {final_avg:.3f}\")\n",
    "print(f\"   True satisfaction: {monitor.true_satisfaction}\")\n",
    "print(f\"   Error: {error:.3f}\")\n",
    "print(f\"   95% Confidence interval: [{final_ci[0]:.3f}, {final_ci[1]:.3f}]\")\n",
    "print(f\"   True value in CI: {'‚úÖ Yes' if final_ci[0] <= monitor.true_satisfaction <= final_ci[1] else '‚ùå No'}\")\n",
    "\n",
    "print(f\"\\nüéØ Both theorems working together:\")\n",
    "print(f\"   ‚Ä¢ LLN: Our estimate gets closer to truth with more data\")\n",
    "print(f\"   ‚Ä¢ CLT: We can quantify uncertainty with confidence intervals\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up & Key Takeaways\n",
    "‚úÖ You understand the Central Limit Theorem - the foundation of statistical inference  \n",
    "‚úÖ You've seen CLT work with ANY distribution (exponential, uniform, weird shapes)  \n",
    "‚úÖ You understand the Law of Large Numbers and why sample size matters  \n",
    "‚úÖ You can avoid the Gambler's Fallacy trap  \n",
    "‚úÖ You know how these theorems power real business applications  \n",
    "\n",
    "**Quick Reference Card:**\n",
    "- üé™ **CLT**: Sample means become normal regardless of original distribution\n",
    "- üìè **LLN**: Sample averages approach population mean as n increases\n",
    "- üéØ **Magic Number**: n ‚â• 30 usually sufficient for CLT\n",
    "- üìä **Standard Error**: œÉ/‚àön (precision increases with sample size)\n",
    "- üö´ **Independence**: Each trial is independent (no Gambler's Fallacy!)\n",
    "\n",
    "**Mathematical Foundations Mastered:**\n",
    "- Central Limit Theorem enables confidence intervals and hypothesis testing\n",
    "- Law of Large Numbers justifies using samples to estimate populations\n",
    "- Together, they make statistical inference possible!\n",
    "\n",
    "**Next Steps:** Apply these theorems to hypothesis testing, confidence intervals, and advanced statistical methods!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
