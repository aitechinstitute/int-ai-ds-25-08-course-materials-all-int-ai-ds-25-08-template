{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 04 ¬∑ Notebook 09 ‚Äî A/B Testing Framework: The Science of Better Decisions\n",
    "**Instructor:** Amir Charkhi  |  **Goal:** Master the gold standard of data-driven decision making in tech and business.\n",
    "\n",
    "> Format: short theory ‚Üí quick practice ‚Üí build understanding ‚Üí mini-challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Objectives\n",
    "- Understand A/B testing fundamentals and when to use them\n",
    "- Design statistically sound experiments with proper sample sizes\n",
    "- Apply statistical significance testing and interpret p-values correctly\n",
    "- Calculate statistical power and avoid common testing pitfalls\n",
    "- Build a complete A/B testing framework for real business scenarios\n",
    "- Recognize and avoid the most dangerous A/B testing mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A/B Testing: The Scientific Method for Business\n",
    "**The most powerful tool in data science**: Turn opinions into evidence with controlled experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, chi2_contingency\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for nice plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B Testing fundamentals\n",
    "ab_testing_facts = {\n",
    "    \"üéØ The Goal\": \"Compare two versions to see which performs better\",\n",
    "    \"üß™ The Method\": \"Randomly split users into control (A) and treatment (B) groups\",\n",
    "    \"üìä The Math\": \"Use statistical tests to determine if differences are real or random\",\n",
    "    \"‚öñÔ∏è The Standard\": \"Statistical significance (usually p < 0.05)\",\n",
    "    \"üî¨ The Power\": \"Turns hunches into data-driven decisions\",\n",
    "    \"üíº The Impact\": \"Billions in revenue optimization across tech industry\"\n",
    "}\n",
    "\n",
    "print(\"üöÄ A/B Testing: The Engine of Data-Driven Growth!\")\n",
    "for key, value in ab_testing_facts.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "    \n",
    "print(\"\\nüí° A/B testing is how Google, Facebook, Netflix, and Amazon make billions in optimizations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Anatomy of an A/B Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic A/B test example: Website button color\n",
    "np.random.seed(42)\n",
    "\n",
    "# Test parameters\n",
    "visitors_per_group = 10000\n",
    "control_rate = 0.12      # 12% click rate for blue button\n",
    "treatment_rate = 0.14    # 14% click rate for red button (2% improvement)\n",
    "\n",
    "# Simulate the experiment\n",
    "control_clicks = np.random.binomial(visitors_per_group, control_rate)\n",
    "treatment_clicks = np.random.binomial(visitors_per_group, treatment_rate)\n",
    "\n",
    "print(f\"üîµ Button Color A/B Test Results:\")\n",
    "print(f\"   Control (Blue Button): {control_clicks}/{visitors_per_group} = {control_clicks/visitors_per_group:.1%}\")\n",
    "print(f\"   Treatment (Red Button): {treatment_clicks}/{visitors_per_group} = {treatment_clicks/visitors_per_group:.1%}\")\n",
    "print(f\"   Observed lift: {(treatment_clicks/visitors_per_group - control_clicks/visitors_per_group):.1%}\")\n",
    "print(f\"\\n‚ùì But is this difference statistically significant or just luck?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistical Significance Testing: The Heart of A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The Two-Proportion Z-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our statistical testing function\n",
    "def two_proportion_z_test(successes_a, n_a, successes_b, n_b, alpha=0.05):\n",
    "    \"\"\"Perform two-proportion z-test for A/B testing\"\"\"\n",
    "    \n",
    "    # Calculate proportions\n",
    "    p_a = successes_a / n_a\n",
    "    p_b = successes_b / n_b\n",
    "    \n",
    "    # Pooled proportion (assuming null hypothesis: no difference)\n",
    "    p_pool = (successes_a + successes_b) / (n_a + n_b)\n",
    "    \n",
    "    # Standard error\n",
    "    se = np.sqrt(p_pool * (1 - p_pool) * (1/n_a + 1/n_b))\n",
    "    \n",
    "    # Z-statistic\n",
    "    z_stat = (p_b - p_a) / se\n",
    "    \n",
    "    # P-value (two-tailed test)\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "    \n",
    "    # Confidence interval for difference\n",
    "    se_diff = np.sqrt((p_a * (1 - p_a) / n_a) + (p_b * (1 - p_b) / n_b))\n",
    "    z_critical = stats.norm.ppf(1 - alpha/2)\n",
    "    diff = p_b - p_a\n",
    "    margin = z_critical * se_diff\n",
    "    ci_lower = diff - margin\n",
    "    ci_upper = diff + margin\n",
    "    \n",
    "    return {\n",
    "        'p_a': p_a,\n",
    "        'p_b': p_b,\n",
    "        'difference': diff,\n",
    "        'z_statistic': z_stat,\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < alpha,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper\n",
    "    }\n",
    "\n",
    "print(\"üìä Statistical Testing Function Ready!\")\n",
    "print(\"   This function implements the math behind A/B test significance...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our button color experiment\n",
    "test_results = two_proportion_z_test(control_clicks, visitors_per_group, \n",
    "                                   treatment_clicks, visitors_per_group)\n",
    "\n",
    "print(f\"üî¨ Statistical Analysis Results:\")\n",
    "print(f\"   Control rate: {test_results['p_a']:.3f} ({test_results['p_a']:.1%})\")\n",
    "print(f\"   Treatment rate: {test_results['p_b']:.3f} ({test_results['p_b']:.1%})\")\n",
    "print(f\"   Difference: {test_results['difference']:.3f} ({test_results['difference']:.1%})\")\n",
    "print(f\"   Z-statistic: {test_results['z_statistic']:.3f}\")\n",
    "print(f\"   P-value: {test_results['p_value']:.6f}\")\n",
    "print(f\"   95% CI: [{test_results['ci_lower']:.3f}, {test_results['ci_upper']:.3f}]\")\n",
    "print(f\"\\nüéØ Result: {'‚úÖ Statistically significant!' if test_results['significant'] else '‚ùå Not significant'}\")\n",
    "\n",
    "if test_results['significant']:\n",
    "    print(f\"   The red button IS better - this difference is unlikely due to chance\")\n",
    "else:\n",
    "    print(f\"   Cannot conclude the red button is better - might just be random variation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Understanding P-Values and Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what p-values and confidence intervals mean\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# P-value visualization\n",
    "z_stat = test_results['z_statistic']\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "y = stats.norm.pdf(x)\n",
    "\n",
    "ax1.plot(x, y, 'b-', linewidth=2, label='Standard Normal')\n",
    "ax1.axvline(z_stat, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Z-statistic: {z_stat:.2f}')\n",
    "ax1.axvline(-z_stat, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "\n",
    "# Shade p-value regions\n",
    "x_tail_right = x[x >= abs(z_stat)]\n",
    "y_tail_right = stats.norm.pdf(x_tail_right)\n",
    "ax1.fill_between(x_tail_right, y_tail_right, alpha=0.3, color='red')\n",
    "\n",
    "x_tail_left = x[x <= -abs(z_stat)]\n",
    "y_tail_left = stats.norm.pdf(x_tail_left)\n",
    "ax1.fill_between(x_tail_left, y_tail_left, alpha=0.3, color='red')\n",
    "\n",
    "ax1.set_title(f'P-Value Visualization\\nP-value = {test_results[\"p_value\"]:.4f}')\n",
    "ax1.set_xlabel('Z-score')\n",
    "ax1.set_ylabel('Probability Density')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Confidence interval visualization\n",
    "diff = test_results['difference']\n",
    "ci_lower = test_results['ci_lower']\n",
    "ci_upper = test_results['ci_upper']\n",
    "\n",
    "ax2.errorbar(0, diff, yerr=[[diff - ci_lower], [ci_upper - diff]], \n",
    "            fmt='bo', capsize=10, capthick=3, markersize=10)\n",
    "ax2.axhline(0, color='red', linestyle='--', linewidth=2, alpha=0.7, \n",
    "           label='No Effect (Null Hypothesis)')\n",
    "ax2.set_ylabel('Difference in Conversion Rate')\n",
    "ax2.set_title('95% Confidence Interval\\nfor Treatment Effect')\n",
    "ax2.set_xlim(-0.5, 0.5)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üí° Key Interpretations:\")\n",
    "print(f\"   ‚Ä¢ P-value: Probability of seeing this difference if there's actually no effect\")\n",
    "print(f\"   ‚Ä¢ CI: We're 95% confident the true effect is between {ci_lower:.1%} and {ci_upper:.1%}\")\n",
    "if ci_lower > 0:\n",
    "    print(f\"   ‚Ä¢ Since CI doesn't include 0, we have significant improvement!\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Since CI includes 0, we can't rule out no effect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 ‚Äî Email Subject Line Test (easy)**  \n",
    "Analyze an A/B test comparing email open rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email A/B test scenario\n",
    "# Subject A: \"Don't Miss Out!\" - 15,000 emails sent, 2,100 opened\n",
    "# Subject B: \"Exclusive Offer Inside\" - 15,000 emails sent, 2,400 opened\n",
    "\n",
    "emails_sent = 15000\n",
    "opens_a = 2100\n",
    "opens_b = 2400\n",
    "\n",
    "print(f\"üìß Email Subject Line A/B Test:\")\n",
    "print(f\"   Subject A: {opens_a}/{emails_sent} opened ({opens_a/emails_sent:.1%})\")\n",
    "print(f\"   Subject B: {opens_b}/{emails_sent} opened ({opens_b/emails_sent:.1%})\")\n",
    "\n",
    "# Your task: Use the two_proportion_z_test function to analyze this experiment\n",
    "# Questions to answer:\n",
    "# 1. What are the open rates for each subject line?\n",
    "# 2. Is the difference statistically significant?\n",
    "# 3. What's the confidence interval for the improvement?\n",
    "# 4. What would you recommend to the marketing team?\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Analyze email subject line A/B test\n",
    "email_results = two_proportion_z_test(opens_a, emails_sent, opens_b, emails_sent)\n",
    "\n",
    "print(f\"üìä Email A/B Test Analysis:\")\n",
    "print(f\"   Subject A open rate: {email_results['p_a']:.1%}\")\n",
    "print(f\"   Subject B open rate: {email_results['p_b']:.1%}\")\n",
    "print(f\"   Absolute improvement: {email_results['difference']:.1%}\")\n",
    "print(f\"   Relative improvement: {(email_results['difference']/email_results['p_a']):.1%}\")\n",
    "print(f\"\\nüìà Statistical Results:\")\n",
    "print(f\"   Z-statistic: {email_results['z_statistic']:.3f}\")\n",
    "print(f\"   P-value: {email_results['p_value']:.6f}\")\n",
    "print(f\"   95% CI: [{email_results['ci_lower']:.3f}, {email_results['ci_upper']:.3f}]\")\n",
    "print(f\"   Significant: {'‚úÖ Yes' if email_results['significant'] else '‚ùå No'}\")\n",
    "\n",
    "print(f\"\\nüíº Business Recommendation:\")\n",
    "if email_results['significant']:\n",
    "    improvement = email_results['difference'] * emails_sent\n",
    "    print(f\"   ‚úÖ Use Subject B: 'Exclusive Offer Inside'\")\n",
    "    print(f\"   üìà Expected additional {improvement:.0f} opens per 15K emails\")\n",
    "    print(f\"   üí∞ This represents a {(email_results['difference']/email_results['p_a']):.1%} relative improvement\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è No clear winner - continue testing or collect more data\")\n",
    "    print(f\"   üìä The observed difference could be due to random variation\")\n",
    "    \n",
    "print(f\"\\nüéØ Key Learning: Even small percentage improvements can mean thousands more opens!\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample Size Calculation: Planning for Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Power Analysis and Effect Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size calculator for A/B tests\n",
    "def calculate_ab_sample_size(baseline_rate, minimum_detectable_effect, \n",
    "                           alpha=0.05, power=0.8):\n",
    "    \"\"\"Calculate required sample size per group for A/B test\"\"\"\n",
    "    \n",
    "    # Convert relative effect to absolute\n",
    "    if minimum_detectable_effect > 1:  # Assume it's a percentage if > 1\n",
    "        minimum_detectable_effect = minimum_detectable_effect / 100\n",
    "    \n",
    "    new_rate = baseline_rate * (1 + minimum_detectable_effect)\n",
    "    \n",
    "    # Z-scores for alpha and power\n",
    "    z_alpha = stats.norm.ppf(1 - alpha/2)  # Two-tailed\n",
    "    z_beta = stats.norm.ppf(power)\n",
    "    \n",
    "    # Pooled proportion\n",
    "    p_avg = (baseline_rate + new_rate) / 2\n",
    "    \n",
    "    # Sample size calculation\n",
    "    effect_size = abs(new_rate - baseline_rate)\n",
    "    n = (2 * p_avg * (1 - p_avg) * (z_alpha + z_beta)**2) / (effect_size**2)\n",
    "    \n",
    "    return {\n",
    "        'sample_size_per_group': int(np.ceil(n)),\n",
    "        'total_sample_size': int(np.ceil(2 * n)),\n",
    "        'baseline_rate': baseline_rate,\n",
    "        'target_rate': new_rate,\n",
    "        'effect_size': effect_size,\n",
    "        'relative_improvement': minimum_detectable_effect\n",
    "    }\n",
    "\n",
    "print(\"üéØ A/B Test Sample Size Calculator Ready!\")\n",
    "print(\"   Input your baseline rate and desired improvement to get required sample size...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example calculations for different scenarios\n",
    "scenarios = [\n",
    "    {\"name\": \"Website Conversion\", \"baseline\": 0.05, \"improvement\": 0.20},  # 20% relative improvement\n",
    "    {\"name\": \"Email Open Rate\", \"baseline\": 0.15, \"improvement\": 0.15},     # 15% relative improvement  \n",
    "    {\"name\": \"App Install Rate\", \"baseline\": 0.02, \"improvement\": 0.50},    # 50% relative improvement\n",
    "    {\"name\": \"Purchase Rate\", \"baseline\": 0.08, \"improvement\": 0.25}        # 25% relative improvement\n",
    "]\n",
    "\n",
    "print(f\"üìä Sample Size Requirements for Common A/B Tests:\")\n",
    "print(f\"Test Type              | Baseline | Target  | Per Group | Total   | Days*\")\n",
    "print(f\"-\" * 75)\n",
    "\n",
    "for scenario in scenarios:\n",
    "    calc = calculate_ab_sample_size(scenario['baseline'], scenario['improvement'])\n",
    "    \n",
    "    # Estimate days assuming 1000 visitors per day\n",
    "    daily_visitors = 1000\n",
    "    days_needed = calc['sample_size_per_group'] / (daily_visitors / 2)\n",
    "    \n",
    "    print(f\"{scenario['name']:<20} | {calc['baseline']:>6.1%} | {calc['target_rate']:>5.1%} | \"\n",
    "          f\"{calc['sample_size_per_group']:>7,} | {calc['total_sample_size']:>7,} | {days_needed:>4.0f}\")\n",
    "\n",
    "print(f\"\\n* Days assuming 1,000 visitors per day split 50/50\")\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "print(f\"   ‚Ä¢ Smaller baselines need MUCH larger samples\")\n",
    "print(f\"   ‚Ä¢ Detecting small improvements requires massive sample sizes\")\n",
    "print(f\"   ‚Ä¢ Plan your test duration before starting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Power Curve: Understanding Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how sample size affects statistical power\n",
    "def calculate_power(n_per_group, baseline_rate, effect_size, alpha=0.05):\n",
    "    \"\"\"Calculate statistical power for given sample size\"\"\"\n",
    "    p1 = baseline_rate\n",
    "    p2 = baseline_rate + effect_size\n",
    "    \n",
    "    # Standard error under alternative hypothesis\n",
    "    se = np.sqrt((p1 * (1 - p1) + p2 * (1 - p2)) / n_per_group)\n",
    "    \n",
    "    # Critical value\n",
    "    z_alpha = stats.norm.ppf(1 - alpha/2)\n",
    "    \n",
    "    # Power calculation\n",
    "    z_beta = (effect_size - z_alpha * se) / se\n",
    "    power = stats.norm.cdf(z_beta)\n",
    "    \n",
    "    return max(0, min(1, power))  # Bound between 0 and 1\n",
    "\n",
    "# Plot power curves\n",
    "sample_sizes = np.arange(1000, 20000, 500)\n",
    "baseline = 0.05\n",
    "effect_sizes = [0.01, 0.015, 0.02, 0.025]  # Different absolute improvements\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "colors = ['red', 'orange', 'green', 'blue']\n",
    "for i, effect in enumerate(effect_sizes):\n",
    "    powers = [calculate_power(n, baseline, effect) for n in sample_sizes]\n",
    "    plt.plot(sample_sizes, powers, color=colors[i], linewidth=2, \n",
    "            label=f'{effect:.1%} improvement ({effect/baseline:.0%} relative)')\n",
    "\n",
    "plt.axhline(y=0.8, color='black', linestyle='--', alpha=0.7, \n",
    "           label='80% Power (Standard)')\n",
    "plt.xlabel('Sample Size per Group')\n",
    "plt.ylabel('Statistical Power')\n",
    "plt.title('Power Curves: How Sample Size Affects Ability to Detect Effects')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Power Curve Insights:\")\n",
    "print(\"   ‚Ä¢ Larger effect sizes are easier to detect (need smaller samples)\")\n",
    "print(\"   ‚Ä¢ 80% power is the standard threshold (80% chance to detect a real effect)\")\n",
    "print(\"   ‚Ä¢ Diminishing returns: doubling sample size doesn't double power\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-World A/B Testing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete A/B testing class for real applications\n",
    "class ABTestFramework:\n",
    "    def __init__(self, alpha=0.05, power=0.8):\n",
    "        self.alpha = alpha\n",
    "        self.power = power\n",
    "        self.results = None\n",
    "        \n",
    "    def plan_test(self, baseline_rate, minimum_detectable_effect):\n",
    "        \"\"\"Plan an A/B test with required sample sizes\"\"\"\n",
    "        sample_calc = calculate_ab_sample_size(\n",
    "            baseline_rate, minimum_detectable_effect, self.alpha, self.power\n",
    "        )\n",
    "        \n",
    "        print(f\"üéØ A/B Test Planning:\")\n",
    "        print(f\"   Baseline rate: {sample_calc['baseline_rate']:.1%}\")\n",
    "        print(f\"   Target rate: {sample_calc['target_rate']:.1%}\")\n",
    "        print(f\"   Relative improvement: {sample_calc['relative_improvement']:.1%}\")\n",
    "        print(f\"   Required sample per group: {sample_calc['sample_size_per_group']:,}\")\n",
    "        print(f\"   Total sample needed: {sample_calc['total_sample_size']:,}\")\n",
    "        \n",
    "        return sample_calc\n",
    "    \n",
    "    def analyze_results(self, control_conversions, control_visitors, \n",
    "                       treatment_conversions, treatment_visitors):\n",
    "        \"\"\"Analyze A/B test results\"\"\"\n",
    "        self.results = two_proportion_z_test(\n",
    "            control_conversions, control_visitors,\n",
    "            treatment_conversions, treatment_visitors,\n",
    "            self.alpha\n",
    "        )\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def make_recommendation(self, revenue_per_conversion=None):\n",
    "        \"\"\"Make business recommendation based on test results\"\"\"\n",
    "        if self.results is None:\n",
    "            return \"No test results to analyze\"\n",
    "        \n",
    "        print(f\"\\nüèÜ A/B Test Recommendation:\")\n",
    "        \n",
    "        if self.results['significant'] and self.results['difference'] > 0:\n",
    "            print(f\"   ‚úÖ LAUNCH the treatment version!\")\n",
    "            print(f\"   üìà Improvement: {self.results['difference']:.1%} (statistically significant)\")\n",
    "            print(f\"   üéØ 95% CI: [{self.results['ci_lower']:.1%}, {self.results['ci_upper']:.1%}]\")\n",
    "            \n",
    "            if revenue_per_conversion:\n",
    "                annual_visitors = 1000000  # Assume 1M annual visitors\n",
    "                additional_conversions = annual_visitors * self.results['difference']\n",
    "                revenue_impact = additional_conversions * revenue_per_conversion\n",
    "                print(f\"   üí∞ Estimated annual revenue impact: ${revenue_impact:,.0f}\")\n",
    "                \n",
    "        elif self.results['significant'] and self.results['difference'] < 0:\n",
    "            print(f\"   ‚ùå DO NOT launch - treatment performs WORSE\")\n",
    "            print(f\"   üìâ Decrease: {abs(self.results['difference']):.1%}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è INCONCLUSIVE - no statistically significant difference\")\n",
    "            print(f\"   üìä Observed difference: {self.results['difference']:.1%}\")\n",
    "            print(f\"   üîÑ Consider: Longer test duration, larger effect size, or different variation\")\n",
    "\n",
    "print(\"üöÄ Complete A/B Testing Framework Ready!\")\n",
    "print(\"   This class handles planning, analysis, and recommendations...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 ‚Äî E-commerce Checkout Optimization (medium)**  \n",
    "Use the framework to analyze a complete A/B test scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E-commerce scenario: Testing a simplified checkout process\n",
    "# Current checkout: 15% completion rate\n",
    "# Goal: Detect 20% relative improvement (3% absolute improvement)\n",
    "# Revenue per completed checkout: $45\n",
    "\n",
    "baseline_checkout_rate = 0.15\n",
    "target_improvement = 0.20  # 20% relative improvement\n",
    "revenue_per_checkout = 45\n",
    "\n",
    "print(f\"üõí E-commerce Checkout A/B Test Scenario:\")\n",
    "print(f\"   Current checkout rate: {baseline_checkout_rate:.1%}\")\n",
    "print(f\"   Target improvement: {target_improvement:.1%} relative\")\n",
    "print(f\"   Revenue per checkout: ${revenue_per_checkout}\")\n",
    "\n",
    "# Your tasks using the ABTestFramework:\n",
    "# 1. Plan the test - how many visitors do you need?\n",
    "# 2. Simulate the test results (assume the treatment actually works as hoped)\n",
    "# 3. Analyze the results \n",
    "# 4. Make a business recommendation with revenue impact\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "# Task 1: Plan the test\n",
    "ab_test = ABTestFramework(alpha=0.05, power=0.8)\n",
    "test_plan = ab_test.plan_test(baseline_checkout_rate, target_improvement)\n",
    "\n",
    "# Task 2: Simulate the test results\n",
    "np.random.seed(123)\n",
    "visitors_per_group = test_plan['sample_size_per_group']\n",
    "\n",
    "# Simulate results assuming treatment works as hoped\n",
    "control_checkouts = np.random.binomial(visitors_per_group, baseline_checkout_rate)\n",
    "treatment_checkouts = np.random.binomial(visitors_per_group, test_plan['target_rate'])\n",
    "\n",
    "print(f\"\\nüìä Simulated Test Results:\")\n",
    "print(f\"   Control: {control_checkouts:,} checkouts from {visitors_per_group:,} visitors ({control_checkouts/visitors_per_group:.1%})\")\n",
    "print(f\"   Treatment: {treatment_checkouts:,} checkouts from {visitors_per_group:,} visitors ({treatment_checkouts/visitors_per_group:.1%})\")\n",
    "\n",
    "# Task 3: Analyze the results\n",
    "results = ab_test.analyze_results(control_checkouts, visitors_per_group,\n",
    "                                 treatment_checkouts, visitors_per_group)\n",
    "\n",
    "print(f\"\\nüî¨ Statistical Analysis:\")\n",
    "print(f\"   Control rate: {results['p_a']:.3f} ({results['p_a']:.1%})\")\n",
    "print(f\"   Treatment rate: {results['p_b']:.3f} ({results['p_b']:.1%})\")\n",
    "print(f\"   Absolute improvement: {results['difference']:.3f} ({results['difference']:.1%})\")\n",
    "print(f\"   Relative improvement: {(results['difference']/results['p_a']):.1%}\")\n",
    "print(f\"   P-value: {results['p_value']:.6f}\")\n",
    "print(f\"   Statistically significant: {'‚úÖ Yes' if results['significant'] else '‚ùå No'}\")\n",
    "\n",
    "# Task 4: Make business recommendation\n",
    "ab_test.make_recommendation(revenue_per_checkout)\n",
    "\n",
    "# Additional business insights\n",
    "if results['significant'] and results['difference'] > 0:\n",
    "    print(f\"\\nüìà Additional Business Insights:\")\n",
    "    \n",
    "    # Calculate confidence interval for revenue impact\n",
    "    annual_visitors = 1000000\n",
    "    min_revenue_impact = results['ci_lower'] * annual_visitors * revenue_per_checkout\n",
    "    max_revenue_impact = results['ci_upper'] * annual_visitors * revenue_per_checkout\n",
    "    \n",
    "    print(f\"   üí∞ Revenue impact range: ${min_revenue_impact:,.0f} to ${max_revenue_impact:,.0f} annually\")\n",
    "    print(f\"   üéØ Conservative estimate: ${min_revenue_impact:,.0f} additional annual revenue\")\n",
    "    print(f\"   ‚ö° This justifies significant development investment in checkout optimization!\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Common A/B Testing Pitfalls and How to Avoid Them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 The Multiple Testing Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the multiple testing problem\n",
    "def simulate_multiple_tests(num_tests=20, visitors_per_group=5000, true_effect=0):\n",
    "    \"\"\"Simulate multiple A/B tests with no real effect\"\"\"\n",
    "    baseline_rate = 0.10\n",
    "    false_positives = 0\n",
    "    p_values = []\n",
    "    \n",
    "    print(f\"üé≤ Multiple Testing Simulation: {num_tests} tests with NO real effect\")\n",
    "    print(f\"   Each test: {visitors_per_group:,} visitors per group\")\n",
    "    print(f\"   True effect: {true_effect} (no improvement)\")\n",
    "    print(f\"\\nTest # | Control Rate | Treatment Rate | P-value | Significant?\")\n",
    "    print(f\"-\" * 70)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    for i in range(num_tests):\n",
    "        # Both groups have same true rate (no effect)\n",
    "        control_conversions = np.random.binomial(visitors_per_group, baseline_rate)\n",
    "        treatment_conversions = np.random.binomial(visitors_per_group, baseline_rate + true_effect)\n",
    "        \n",
    "        results = two_proportion_z_test(control_conversions, visitors_per_group,\n",
    "                                      treatment_conversions, visitors_per_group)\n",
    "        \n",
    "        p_values.append(results['p_value'])\n",
    "        \n",
    "        if results['significant']:\n",
    "            false_positives += 1\n",
    "            \n",
    "        significance = \"‚úÖ YES\" if results['significant'] else \"‚ùå No\"\n",
    "        \n",
    "        print(f\"  {i+1:2d}   |    {results['p_a']:6.3f}    |     {results['p_b']:6.3f}     | {results['p_value']:7.4f} | {significance}\")\n",
    "    \n",
    "    false_positive_rate = false_positives / num_tests\n",
    "    \n",
    "    print(f\"\\nüö® Multiple Testing Problem Results:\")\n",
    "    print(f\"   False positives: {false_positives}/{num_tests} ({false_positive_rate:.1%})\")\n",
    "    print(f\"   Expected false positives: ~{0.05 * num_tests:.1f} (5% of tests)\")\n",
    "    print(f\"\\nüí° Problem: With many tests, you'll find 'significant' results by chance!\")\n",
    "    \n",
    "    return p_values, false_positives\n",
    "\n",
    "p_values, false_positives = simulate_multiple_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the p-value distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(p_values, bins=20, alpha=0.7, edgecolor='black', density=True)\n",
    "plt.axvline(0.05, color='red', linestyle='--', linewidth=2, \n",
    "           label='Œ± = 0.05 (Significance Threshold)')\n",
    "plt.xlabel('P-value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('P-value Distribution from Multiple Tests (No Real Effect)\\nShould be Uniform if No Effect Exists')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"üîç Solutions to Multiple Testing Problem:\")\n",
    "print(f\"   1. Bonferroni Correction: Use Œ±/{len(p_values)} = {0.05/len(p_values):.4f} as threshold\")\n",
    "print(f\"   2. False Discovery Rate (FDR) control\")\n",
    "print(f\"   3. Plan fewer, more focused tests\")\n",
    "print(f\"   4. Use holdout periods between tests\")\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "bonferroni_threshold = 0.05 / len(p_values)\n",
    "bonferroni_significant = sum(1 for p in p_values if p < bonferroni_threshold)\n",
    "print(f\"\\n   With Bonferroni: {bonferroni_significant}/{len(p_values)} tests significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Peeking Problem and Sequential Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the peeking problem\n",
    "def simulate_peeking_problem(target_sample_size=10000, peek_interval=500):\n",
    "    \"\"\"Show how continuous monitoring inflates Type I error\"\"\"\n",
    "    baseline_rate = 0.10\n",
    "    true_treatment_rate = 0.10  # No real effect\n",
    "    \n",
    "    peek_points = []\n",
    "    p_values = []\n",
    "    false_discoveries = []\n",
    "    \n",
    "    np.random.seed(789)\n",
    "    \n",
    "    print(f\"üëÄ Peeking Problem Simulation:\")\n",
    "    print(f\"   True effect: 0% (no difference between A and B)\")\n",
    "    print(f\"   Checking results every {peek_interval} visitors\")\n",
    "    print(f\"\\nSample Size | P-value | Significant?\")\n",
    "    print(f\"-\" * 35)\n",
    "    \n",
    "    for n in range(peek_interval, target_sample_size + 1, peek_interval):\n",
    "        # Simulate cumulative results\n",
    "        control_conversions = np.random.binomial(n, baseline_rate)\n",
    "        treatment_conversions = np.random.binomial(n, true_treatment_rate)\n",
    "        \n",
    "        results = two_proportion_z_test(control_conversions, n,\n",
    "                                      treatment_conversions, n)\n",
    "        \n",
    "        peek_points.append(n)\n",
    "        p_values.append(results['p_value'])\n",
    "        false_discoveries.append(results['significant'])\n",
    "        \n",
    "        significance = \"‚úÖ YES\" if results['significant'] else \"‚ùå No\"\n",
    "        \n",
    "        print(f\"   {n:5,}    | {results['p_value']:.4f}  | {significance}\")\n",
    "        \n",
    "        # If someone was \"peeking\", they might stop here!\n",
    "        if results['significant']:\n",
    "            print(f\"         ‚ö†Ô∏è Danger: Might stop test here and claim victory!\")\n",
    "    \n",
    "    any_significant = any(false_discoveries)\n",
    "    print(f\"\\nüö® Peeking Problem Result:\")\n",
    "    print(f\"   Any 'significant' result during monitoring: {'Yes' if any_significant else 'No'}\")\n",
    "    print(f\"   Risk: {len([x for x in false_discoveries if x])}/{len(false_discoveries)} peeks showed significance\")\n",
    "    \n",
    "    return peek_points, p_values\n",
    "\n",
    "peek_points, peek_p_values = simulate_peeking_problem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the peeking problem\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(peek_points, peek_p_values, 'bo-', linewidth=2, markersize=6)\n",
    "plt.axhline(0.05, color='red', linestyle='--', linewidth=2, \n",
    "           label='Œ± = 0.05 (Danger Zone)')\n",
    "plt.ylabel('P-value')\n",
    "plt.title('P-values During Continuous Monitoring (\"Peeking\")\\nNo Real Effect - All Significance is False Positive!')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "# Show cumulative \"significant\" results\n",
    "cumulative_sig = np.cumsum([1 if p < 0.05 else 0 for p in peek_p_values])\n",
    "plt.plot(peek_points, cumulative_sig, 'ro-', linewidth=2, markersize=6)\n",
    "plt.ylabel('Cumulative \"Significant\" Results')\n",
    "plt.xlabel('Sample Size per Group')\n",
    "plt.title('False Discoveries Accumulate with Peeking')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üõ°Ô∏è Solutions to Peeking Problem:\")\n",
    "print(f\"   1. Pre-commit to sample size and only analyze at the end\")\n",
    "print(f\"   2. Use sequential testing methods (more complex statistics)\")\n",
    "print(f\"   3. Adjust Œ± for multiple looks (spend your Œ± budget wisely)\")\n",
    "print(f\"   4. Monitor for guardrail metrics only, not primary metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3 ‚Äî A/B Testing Best Practices Audit (hard)**  \n",
    "Evaluate and fix problematic A/B testing scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B testing scenarios audit\n",
    "# Each scenario has problems - identify and suggest fixes\n",
    "\n",
    "problematic_scenarios = [\n",
    "    {\n",
    "        \"name\": \"Social Media Ad Campaign\",\n",
    "        \"description\": \"Testing 5 different ad creatives simultaneously, checking results daily, planning to stop when one shows p<0.05\",\n",
    "        \"sample_size\": \"1000 per variation\",\n",
    "        \"duration\": \"Until significant result found\",\n",
    "        \"problems\": [],  # You'll identify these\n",
    "        \"solutions\": []  # You'll suggest these\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Email Subject Line Test\", \n",
    "        \"description\": \"Testing new subject line, but sent to existing customers vs new customers due to technical constraints\",\n",
    "        \"sample_size\": \"5000 existing customers (A), 5000 new customers (B)\",\n",
    "        \"duration\": \"One week\",\n",
    "        \"problems\": [],\n",
    "        \"solutions\": []\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Website Redesign\",\n",
    "        \"description\": \"Major site redesign tested for 2 days, showing 2% conversion improvement, team wants to launch immediately\",\n",
    "        \"sample_size\": \"500 per group\",\n",
    "        \"duration\": \"2 days\",\n",
    "        \"problems\": [],\n",
    "        \"solutions\": []\n",
    "    }\n",
    "]\n",
    "\n",
    "# Your task: For each scenario, identify the problems and suggest solutions\n",
    "# Consider issues like:\n",
    "# - Multiple testing\n",
    "# - Peeking/early stopping\n",
    "# - Selection bias\n",
    "# - Insufficient sample size\n",
    "# - Novelty effects\n",
    "# - Seasonal/temporal effects\n",
    "\n",
    "print(\"üîç A/B Testing Best Practices Audit\")\n",
    "print(\"\\nAnalyze each scenario and identify problems + solutions:\")\n",
    "\n",
    "for i, scenario in enumerate(problematic_scenarios, 1):\n",
    "    print(f\"\\n{i}. {scenario['name']}:\")\n",
    "    print(f\"   Description: {scenario['description']}\")\n",
    "    print(f\"   Sample size: {scenario['sample_size']}\")\n",
    "    print(f\"   Duration: {scenario['duration']}\")\n",
    "    print(f\"   Problems: [Your analysis here]\")\n",
    "    print(f\"   Solutions: [Your recommendations here]\")\n",
    "\n",
    "# Your analysis here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "print(\"üîç A/B Testing Best Practices Audit - Analysis:\")\n",
    "\n",
    "audit_results = [\n",
    "    {\n",
    "        \"name\": \"Social Media Ad Campaign\",\n",
    "        \"problems\": [\n",
    "            \"Multiple testing problem (5 variations = inflated Type I error)\",\n",
    "            \"Peeking problem (checking daily, stopping early)\",\n",
    "            \"No pre-planned sample size calculation\",\n",
    "            \"Alpha spending not controlled\"\n",
    "        ],\n",
    "        \"solutions\": [\n",
    "            \"Use Bonferroni correction: Œ± = 0.05/4 = 0.0125 for comparisons\",\n",
    "            \"Pre-commit to sample size using power analysis\",\n",
    "            \"Set fixed test duration (e.g., 2 weeks minimum)\", \n",
    "            \"Consider testing fewer variations or use multi-armed bandit\",\n",
    "            \"Only analyze results at predetermined timepoints\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Email Subject Line Test\",\n",
    "        \"problems\": [\n",
    "            \"Selection bias - different populations (existing vs new customers)\",\n",
    "            \"Confounding variable (customer type) with treatment\",\n",
    "            \"Results won't generalize to overall customer base\",\n",
    "            \"Not truly randomized assignment\"\n",
    "        ],\n",
    "        \"solutions\": [\n",
    "            \"Fix randomization - split both customer types 50/50 between A and B\",\n",
    "            \"Stratified randomization by customer type if needed\",\n",
    "            \"If technical constraints exist, analyze as two separate experiments\",\n",
    "            \"Report results separately for each customer segment\",\n",
    "            \"Consider customer type as a potential effect modifier\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Website Redesign\",\n",
    "        \"problems\": [\n",
    "            \"Severely underpowered (only 500 per group)\",\n",
    "            \"Too short duration (2 days - novelty effect)\",\n",
    "            \"Rushing to launch without proper validation\",\n",
    "            \"Day-of-week effects not controlled\",\n",
    "            \"Major change needs longer evaluation period\"\n",
    "        ],\n",
    "        \"solutions\": [\n",
    "            \"Calculate proper sample size for 2% improvement detection\",\n",
    "            \"Run test for minimum 1-2 weeks to avoid temporal biases\",\n",
    "            \"Include full business cycles (weekdays + weekends)\",\n",
    "            \"Monitor guardrail metrics (bounce rate, time on site)\",\n",
    "            \"Consider gradual rollout instead of immediate full launch\",\n",
    "            \"A/A test first to validate measurement system\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, result in enumerate(audit_results, 1):\n",
    "    print(f\"\\n{i}. {result['name']} - AUDIT RESULTS:\")\n",
    "    \n",
    "    print(f\"   üö® Problems Identified:\")\n",
    "    for problem in result['problems']:\n",
    "        print(f\"      ‚Ä¢ {problem}\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Recommended Solutions:\")\n",
    "    for solution in result['solutions']:\n",
    "        print(f\"      ‚Ä¢ {solution}\")\n",
    "\n",
    "print(f\"\\nüéØ Key Takeaways:\")\n",
    "print(f\"   ‚Ä¢ Always randomize properly - no selection bias\")\n",
    "print(f\"   ‚Ä¢ Pre-plan sample sizes and test duration\")\n",
    "print(f\"   ‚Ä¢ Control for multiple testing when running several tests\")\n",
    "print(f\"   ‚Ä¢ Don't peek at results or stop early\")\n",
    "print(f\"   ‚Ä¢ Consider temporal effects and novelty bias\")\n",
    "print(f\"   ‚Ä¢ Bigger changes need longer validation periods\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Topics: Beyond Basic A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Multi-Armed Bandits: When to Explore vs Exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple epsilon-greedy bandit for comparison with A/B testing\n",
    "class EpsilonGreedyBandit:\n",
    "    def __init__(self, arms, epsilon=0.1):\n",
    "        self.arms = arms  # True conversion rates for each arm\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = np.zeros(len(arms))\n",
    "        self.values = np.zeros(len(arms))\n",
    "        self.total_reward = 0\n",
    "        \n",
    "    def select_arm(self):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore: random arm\n",
    "            return np.random.randint(len(self.arms))\n",
    "        else:\n",
    "            # Exploit: best arm so far\n",
    "            return np.argmax(self.values)\n",
    "    \n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] += 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        value = self.values[chosen_arm]\n",
    "        self.values[chosen_arm] = ((n - 1) / n) * value + (1 / n) * reward\n",
    "        self.total_reward += reward\n",
    "\n",
    "# Compare A/B testing vs bandit\n",
    "def compare_ab_vs_bandit(true_rates, total_visitors=10000):\n",
    "    \"\"\"Compare cumulative regret of A/B test vs epsilon-greedy bandit\"\"\"\n",
    "    \n",
    "    # A/B testing: split traffic evenly\n",
    "    best_arm = np.argmax(true_rates)\n",
    "    best_rate = true_rates[best_arm]\n",
    "    \n",
    "    ab_reward = 0\n",
    "    visitors_per_arm = total_visitors // len(true_rates)\n",
    "    \n",
    "    for i, rate in enumerate(true_rates):\n",
    "        ab_reward += np.random.binomial(visitors_per_arm, rate)\n",
    "    \n",
    "    ab_regret = total_visitors * best_rate - ab_reward\n",
    "    \n",
    "    # Bandit approach\n",
    "    bandit = EpsilonGreedyBandit(true_rates, epsilon=0.1)\n",
    "    bandit_rewards = []\n",
    "    \n",
    "    for t in range(total_visitors):\n",
    "        arm = bandit.select_arm()\n",
    "        reward = np.random.binomial(1, true_rates[arm])\n",
    "        bandit.update(arm, reward)\n",
    "        bandit_rewards.append(reward)\n",
    "    \n",
    "    bandit_regret = total_visitors * best_rate - sum(bandit_rewards)\n",
    "    \n",
    "    return {\n",
    "        'ab_regret': ab_regret,\n",
    "        'bandit_regret': bandit_regret,\n",
    "        'ab_total_reward': ab_reward,\n",
    "        'bandit_total_reward': sum(bandit_rewards),\n",
    "        'bandit_final_values': bandit.values,\n",
    "        'bandit_counts': bandit.counts\n",
    "    }\n",
    "\n",
    "# Example: 3 ad variants with different performance\n",
    "true_conversion_rates = [0.10, 0.12, 0.15]  # Variant C is best\n",
    "comparison = compare_ab_vs_bandit(true_conversion_rates)\n",
    "\n",
    "print(f\"üé∞ A/B Testing vs Multi-Armed Bandit Comparison:\")\n",
    "print(f\"   True conversion rates: {[f'{r:.1%}' for r in true_conversion_rates]}\")\n",
    "print(f\"   Best possible rate: {max(true_conversion_rates):.1%}\")\n",
    "print(f\"\\nüìä Results (10,000 visitors):\")\n",
    "print(f\"   A/B Testing total conversions: {comparison['ab_total_reward']:.0f}\")\n",
    "print(f\"   Bandit total conversions: {comparison['bandit_total_reward']:.0f}\")\n",
    "print(f\"   Bandit advantage: +{comparison['bandit_total_reward'] - comparison['ab_total_reward']:.0f} conversions\")\n",
    "print(f\"\\nüéØ Bandit learned rates: {[f'{r:.3f}' for r in comparison['bandit_final_values']]}\")\n",
    "print(f\"   Visitor allocation: {[f'{int(c)}' for c in comparison['bandit_counts']]}\")\n",
    "\n",
    "print(f\"\\nüí° When to use each:\")\n",
    "print(f\"   ‚Ä¢ A/B Testing: Need statistical certainty, regulatory requirements\")\n",
    "print(f\"   ‚Ä¢ Bandits: Minimize opportunity cost, ongoing optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Bayesian A/B Testing Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Bayesian A/B test using Beta distributions\n",
    "def bayesian_ab_test(alpha_a, beta_a, alpha_b, beta_b, num_samples=10000):\n",
    "    \"\"\"Simple Bayesian A/B test using Beta distributions\"\"\"\n",
    "    \n",
    "    # Sample from posterior distributions\n",
    "    samples_a = np.random.beta(alpha_a, beta_a, num_samples)\n",
    "    samples_b = np.random.beta(alpha_b, beta_b, num_samples)\n",
    "    \n",
    "    # Probability B > A\n",
    "    prob_b_better = np.mean(samples_b > samples_a)\n",
    "    \n",
    "    # Expected lift\n",
    "    expected_lift = np.mean(samples_b - samples_a)\n",
    "    \n",
    "    # Credible interval for the difference\n",
    "    diff_samples = samples_b - samples_a\n",
    "    ci_lower = np.percentile(diff_samples, 2.5)\n",
    "    ci_upper = np.percentile(diff_samples, 97.5)\n",
    "    \n",
    "    return {\n",
    "        'prob_b_better': prob_b_better,\n",
    "        'expected_lift': expected_lift,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'samples_a': samples_a,\n",
    "        'samples_b': samples_b\n",
    "    }\n",
    "\n",
    "# Example with our earlier button test data\n",
    "# Using uninformative prior: Beta(1, 1)\n",
    "prior_alpha, prior_beta = 1, 1\n",
    "\n",
    "# Update with observed data\n",
    "control_successes, control_failures = control_clicks, visitors_per_group - control_clicks\n",
    "treatment_successes, treatment_failures = treatment_clicks, visitors_per_group - treatment_clicks\n",
    "\n",
    "# Posterior parameters\n",
    "alpha_a = prior_alpha + control_successes\n",
    "beta_a = prior_beta + control_failures\n",
    "alpha_b = prior_alpha + treatment_successes  \n",
    "beta_b = prior_beta + treatment_failures\n",
    "\n",
    "bayesian_results = bayesian_ab_test(alpha_a, beta_a, alpha_b, beta_b)\n",
    "\n",
    "print(f\"üéØ Bayesian A/B Test Results:\")\n",
    "print(f\"   Probability B > A: {bayesian_results['prob_b_better']:.1%}\")\n",
    "print(f\"   Expected lift: {bayesian_results['expected_lift']:.3f} ({bayesian_results['expected_lift']:.1%})\")\n",
    "print(f\"   95% Credible Interval: [{bayesian_results['ci_lower']:.3f}, {bayesian_results['ci_upper']:.3f}]\")\n",
    "\n",
    "print(f\"\\nüîÑ Comparison with Frequentist:\")\n",
    "print(f\"   Frequentist p-value: {test_results['p_value']:.4f}\")\n",
    "print(f\"   Bayesian prob B better: {bayesian_results['prob_b_better']:.1%}\")\n",
    "print(f\"   Interpretation: {'Similar' if abs(bayesian_results['prob_b_better'] - 0.5) > 0.4 else 'Different'} conclusions\")\n",
    "\n",
    "print(f\"\\nüí° Bayesian Advantages:\")\n",
    "print(f\"   ‚Ä¢ Direct probability statements\")\n",
    "print(f\"   ‚Ä¢ Can incorporate prior knowledge\")\n",
    "print(f\"   ‚Ä¢ Natural stopping rules\")\n",
    "print(f\"   ‚Ä¢ No p-hacking concerns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Mini-Challenges\n",
    "- **M1 (easy):** Build a sample size calculator with business impact estimates\n",
    "- **M2 (medium):** Create an A/B test monitoring dashboard with alerts\n",
    "- **M3 (hard):** Design a complete testing roadmap for a product launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn - try the challenges!\n",
    "# M1: Calculator that includes revenue projections and confidence intervals\n",
    "# M2: Dashboard that monitors for guardrail metrics and early stopping\n",
    "# M3: Multi-test roadmap with priority ordering and resource allocation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solutions</b></summary>\n",
    "\n",
    "```python\n",
    "# M1 - Business Impact Sample Size Calculator\n",
    "class BusinessImpactCalculator:\n",
    "    def __init__(self):\n",
    "        self.scenarios = []\n",
    "    \n",
    "    def calculate_business_impact(self, baseline_rate, improvement_rate, \n",
    "                                annual_visitors, revenue_per_conversion, \n",
    "                                alpha=0.05, power=0.8):\n",
    "        \"\"\"Calculate sample size with business impact projections\"\"\"\n",
    "        \n",
    "        # Sample size calculation\n",
    "        sample_calc = calculate_ab_sample_size(baseline_rate, improvement_rate, alpha, power)\n",
    "        \n",
    "        # Business impact calculations\n",
    "        annual_revenue_lift = annual_visitors * sample_calc['effect_size'] * revenue_per_conversion\n",
    "        \n",
    "        # Conservative and optimistic scenarios\n",
    "        conservative_improvement = improvement_rate * 0.5  # 50% of hoped improvement\n",
    "        optimistic_improvement = improvement_rate * 1.5   # 150% of hoped improvement\n",
    "        \n",
    "        conservative_revenue = annual_visitors * baseline_rate * conservative_improvement * revenue_per_conversion\n",
    "        optimistic_revenue = annual_visitors * baseline_rate * optimistic_improvement * revenue_per_conversion\n",
    "        \n",
    "        # Test duration and cost\n",
    "        daily_visitors = annual_visitors / 365\n",
    "        test_duration_days = sample_calc['total_sample_size'] / daily_visitors\n",
    "        \n",
    "        return {\n",
    "            'sample_size': sample_calc,\n",
    "            'annual_revenue_lift': annual_revenue_lift,\n",
    "            'conservative_revenue': conservative_revenue,\n",
    "            'optimistic_revenue': optimistic_revenue,\n",
    "            'test_duration_days': test_duration_days,\n",
    "            'opportunity_cost': (test_duration_days / 365) * annual_revenue_lift\n",
    "        }\n",
    "\n",
    "print(\"üíº M1 - Business Impact Calculator:\")\n",
    "calculator = BusinessImpactCalculator()\n",
    "\n",
    "# Example calculation\n",
    "impact = calculator.calculate_business_impact(\n",
    "    baseline_rate=0.05,\n",
    "    improvement_rate=0.20,\n",
    "    annual_visitors=2000000,\n",
    "    revenue_per_conversion=25\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Sample Size & Business Impact:\")\n",
    "print(f\"   Required per group: {impact['sample_size']['sample_size_per_group']:,}\")\n",
    "print(f\"   Test duration: {impact['test_duration_days']:.0f} days\")\n",
    "print(f\"\\nüí∞ Revenue Impact Projections:\")\n",
    "print(f\"   Expected annual lift: ${impact['annual_revenue_lift']:,.0f}\")\n",
    "print(f\"   Conservative scenario: ${impact['conservative_revenue']:,.0f}\")\n",
    "print(f\"   Optimistic scenario: ${impact['optimistic_revenue']:,.0f}\")\n",
    "print(f\"   Opportunity cost during test: ${impact['opportunity_cost']:,.0f}\")\n",
    "\n",
    "# M2 - A/B Test Monitoring Dashboard\n",
    "class ABTestMonitor:\n",
    "    def __init__(self, test_name, planned_sample_size, guardrail_metrics=None):\n",
    "        self.test_name = test_name\n",
    "        self.planned_sample_size = planned_sample_size\n",
    "        self.guardrail_metrics = guardrail_metrics or {}\n",
    "        self.alerts = []\n",
    "    \n",
    "    def check_guardrails(self, control_data, treatment_data):\n",
    "        \"\"\"Check if guardrail metrics are within acceptable bounds\"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        for metric, threshold in self.guardrail_metrics.items():\n",
    "            if metric in control_data and metric in treatment_data:\n",
    "                control_value = control_data[metric]\n",
    "                treatment_value = treatment_data[metric]\n",
    "                \n",
    "                change = (treatment_value - control_value) / control_value\n",
    "                \n",
    "                if abs(change) > threshold:\n",
    "                    alerts.append({\n",
    "                        'metric': metric,\n",
    "                        'change': change,\n",
    "                        'threshold': threshold,\n",
    "                        'severity': 'HIGH' if abs(change) > threshold * 1.5 else 'MEDIUM'\n",
    "                    })\n",
    "        \n",
    "        return alerts\n",
    "    \n",
    "    def generate_report(self, current_sample_size, test_results, guardrail_data):\n",
    "        \"\"\"Generate monitoring report\"\"\"\n",
    "        progress = current_sample_size / self.planned_sample_size\n",
    "        \n",
    "        print(f\"üìà M2 - A/B Test Monitor: {self.test_name}\")\n",
    "        print(f\"   Progress: {progress:.1%} ({current_sample_size:,}/{self.planned_sample_size:,})\")\n",
    "        print(f\"   Current p-value: {test_results.get('p_value', 'N/A'):.4f}\")\n",
    "        print(f\"   Current lift: {test_results.get('difference', 0):.1%}\")\n",
    "        \n",
    "        # Check guardrails\n",
    "        alerts = self.check_guardrails(guardrail_data.get('control', {}), \n",
    "                                     guardrail_data.get('treatment', {}))\n",
    "        \n",
    "        if alerts:\n",
    "            print(f\"\\nüö® GUARDRAIL ALERTS:\")\n",
    "            for alert in alerts:\n",
    "                print(f\"   {alert['severity']}: {alert['metric']} changed by {alert['change']:+.1%}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ All guardrails healthy\")\n",
    "\n",
    "# Example monitoring\n",
    "monitor = ABTestMonitor(\n",
    "    \"Checkout Button Color\", \n",
    "    planned_sample_size=10000,\n",
    "    guardrail_metrics={'bounce_rate': 0.05, 'avg_session_time': 0.10}\n",
    ")\n",
    "\n",
    "# Simulate current status\n",
    "current_results = {'p_value': 0.032, 'difference': 0.021}\n",
    "guardrail_data = {\n",
    "    'control': {'bounce_rate': 0.25, 'avg_session_time': 180},\n",
    "    'treatment': {'bounce_rate': 0.28, 'avg_session_time': 175}  # Bounce rate increased!\n",
    "}\n",
    "\n",
    "monitor.generate_report(7500, current_results, guardrail_data)\n",
    "\n",
    "# M3 - Complete Testing Roadmap\n",
    "class TestingRoadmap:\n",
    "    def __init__(self):\n",
    "        self.tests = []\n",
    "        \n",
    "    def add_test(self, name, impact_score, effort_score, baseline_rate, \n",
    "                improvement_rate, annual_visitors, revenue_per_conversion):\n",
    "        \"\"\"Add test to roadmap with prioritization\"\"\"\n",
    "        \n",
    "        # Calculate business value\n",
    "        annual_lift = annual_visitors * baseline_rate * improvement_rate * revenue_per_conversion\n",
    "        \n",
    "        # Calculate required sample size\n",
    "        sample_calc = calculate_ab_sample_size(baseline_rate, improvement_rate)\n",
    "        test_duration = sample_calc['total_sample_size'] / (annual_visitors / 365)\n",
    "        \n",
    "        # Priority score (impact/effort ratio adjusted for confidence)\n",
    "        confidence_multiplier = min(impact_score / 5, 1)  # Lower confidence for ambitious claims\n",
    "        priority_score = (annual_lift * confidence_multiplier) / (effort_score * test_duration)\n",
    "        \n",
    "        test = {\n",
    "            'name': name,\n",
    "            'impact_score': impact_score,\n",
    "            'effort_score': effort_score,\n",
    "            'annual_lift': annual_lift,\n",
    "            'test_duration': test_duration,\n",
    "            'priority_score': priority_score,\n",
    "            'sample_size': sample_calc['total_sample_size'],\n",
    "            'baseline_rate': baseline_rate,\n",
    "            'improvement_rate': improvement_rate\n",
    "        }\n",
    "        \n",
    "        self.tests.append(test)\n",
    "        return test\n",
    "    \n",
    "    def generate_roadmap(self):\n",
    "        \"\"\"Generate prioritized testing roadmap\"\"\"\n",
    "        sorted_tests = sorted(self.tests, key=lambda x: x['priority_score'], reverse=True)\n",
    "        \n",
    "        print(f\"\\nüó∫Ô∏è M3 - Testing Roadmap (Prioritized):\")\n",
    "        print(f\"Rank | Test Name                    | Priority | Annual Lift | Duration\")\n",
    "        print(f\"-\" * 75)\n",
    "        \n",
    "        for i, test in enumerate(sorted_tests, 1):\n",
    "            print(f\"{i:2d}   | {test['name']:<28} | {test['priority_score']:>8.0f} | \"\n",
    "                  f\"${test['annual_lift']:>9,.0f} | {test['test_duration']:>4.0f}d\")\n",
    "        \n",
    "        return sorted_tests\n",
    "\n",
    "# Build example roadmap\n",
    "roadmap = TestingRoadmap()\n",
    "\n",
    "# Add potential tests\n",
    "test_ideas = [\n",
    "    (\"Checkout Button Color\", 3, 1, 0.15, 0.10, 1000000, 45),\n",
    "    (\"Product Page Redesign\", 5, 4, 0.08, 0.25, 800000, 60),  \n",
    "    (\"Email Subject Optimization\", 2, 1, 0.20, 0.15, 500000, 25),\n",
    "    (\"Mobile App Onboarding\", 4, 3, 0.12, 0.30, 600000, 35),\n",
    "    (\"Pricing Page Layout\", 5, 2, 0.05, 0.40, 1200000, 80)\n",
    "]\n",
    "\n",
    "for test_idea in test_ideas:\n",
    "    roadmap.add_test(*test_idea)\n",
    "\n",
    "prioritized_tests = roadmap.generate_roadmap()\n",
    "\n",
    "print(f\"\\nüìã Implementation Notes:\")\n",
    "print(f\"   ‚Ä¢ Run tests sequentially to avoid interaction effects\")\n",
    "print(f\"   ‚Ä¢ Allow 1-week buffer between tests for data collection\")\n",
    "print(f\"   ‚Ä¢ Monitor guardrail metrics for all tests\")\n",
    "print(f\"   ‚Ä¢ Re-evaluate roadmap after each major test result\")\n",
    "\n",
    "total_duration = sum(test['test_duration'] for test in prioritized_tests[:3])\n",
    "total_potential_lift = sum(test['annual_lift'] for test in prioritized_tests[:3])\n",
    "print(f\"\\nüéØ Top 3 tests: {total_duration:.0f} days, ${total_potential_lift:,.0f} potential annual lift\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up & The Road Ahead\n",
    "‚úÖ You understand A/B testing fundamentals and statistical significance  \n",
    "‚úÖ You can design experiments with proper sample sizes and power  \n",
    "‚úÖ You know how to avoid the most dangerous A/B testing pitfalls  \n",
    "‚úÖ You can build complete testing frameworks for real business scenarios  \n",
    "‚úÖ You've glimpsed advanced topics like bandits and Bayesian methods  \n",
    "\n",
    "**Quick Reference Card:**\n",
    "- üéØ **Goal**: Compare two versions scientifically\n",
    "- üìä **Process**: Randomize ‚Üí Measure ‚Üí Test significance\n",
    "- ‚öñÔ∏è **Standard**: p < 0.05, power ‚â• 80%\n",
    "- üö´ **Avoid**: Peeking, multiple testing, selection bias\n",
    "- üíº **Impact**: Calculate sample sizes with business metrics\n",
    "- üî¨ **Advanced**: Consider bandits and Bayesian approaches\n",
    "\n",
    "## üéì Statistics Journey Complete!\n",
    "\n",
    "You've now mastered the complete statistical foundation for AI and Data Science:\n",
    "\n",
    "### Your Statistical Toolkit:\n",
    "1. **üìä Data Visualization** - Tell stories with data\n",
    "2. **üìà Descriptive Statistics** - Summarize and understand distributions  \n",
    "3. **üìä Probability** - Model uncertainty and random events\n",
    "4. **üîó Correlation** - Find relationships (and avoid causation traps)\n",
    "5. **üë• Sampling** - Make big conclusions from small samples\n",
    "6. **üé™ Central Limit Theorem** - The magic that makes inference possible\n",
    "7. **üß™ A/B Testing** - The gold standard for making data-driven decisions\n",
    "\n",
    "### Real-World Applications:\n",
    "- Design experiments that drive business growth\n",
    "- Calculate confidence intervals for key metrics\n",
    "- Detect statistical significance in comparisons\n",
    "- Avoid bias and statistical pitfalls\n",
    "- Build complete testing frameworks\n",
    "- Make million-dollar decisions with data\n",
    "\n",
    "### Next Steps in Your Journey:\n",
    "1. **Machine Learning Foundations** - Use statistics for predictive modeling\n",
    "2. **Advanced Experimentation** - Multi-armed bandits, Bayesian methods\n",
    "3. **Causal Inference** - Move beyond correlation to causation\n",
    "4. **Time Series Analysis** - Handle temporal data patterns\n",
    "5. **Deep Learning** - Neural networks built on statistical foundations\n",
    "\n",
    "### Your Data Science Superpower:\n",
    "You now have the statistical foundation to:\n",
    "- ‚úÖ Design rigorous experiments\n",
    "- ‚úÖ Quantify uncertainty in predictions  \n",
    "- ‚úÖ Make data-driven business decisions\n",
    "- ‚úÖ Communicate findings with confidence\n",
    "- ‚úÖ Avoid statistical traps that fool others\n",
    "\n",
    "**Remember**: Every major tech company (Google, Facebook, Amazon, Netflix) relies heavily on A/B testing and statistical methods. You now have the same tools they use to optimize products and drive billions in revenue.\n",
    "\n",
    "üöÄ **Go forth and experiment! The data-driven future awaits.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}