{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Stacking Ensemble - Combining Models for Maximum Power\n",
    "**Instructor:** Amir Charkhi | **Dataset:** Bank Marketing (UCI)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ The Big Idea: Model Democracy\n",
    "\n",
    "**Single Model:** One expert makes the decision\n",
    "```\n",
    "Random Forest ‚Üí Prediction ‚úÖ\n",
    "```\n",
    "\n",
    "**Simple Voting:** Multiple experts vote (majority wins)\n",
    "```\n",
    "Logistic Regression ‚Üí Vote: Yes\n",
    "Random Forest       ‚Üí Vote: No\n",
    "SVM                ‚Üí Vote: Yes\n",
    "                     ‚Üì\n",
    "Final: Yes (2 vs 1)\n",
    "```\n",
    "\n",
    "**Stacking:** A meta-expert learns from other experts!\n",
    "```\n",
    "Logistic Regression ‚Üí Probability: 0.7\n",
    "Random Forest       ‚Üí Probability: 0.3\n",
    "SVM                ‚Üí Probability: 0.8\n",
    "                     ‚Üì\n",
    "Meta-Model (trained on these predictions)\n",
    "  \"I've learned that when LR and SVM agree high,\n",
    "   but RF says low, trust LR and SVM more...\"\n",
    "                     ‚Üì\n",
    "Final Smart Prediction: 0.75 ‚Üí Yes\n",
    "```\n",
    "\n",
    "**Key Insight:** The meta-model learns WHEN to trust each base model!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Base models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Stacking\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "# Plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Data (Quick Prep)\n",
    "\n",
    "**Classification Dataset:**\n",
    "- **Goal:** Binary classification task\n",
    "- **Features:** 20 numeric features\n",
    "- **5,000 samples** with imbalanced classes (85/15 split)\n",
    "- Mimics real-world scenarios like customer churn, fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data - using sklearn's built-in dataset for reliability\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate synthetic classification dataset (mimics bank marketing characteristics)\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    weights=[0.85, 0.15],  # Imbalanced like real bank data\n",
    "    flip_y=0.02,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "feature_names = [f'feature_{i}' for i in range(20)]\n",
    "X = pd.DataFrame(X, columns=feature_names)\n",
    "y = pd.Series(y, name='target')\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ Data ready: {len(X_train):,} train, {len(X_test):,} test\")\n",
    "print(f\"üìä Class balance: {y.mean():.1%} positive class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Train Individual Base Models\n",
    "\n",
    "### üéØ Strategy: Choose DIVERSE Models\n",
    "\n",
    "**Why diversity matters:**\n",
    "```\n",
    "If all models make the SAME errors:\n",
    "  Model A: Wrong on cases [1, 2, 3]\n",
    "  Model B: Wrong on cases [1, 2, 3]  ‚Üê No help!\n",
    "  Model C: Wrong on cases [1, 2, 3]\n",
    "\n",
    "If models make DIFFERENT errors:\n",
    "  Model A: Wrong on cases [1, 2]\n",
    "  Model B: Wrong on cases [3, 4]     ‚Üê Can combine!\n",
    "  Model C: Wrong on cases [5, 6]\n",
    "  \n",
    "Meta-model learns: \"Use A for cases like 3,4,5,6\"\n",
    "```\n",
    "\n",
    "**Our diverse base models:**\n",
    "- **Logistic Regression** - Linear, fast, interpretable\n",
    "- **Random Forest** - Non-linear, robust\n",
    "- **SVM** - Maximum margin, kernel trick\n",
    "- **KNN** - Instance-based, local patterns\n",
    "- **Gradient Boosting** - Sequential, error correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models\n",
    "base_models = [\n",
    "    ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "    ('svm', SVC(probability=True, random_state=42)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "]\n",
    "\n",
    "# Train and evaluate each\n",
    "base_results = []\n",
    "\n",
    "for name, model in base_models:\n",
    "    # Train\n",
    "    if name in ['lr', 'svm', 'knn']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    base_results.append({\n",
    "        'Model': name.upper(),\n",
    "        'Accuracy': accuracy,\n",
    "        'ROC-AUC': roc_auc\n",
    "    })\n",
    "\n",
    "base_df = pd.DataFrame(base_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Base Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize base models\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Accuracy',\n",
    "    x=base_df['Model'],\n",
    "    y=base_df['Accuracy'],\n",
    "    text=[f\"{x:.3f}\" for x in base_df['Accuracy']],\n",
    "    textposition='auto',\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='ROC-AUC',\n",
    "    x=base_df['Model'],\n",
    "    y=base_df['ROC-AUC'],\n",
    "    text=[f\"{x:.3f}\" for x in base_df['ROC-AUC']],\n",
    "    textposition='auto',\n",
    "    marker_color='lightcoral'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Individual Base Model Performance',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    template='plotly_white',\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nüìä Best single model: {base_df.loc[base_df['ROC-AUC'].idxmax(), 'Model']}\")\n",
    "print(f\"   ROC-AUC: {base_df['ROC-AUC'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Understanding Model Diversity\n",
    "\n",
    "### üîç Do Models Make Different Errors?\n",
    "\n",
    "**Key question:** Are predictions correlated?\n",
    "- **High correlation:** Models agree (less benefit from stacking)\n",
    "- **Low correlation:** Models disagree (great for stacking!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from each model\n",
    "predictions = {}\n",
    "\n",
    "for name, model in base_models:\n",
    "    if name in ['lr', 'svm', 'knn']:\n",
    "        pred = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        pred = model.predict_proba(X_test)[:, 1]\n",
    "    predictions[name.upper()] = pred\n",
    "\n",
    "pred_df = pd.DataFrame(predictions)\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = pred_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=correlation.values,\n",
    "    x=correlation.columns,\n",
    "    y=correlation.columns,\n",
    "    text=np.round(correlation.values, 2),\n",
    "    texttemplate='%{text}',\n",
    "    textfont={\"size\": 12},\n",
    "    colorscale='RdBu_r',\n",
    "    zmid=0,\n",
    "    colorbar=dict(title='Correlation')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Base Model Prediction Correlation<br>Lower = More Diverse = Better for Stacking!',\n",
    "    template='plotly_white',\n",
    "    height=500,\n",
    "    xaxis=dict(side='bottom')\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Calculate average correlation (excluding diagonal)\n",
    "mask = np.ones_like(correlation, dtype=bool)\n",
    "np.fill_diagonal(mask, False)\n",
    "avg_corr = correlation.values[mask].mean()\n",
    "\n",
    "print(f\"\\nüìä Average correlation: {avg_corr:.3f}\")\n",
    "if avg_corr < 0.7:\n",
    "    print(\"‚úÖ Good diversity! Models disagree enough for effective stacking.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è High correlation - models are similar. Stacking may have limited benefit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Build Stacking Ensemble\n",
    "\n",
    "### üèóÔ∏è Stacking Architecture\n",
    "\n",
    "```\n",
    "LAYER 0: Original Features\n",
    "         [age, job, balance, ...]\n",
    "                  ‚Üì\n",
    "LAYER 1: Base Models (trained on original features)\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ    LR    ‚îÇ    RF    ‚îÇ   SVM    ‚îÇ   KNN    ‚îÇ    GB    ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ\n",
    "       P=0.7      P=0.3      P=0.8      P=0.4      P=0.9\n",
    "                  ‚Üì\n",
    "LAYER 2: Meta-Model (trained on base model predictions)\n",
    "         [0.7, 0.3, 0.8, 0.4, 0.9] ‚Üí Meta-Learner\n",
    "                  ‚Üì\n",
    "         Final Prediction: 0.75\n",
    "```\n",
    "\n",
    "**Important:** Meta-model is trained on **out-of-fold predictions** to prevent overfitting!\n",
    "\n",
    "### üéØ Choosing the Meta-Model\n",
    "\n",
    "**Options:**\n",
    "- **Logistic Regression** ‚Üê Most common (simple, interpretable)\n",
    "- **Random Forest** (can capture non-linear combinations)\n",
    "- **Gradient Boosting** (powerful, but risk of overfitting)\n",
    "- **Neural Network** (maximum flexibility)\n",
    "\n",
    "**We'll use Logistic Regression** - learns linear weights for each base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stacking classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    cv=5,  # 5-fold CV for meta-features\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Note: StackingClassifier automatically handles:\n",
    "# 1. Training base models\n",
    "# 2. Generating out-of-fold predictions\n",
    "# 3. Training meta-model on those predictions\n",
    "\n",
    "# Train on mixed data (some need scaling, some don't)\n",
    "# For simplicity, we'll use scaled data for all\n",
    "stacking_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_stack = stacking_clf.predict(X_test_scaled)\n",
    "y_prob_stack = stacking_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "accuracy_stack = accuracy_score(y_test, y_pred_stack)\n",
    "roc_auc_stack = roc_auc_score(y_test, y_prob_stack)\n",
    "\n",
    "print(\"\\nüèÜ Stacking Ensemble Performance:\")\n",
    "print(f\"   Accuracy: {accuracy_stack:.4f}\")\n",
    "print(f\"   ROC-AUC:  {roc_auc_stack:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Compare: Base Models vs Stacking\n",
    "\n",
    "### üìä The Moment of Truth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add stacking to results\n",
    "comparison = base_df.copy()\n",
    "comparison = pd.concat([comparison, pd.DataFrame([{\n",
    "    'Model': 'STACKING',\n",
    "    'Accuracy': accuracy_stack,\n",
    "    'ROC-AUC': roc_auc_stack\n",
    "}])], ignore_index=True)\n",
    "\n",
    "# Sort by ROC-AUC\n",
    "comparison = comparison.sort_values('ROC-AUC', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = ['lightblue'] * len(base_df) + ['gold']\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=comparison['ROC-AUC'],\n",
    "    y=comparison['Model'],\n",
    "    orientation='h',\n",
    "    text=[f\"{x:.4f}\" for x in comparison['ROC-AUC']],\n",
    "    textposition='auto',\n",
    "    marker_color=colors,\n",
    "    marker_line_color='black',\n",
    "    marker_line_width=[1] * len(base_df) + [3]\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='üèÜ Stacking vs Individual Models (ROC-AUC)',\n",
    "    xaxis_title='ROC-AUC Score',\n",
    "    yaxis_title='Model',\n",
    "    template='plotly_white',\n",
    "    height=450,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Calculate improvement\n",
    "best_base = comparison[comparison['Model'] != 'STACKING']['ROC-AUC'].max()\n",
    "improvement = roc_auc_stack - best_base\n",
    "pct_improvement = (improvement / best_base) * 100\n",
    "\n",
    "print(f\"\\nüìà Performance Gain:\")\n",
    "print(f\"   Best base model:  {best_base:.4f}\")\n",
    "print(f\"   Stacking:         {roc_auc_stack:.4f}\")\n",
    "print(f\"   Improvement:      +{improvement:.4f} ({pct_improvement:+.2f}%)\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(\"\\n‚úÖ Stacking wins! Meta-model successfully combines base models.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No improvement. Possible reasons: high correlation or overfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Understanding the Meta-Model\n",
    "\n",
    "### üîç What Did the Meta-Model Learn?\n",
    "\n",
    "The meta-model (Logistic Regression) assigns **weights** to each base model.\n",
    "\n",
    "**Interpretation:**\n",
    "- **Positive weight:** Trust this model's predictions\n",
    "- **Larger weight:** Trust it more\n",
    "- **Negative weight:** Inverse relationship (rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract meta-model coefficients\n",
    "meta_model = stacking_clf.final_estimator_\n",
    "coefficients = meta_model.coef_[0]\n",
    "model_names = [name.upper() for name, _ in base_models]\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Weight': coefficients,\n",
    "    'Abs_Weight': np.abs(coefficients)\n",
    "}).sort_values('Weight', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize meta-model weights\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = ['red' if x < 0 else 'green' for x in coef_df['Weight']]\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=coef_df['Weight'],\n",
    "    y=coef_df['Model'],\n",
    "    orientation='h',\n",
    "    text=[f\"{x:.3f}\" for x in coef_df['Weight']],\n",
    "    textposition='auto',\n",
    "    marker_color=colors\n",
    "))\n",
    "\n",
    "fig.add_vline(x=0, line_dash=\"dash\", line_color=\"gray\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Meta-Model Weights: How Much to Trust Each Base Model',\n",
    "    xaxis_title='Weight (Higher = More Influence)',\n",
    "    yaxis_title='Base Model',\n",
    "    template='plotly_white',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüìä Meta-Model Insights:\")\n",
    "most_trusted = coef_df.loc[coef_df['Abs_Weight'].idxmax()]\n",
    "print(f\"   Most trusted model: {most_trusted['Model']} (weight: {most_trusted['Weight']:.3f})\")\n",
    "print(f\"\\nüí° The meta-model learned which base models to trust for different cases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Visualize Prediction Agreement\n",
    "\n",
    "### üéØ When Do Models Agree vs Disagree?\n",
    "\n",
    "Stacking shines when base models disagree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get binary predictions from each model\n",
    "binary_preds = {}\n",
    "for name, model in base_models:\n",
    "    if name in ['lr', 'svm', 'knn']:\n",
    "        binary_preds[name.upper()] = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        binary_preds[name.upper()] = model.predict(X_test)\n",
    "\n",
    "binary_df = pd.DataFrame(binary_preds)\n",
    "\n",
    "# Calculate agreement (how many models agree)\n",
    "agreement = binary_df.sum(axis=1)  # Count of 'yes' votes\n",
    "\n",
    "# Add stacking prediction\n",
    "binary_df['STACKING'] = y_pred_stack\n",
    "binary_df['Agreement'] = agreement\n",
    "binary_df['True_Label'] = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize agreement distribution\n",
    "agreement_counts = agreement.value_counts().sort_index()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=agreement_counts.index,\n",
    "    y=agreement_counts.values,\n",
    "    text=agreement_counts.values,\n",
    "    textposition='auto',\n",
    "    marker_color=['#E74C3C', '#E67E22', '#F39C12', '#52BE80', '#3498DB', '#9B59B6']\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Agreement Distribution<br>How Many Base Models Predict Positive Class?',\n",
    "    xaxis_title='Number of Models Voting \"Yes\"',\n",
    "    yaxis_title='Number of Test Cases',\n",
    "    template='plotly_white',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nüìä Agreement Analysis:\")\n",
    "print(f\"   Cases with full agreement (0 or 5 votes): {((agreement == 0) | (agreement == 5)).sum()}\")\n",
    "print(f\"   Cases with split decisions (2-3 votes): {((agreement == 2) | (agreement == 3)).sum()}\")\n",
    "print(f\"\\nüí° Split decisions are where stacking adds the most value!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. When to Use Stacking\n",
    "\n",
    "### ‚úÖ Use Stacking When:\n",
    "\n",
    "**1. You Have Diverse Base Models**\n",
    "```\n",
    "‚úÖ Different algorithms (linear, tree, instance-based)\n",
    "‚úÖ Different feature subsets\n",
    "‚úÖ Different hyperparameters\n",
    "‚úÖ Low correlation between predictions\n",
    "```\n",
    "\n",
    "**2. You Need Maximum Performance**\n",
    "```\n",
    "‚úÖ Competitions (Kaggle)\n",
    "‚úÖ Critical applications\n",
    "‚úÖ Small performance gains matter\n",
    "‚úÖ Have computational resources\n",
    "```\n",
    "\n",
    "**3. You Have Enough Data**\n",
    "```\n",
    "‚úÖ Large dataset (>10K samples)\n",
    "‚úÖ Can afford train/validation split\n",
    "‚úÖ Avoid overfitting\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Don't Use Stacking When:\n",
    "\n",
    "**1. Base Models Are Too Similar**\n",
    "```\n",
    "‚ùå All tree-based models\n",
    "‚ùå Same algorithm, slightly different hyperparameters\n",
    "‚ùå High correlation (>0.9)\n",
    "‚Üí Simple voting ensemble is enough\n",
    "```\n",
    "\n",
    "**2. Limited Resources**\n",
    "```\n",
    "‚ùå Need fast predictions\n",
    "‚ùå Limited memory\n",
    "‚ùå Limited training time\n",
    "‚Üí Use best single model\n",
    "```\n",
    "\n",
    "**3. Small Dataset**\n",
    "```\n",
    "‚ùå <5K samples\n",
    "‚ùå Risk of overfitting\n",
    "‚ùå Not enough data for meta-model\n",
    "‚Üí Stick with single model + CV\n",
    "```\n",
    "\n",
    "**4. Need Interpretability**\n",
    "```\n",
    "‚ùå Must explain predictions\n",
    "‚ùå Regulatory requirements\n",
    "‚ùå Medical/legal applications\n",
    "‚Üí Use interpretable single model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Stacking vs Other Ensembles:\n",
    "\n",
    "| Method | How It Works | Pros | Cons |\n",
    "|--------|--------------|------|------|\n",
    "| **Voting** | Average/majority vote | Simple, fast | No learning |\n",
    "| **Bagging** | Bootstrap + average (e.g., Random Forest) | Reduces variance | Same algorithm |\n",
    "| **Boosting** | Sequential error correction (e.g., XGBoost) | Powerful | Risk overfitting |\n",
    "| **Stacking** | Meta-model learns combination | Maximum performance | Complex, slow |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "### üéØ Core Concepts:\n",
    "\n",
    "**1. Stacking = Two-Level Learning**\n",
    "- Level 1: Base models learn from data\n",
    "- Level 2: Meta-model learns from base models\n",
    "- **Key:** Uses out-of-fold predictions to avoid overfitting\n",
    "\n",
    "**2. Diversity Is Everything**\n",
    "- Choose different algorithm families\n",
    "- Low correlation = high diversity = better stacking\n",
    "- If models always agree, stacking won't help much\n",
    "\n",
    "**3. Meta-Model Choices**\n",
    "- **Logistic Regression:** Simple, linear combination (most common)\n",
    "- **Random Forest:** Can learn non-linear combinations\n",
    "- **Neural Network:** Maximum flexibility (for complex patterns)\n",
    "\n",
    "**4. Trade-offs**\n",
    "- **Pro:** Best possible performance\n",
    "- **Con:** Slower, more complex, harder to interpret\n",
    "- **Use:** When that extra 1-2% matters!\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Practical Tips:\n",
    "\n",
    "1. **Start with 3-5 diverse base models** (more isn't always better)\n",
    "2. **Ensure base models are well-tuned** (garbage in, garbage out)\n",
    "3. **Use cross-validation** for meta-features (prevents leakage)\n",
    "4. **Monitor for overfitting** (meta-model can overfit to base predictions)\n",
    "5. **Compare to best single model** (is complexity worth it?)\n",
    "\n",
    "---\n",
    "\n",
    "### üèÜ Our Results:\n",
    "\n",
    "- Combined 5 diverse models (LR, RF, SVM, KNN, GB)\n",
    "- Stacking achieved **{:.4f} ROC-AUC**\n",
    "- Improved by **{:+.2f}%** over best single model\n",
    "- Meta-model learned optimal weights for each base model\n",
    "\n",
    "**Stacking successfully combined the strengths of all models!**\n",
    "\n",
    "---\n",
    "\n",
    "**AI Tech Institute** | *Building Tomorrow's AI Engineers Today*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
