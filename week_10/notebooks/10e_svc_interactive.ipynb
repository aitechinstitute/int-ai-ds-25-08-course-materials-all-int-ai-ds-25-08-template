{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** Â· *Intermediate AI & Data Science*\n",
    "### Support Vector Classifier (SVC) - Interactive Deep Dive\n",
    "**Instructor:** Amir Charkhi | **Dataset:** Breast Cancer Wisconsin (UCI)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š What You'll Learn\n",
    "\n",
    "- How SVC finds the optimal decision boundary\n",
    "- The role of support vectors\n",
    "- Kernel trick for non-linear problems\n",
    "- Hyperparameter tuning (C, gamma, kernel)\n",
    "- Real-world application: Cancer diagnosis\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ The Big Idea: Maximum Margin Classifier\n",
    "\n",
    "Imagine you're separating red and blue marbles on a table:\n",
    "\n",
    "```\n",
    "Bad separator:                Good separator (SVM):\n",
    "                              \n",
    "ğŸ”´ ğŸ”´ | ğŸ”µ ğŸ”µ                 ğŸ”´ ğŸ”´    |    ğŸ”µ ğŸ”µ\n",
    "ğŸ”´ ğŸ”´ | ğŸ”µ ğŸ”µ                 ğŸ”´ ğŸ”´    |    ğŸ”µ ğŸ”µ\n",
    "      â†‘                                â†‘\n",
    "  Too close!                    Maximum margin!\n",
    "```\n",
    "\n",
    "**SVC finds the line (or hyperplane) that maximizes the distance to the nearest points from both classes.**\n",
    "\n",
    "These nearest points are called **support vectors** - they \"support\" the decision boundary!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Plotly for interactive visualizations\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Real Medical Data\n",
    "\n",
    "**Dataset:** Breast Cancer Wisconsin (Diagnostic)\n",
    "\n",
    "**Context:** Features computed from digitized images of breast mass:\n",
    "- Radius, texture, perimeter, area, smoothness, etc.\n",
    "- **Goal:** Predict if tumor is benign or malignant\n",
    "- **569 patients**, 30 features\n",
    "\n",
    "**Real-world impact:** Early cancer detection saves lives! ğŸ—ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='diagnosis')\n",
    "\n",
    "# Create readable labels\n",
    "y_labels = y.map({0: 'Malignant', 1: 'Benign'})\n",
    "\n",
    "# Display info\n",
    "info_df = pd.DataFrame({\n",
    "    'Metric': ['Total Samples', 'Features', 'Benign Cases', 'Malignant Cases', 'Class Balance'],\n",
    "    'Value': [\n",
    "        len(X),\n",
    "        X.shape[1],\n",
    "        (y == 1).sum(),\n",
    "        (y == 0).sum(),\n",
    "        f\"{(y == 1).sum() / len(y):.1%} Benign\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"ğŸ¥ Breast Cancer Wisconsin Dataset\")\n",
    "print(\"=\"*50)\n",
    "for _, row in info_df.iterrows():\n",
    "    print(f\"{row['Metric']:.<30} {row['Value']}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview features\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Explore Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive class distribution\n",
    "class_counts = y_labels.value_counts()\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=class_counts.index,\n",
    "        y=class_counts.values,\n",
    "        text=class_counts.values,\n",
    "        textposition='auto',\n",
    "        marker_color=['#FF6B6B', '#4ECDC4']\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Class Distribution: Benign vs Malignant',\n",
    "    xaxis_title='Diagnosis',\n",
    "    yaxis_title='Number of Cases',\n",
    "    template='plotly_white',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Relationships\n",
    "\n",
    "Let's visualize how different tumor characteristics separate the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization dataframe\n",
    "viz_df = X[['mean radius', 'mean texture', 'mean area', 'mean smoothness']].copy()\n",
    "viz_df['diagnosis'] = y_labels\n",
    "\n",
    "# Interactive scatter matrix\n",
    "fig = px.scatter_matrix(\n",
    "    viz_df,\n",
    "    dimensions=['mean radius', 'mean texture', 'mean area', 'mean smoothness'],\n",
    "    color='diagnosis',\n",
    "    title='Feature Relationships: Can We Separate the Classes?',\n",
    "    height=700,\n",
    "    color_discrete_map={'Benign': '#4ECDC4', 'Malignant': '#FF6B6B'}\n",
    ")\n",
    "\n",
    "fig.update_traces(diagonal_visible=False, showupperhalf=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ’¡ Observation:** Some feature pairs show clear separation! SVC will find optimal boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Prepare Data for SVC\n",
    "\n",
    "### ğŸ¯ Critical Step: Feature Scaling\n",
    "\n",
    "**Why scale for SVC?**\n",
    "- SVC uses distances between points\n",
    "- Features with large ranges dominate the calculation\n",
    "- Example: `mean area` (range: 143-2501) vs `mean smoothness` (range: 0.05-0.16)\n",
    "\n",
    "**Without scaling:** Area would dominate, smoothness would be ignored  \n",
    "**With scaling:** All features equally important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Visualize Scaling Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare before/after scaling\n",
    "feature_name = 'mean radius'\n",
    "feature_idx = list(X.columns).index(feature_name)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=['Before Scaling', 'After Scaling'])\n",
    "\n",
    "# Before scaling\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=X_train[feature_name], name='Original', marker_color='lightblue'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# After scaling\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=X_train_scaled[:, feature_idx], name='Scaled', marker_color='lightgreen'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(title_text=f'Feature Scaling: {feature_name}', showlegend=False, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Linear SVC: When Data is Linearly Separable\n",
    "\n",
    "### ğŸ” Linear Kernel\n",
    "\n",
    "Finds a straight line (or hyperplane) to separate classes:\n",
    "\n",
    "```\n",
    "Decision function: f(x) = wÂ·x + b\n",
    "\n",
    "Where:\n",
    "  w = weights (normal vector to hyperplane)\n",
    "  b = bias (intercept)\n",
    "  \n",
    "Prediction: sign(f(x))\n",
    "  > 0 â†’ Class 1\n",
    "  < 0 â†’ Class 0\n",
    "```\n",
    "\n",
    "### ğŸ“Š Hyperparameter: C (Regularization)\n",
    "\n",
    "**C controls the trade-off:**\n",
    "- **Small C (e.g., 0.1):** Wide margin, tolerates misclassifications â†’ **Soft margin**\n",
    "- **Large C (e.g., 100):** Narrow margin, fewer misclassifications â†’ **Hard margin**\n",
    "\n",
    "```\n",
    "Small C:                 Large C:\n",
    "ğŸ”´ ğŸ”´   |   ğŸ”µ ğŸ”µ         ğŸ”´ ğŸ”´|ğŸ”µ ğŸ”µ\n",
    "ğŸ”´  ğŸ”µ  |   ğŸ”µ ğŸ”µ         ğŸ”´ ğŸ”´|ğŸ”µ ğŸ”µ\n",
    "   â†‘                         â†‘\n",
    "Allows error             Strict boundary\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear SVC\n",
    "svc_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svc_linear.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_linear = svc_linear.predict(X_test_scaled)\n",
    "accuracy_linear = accuracy_score(y_test, y_pred_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix visualization\n",
    "cm = confusion_matrix(y_test, y_pred_linear)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=cm,\n",
    "    x=['Predicted Malignant', 'Predicted Benign'],\n",
    "    y=['Actual Malignant', 'Actual Benign'],\n",
    "    text=cm,\n",
    "    texttemplate='%{text}',\n",
    "    textfont={\"size\": 20},\n",
    "    colorscale='Blues',\n",
    "    showscale=False\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Linear SVC Confusion Matrix<br>Accuracy: {accuracy_linear:.2%}',\n",
    "    height=400,\n",
    "    xaxis_title='Predicted',\n",
    "    yaxis_title='Actual'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Support Vectors: The Key Players\n",
    "\n",
    "**Support vectors** are the critical points closest to the decision boundary. Only these points determine the boundary position!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of support vectors\n",
    "n_support = svc_linear.n_support_\n",
    "total_support = svc_linear.support_vectors_.shape[0]\n",
    "\n",
    "print(f\"ğŸ“Š Support Vectors Analysis\")\n",
    "print(f\"   Total training samples: {len(X_train_scaled)}\")\n",
    "print(f\"   Support vectors used: {total_support} ({total_support/len(X_train_scaled):.1%})\")\n",
    "print(f\"   Class 0 (Malignant): {n_support[0]}\")\n",
    "print(f\"   Class 1 (Benign): {n_support[1]}\")\n",
    "print(f\"\\nğŸ’¡ Only {total_support/len(X_train_scaled):.1%} of training data determines the boundary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. RBF Kernel: For Non-Linear Boundaries\n",
    "\n",
    "### ğŸ¨ The Kernel Trick\n",
    "\n",
    "**Problem:** What if classes aren't linearly separable?\n",
    "\n",
    "```\n",
    "Linear won't work:        RBF kernel can handle:\n",
    "                         \n",
    "  ğŸ”´ ğŸ”µ ğŸ”´                   ğŸ”´  ğŸ”´  ğŸ”´\n",
    "  ğŸ”µ ğŸ”´ ğŸ”µ      â†’           ğŸ”µ  ğŸ”µ  ğŸ”µ\n",
    "  ğŸ”´ ğŸ”µ ğŸ”´                   ğŸ”´  ğŸ”´  ğŸ”´\n",
    "```\n",
    "\n",
    "**RBF (Radial Basis Function) kernel:**\n",
    "- Creates circular/spherical decision boundaries\n",
    "- Implicitly maps to higher dimensions\n",
    "- Can capture complex patterns\n",
    "\n",
    "### ğŸ“Š New Hyperparameter: Gamma\n",
    "\n",
    "**Gamma controls the influence of each training point:**\n",
    "- **Small gamma:** Far-reaching influence â†’ **Smooth boundary**\n",
    "- **Large gamma:** Local influence â†’ **Complex boundary**\n",
    "\n",
    "```\n",
    "Small gamma (0.001):     Large gamma (10):\n",
    "Smooth, general          Wiggly, specific\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RBF SVC\n",
    "svc_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svc_rbf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rbf = svc_rbf.predict(X_test_scaled)\n",
    "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Linear vs RBF\n",
    "comparison = pd.DataFrame({\n",
    "    'Kernel': ['Linear', 'RBF'],\n",
    "    'Accuracy': [accuracy_linear, accuracy_rbf],\n",
    "    'Support Vectors': [\n",
    "        svc_linear.support_vectors_.shape[0],\n",
    "        svc_rbf.support_vectors_.shape[0]\n",
    "    ]\n",
    "})\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Accuracy',\n",
    "    x=comparison['Kernel'],\n",
    "    y=comparison['Accuracy'],\n",
    "    text=[f\"{x:.2%}\" for x in comparison['Accuracy']],\n",
    "    textposition='auto',\n",
    "    marker_color=['#FF6B6B', '#4ECDC4']\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Linear vs RBF Kernel Performance',\n",
    "    yaxis_title='Accuracy',\n",
    "    yaxis_range=[0.9, 1.0],\n",
    "    template='plotly_white',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Hyperparameter Tuning with Grid Search\n",
    "\n",
    "Let's systematically find the best combination of C and gamma!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(kernel='rbf', random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid search results\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "pivot_table = results.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_gamma',\n",
    "    columns='param_C'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=pivot_table.values,\n",
    "    x=[f'C={c}' for c in pivot_table.columns],\n",
    "    y=[f'Î³={g}' for g in pivot_table.index],\n",
    "    text=np.round(pivot_table.values, 4),\n",
    "    texttemplate='%{text:.4f}',\n",
    "    colorscale='RdYlGn',\n",
    "    colorbar=dict(title='Accuracy')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Grid Search Results: C vs Gamma',\n",
    "    xaxis_title='C (Regularization)',\n",
    "    yaxis_title='Gamma (Kernel Coefficient)',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nğŸ† Best Parameters: C={grid_search.best_params_['C']}, gamma={grid_search.best_params_['gamma']}\")\n",
    "print(f\"ğŸ¯ Best CV Accuracy: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Final Model with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with best parameters\n",
    "best_svc = grid_search.best_estimator_\n",
    "y_pred_best = best_svc.predict(X_test_scaled)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(y_test, y_pred_best, target_names=['Malignant', 'Benign'], output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "print(\"ğŸ“Š Final Model Performance\")\n",
    "print(\"=\"*60)\n",
    "print(report_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Decision Boundary Visualization (2D Projection)\n",
    "\n",
    "Since we have 30 features, let's use PCA to project to 2D and visualize the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project to 2D using PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_train_2d = pca.fit_transform(X_train_scaled)\n",
    "X_test_2d = pca.transform(X_test_scaled)\n",
    "\n",
    "# Train SVC on 2D data\n",
    "svc_2d = SVC(kernel='rbf', C=grid_search.best_params_['C'], \n",
    "             gamma=grid_search.best_params_['gamma'], random_state=42)\n",
    "svc_2d.fit(X_train_2d, y_train)\n",
    "\n",
    "# Create mesh for decision boundary\n",
    "x_min, x_max = X_train_2d[:, 0].min() - 1, X_train_2d[:, 0].max() + 1\n",
    "y_min, y_max = X_train_2d[:, 1].min() - 1, X_train_2d[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "# Predict on mesh\n",
    "Z = svc_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive decision boundary plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Decision boundary\n",
    "fig.add_trace(go.Contour(\n",
    "    x=xx[0],\n",
    "    y=yy[:, 0],\n",
    "    z=Z,\n",
    "    colorscale=[[0, 'rgba(255,107,107,0.3)'], [1, 'rgba(78,205,196,0.3)']],\n",
    "    showscale=False,\n",
    "    hoverinfo='skip'\n",
    "))\n",
    "\n",
    "# Training points\n",
    "for class_val, class_name, color in [(0, 'Malignant', '#FF6B6B'), (1, 'Benign', '#4ECDC4')]:\n",
    "    mask = y_train == class_val\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_train_2d[mask, 0],\n",
    "        y=X_train_2d[mask, 1],\n",
    "        mode='markers',\n",
    "        name=class_name,\n",
    "        marker=dict(color=color, size=8, line=dict(width=1, color='white'))\n",
    "    ))\n",
    "\n",
    "# Support vectors\n",
    "support_vectors_2d = X_train_2d[svc_2d.support_]\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=support_vectors_2d[:, 0],\n",
    "    y=support_vectors_2d[:, 1],\n",
    "    mode='markers',\n",
    "    name='Support Vectors',\n",
    "    marker=dict(size=12, color='yellow', symbol='circle-open', line=dict(width=2, color='black'))\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'SVC Decision Boundary (2D PCA Projection)<br>Support Vectors: {len(svc_2d.support_)}',\n",
    "    xaxis_title='First Principal Component',\n",
    "    yaxis_title='Second Principal Component',\n",
    "    template='plotly_white',\n",
    "    height=600,\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ’¡ Key Observations:**\n",
    "- Yellow circles = support vectors (the critical points)\n",
    "- Shaded regions = predicted classes\n",
    "- Boundary adapts to data structure\n",
    "- Non-linear boundary captures complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Key Takeaways\n",
    "\n",
    "### âœ… What We Learned:\n",
    "\n",
    "**1. SVC Core Concepts:**\n",
    "- Finds maximum margin between classes\n",
    "- Only support vectors matter for the boundary\n",
    "- Effective in high-dimensional spaces\n",
    "\n",
    "**2. Kernels:**\n",
    "- **Linear:** For linearly separable data, fast, interpretable\n",
    "- **RBF:** For complex patterns, flexible, most popular\n",
    "- **Polynomial:** For polynomial relationships\n",
    "- **Sigmoid:** For neural network-like boundaries\n",
    "\n",
    "**3. Hyperparameters:**\n",
    "- **C:** Trade-off between margin width and misclassifications\n",
    "  - Small C â†’ Soft margin (more generalization)\n",
    "  - Large C â†’ Hard margin (less tolerance for errors)\n",
    "- **Gamma (RBF):** Influence of each training point\n",
    "  - Small gamma â†’ Far-reaching influence\n",
    "  - Large gamma â†’ Local influence\n",
    "\n",
    "**4. When to Use SVC:**\n",
    "- âœ… High-dimensional data\n",
    "- âœ… Clear margin between classes\n",
    "- âœ… Small to medium datasets (< 100K samples)\n",
    "- âœ… Need robust to outliers\n",
    "- âŒ Very large datasets (slow to train)\n",
    "- âŒ Need probability estimates (use `probability=True`, but slower)\n",
    "- âŒ Real-time predictions with tight latency requirements\n",
    "\n",
    "**5. Best Practices:**\n",
    "- Always scale features!\n",
    "- Use Grid Search for hyperparameter tuning\n",
    "- Start with RBF kernel\n",
    "- Check number of support vectors (if too many â†’ overfitting)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Real-World Application:\n",
    "\n",
    "Our SVC model achieved **{:.1%} accuracy** on breast cancer diagnosis:\n",
    "- High precision for malignant cases (critical!)\n",
    "- Robust decision boundary\n",
    "- Only {:.0%} of training data (support vectors) needed\n",
    "\n",
    "This demonstrates SVC's power for medical diagnosis where:\n",
    "- Accuracy is critical\n",
    "- Data is high-dimensional\n",
    "- Clear separation exists\n",
    "\n",
    "---\n",
    "\n",
    "**AI Tech Institute** | *Building Tomorrow's AI Engineers Today*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
