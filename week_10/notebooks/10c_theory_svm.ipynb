{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Understanding Support Vector Machines (SVM)\n",
    "**Instructor:** Amir Charkhi\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand what SVM is and how it works\n",
    "- Learn about margins, support vectors, and decision boundaries\n",
    "- Understand the kernel trick for non-linear problems\n",
    "- Know when to use SVM vs other algorithms\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Core Idea: Finding the Best Separator\n",
    "\n",
    "### üéØ **The Problem:**\n",
    "Imagine you have two groups of points (say, red dots and blue dots) on a piece of paper. You need to draw a line that separates them.\n",
    "\n",
    "**Question:** How do you draw the \"best\" line?\n",
    "\n",
    "### ‚úèÔ∏è **Many Lines Work:**\n",
    "```\n",
    "    Red dots        |      Blue dots\n",
    "        ‚Ä¢  ‚Ä¢       |         ‚Ä¢  ‚Ä¢\n",
    "       ‚Ä¢   ‚Ä¢      |        ‚Ä¢   ‚Ä¢\n",
    "      ‚Ä¢  ‚Ä¢       |       ‚Ä¢  ‚Ä¢\n",
    "               LINE\n",
    "```\n",
    "\n",
    "All these lines separate red from blue:\n",
    "- Line far left ‚úì\n",
    "- Line in middle ‚úì \n",
    "- Line far right ‚úì\n",
    "\n",
    "**But which is BEST?**\n",
    "\n",
    "### üéØ **SVM's Answer: The line with the BIGGEST MARGIN**\n",
    "\n",
    "**Margin** = Distance from the line to the nearest points on both sides\n",
    "\n",
    "```\n",
    "       Red              MARGIN              Blue\n",
    "        ‚Ä¢  ‚Ä¢        |~~~~~~~~|         ‚Ä¢  ‚Ä¢\n",
    "       ‚Ä¢   ‚Ä¢       |~~~~~~~~|        ‚Ä¢   ‚Ä¢\n",
    "      ‚Ä¢  ‚Ä¢        |~~~~~~~~|       ‚Ä¢  ‚Ä¢\n",
    "              <----margin---->\n",
    "```\n",
    "\n",
    "**Why biggest margin?**\n",
    "- More robust to new data\n",
    "- Less likely to misclassify points near the boundary\n",
    "- Better generalization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Key Concepts\n",
    "\n",
    "### üìè **1. Decision Boundary (Hyperplane)**\n",
    "The line (or plane in higher dimensions) that separates the classes.\n",
    "\n",
    "In 2D: **Line**  \n",
    "In 3D: **Plane**  \n",
    "In higher dimensions: **Hyperplane**\n",
    "\n",
    "### üéØ **2. Support Vectors**\n",
    "The data points **closest** to the decision boundary.\n",
    "\n",
    "```\n",
    "    ‚Ä¢  ‚Ä¢                     ‚Ä¢  ‚Ä¢\n",
    "   ‚Ä¢   ‚óè  ‚Üê support vector  |  ‚óè   ‚Ä¢  ‚Üê support vector\n",
    "  ‚Ä¢  ‚Ä¢                      |    ‚Ä¢  ‚Ä¢\n",
    "                        BOUNDARY\n",
    "```\n",
    "\n",
    "**Key Insight:** Only these support vectors matter! All other points can be ignored.\n",
    "\n",
    "### üìê **3. Margin**\n",
    "The \"buffer zone\" around the decision boundary.\n",
    "\n",
    "```\n",
    "           |<--margin-->|<--margin-->|\n",
    "    ‚Ä¢  ‚Ä¢   |            |           |  ‚Ä¢  ‚Ä¢\n",
    "   ‚Ä¢   ‚óè---|------------|-----------|---‚óè   ‚Ä¢\n",
    "  ‚Ä¢  ‚Ä¢     | MARGIN     | MARGIN    |    ‚Ä¢  ‚Ä¢\n",
    "           |            |           |\n",
    "         edge      boundary      edge\n",
    "```\n",
    "\n",
    "**Goal:** Maximize this margin!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How SVM Works: Step by Step\n",
    "\n",
    "### **Step 1: Find the Decision Boundary**\n",
    "- Try different lines/planes\n",
    "- Calculate margin for each\n",
    "- Pick the one with the **maximum margin**\n",
    "\n",
    "### **Step 2: Identify Support Vectors**\n",
    "- The points touching the margin edges\n",
    "- These define the decision boundary\n",
    "- Usually only a small subset of all data\n",
    "\n",
    "### **Step 3: Make Predictions**\n",
    "For a new point:\n",
    "- Calculate which side of the boundary it's on\n",
    "- Assign it to that class\n",
    "\n",
    "### üî¢ **Mathematical Formulation (Simplified):**\n",
    "\n",
    "For a point **x**, the decision function is:\n",
    "$$f(x) = w \\cdot x + b$$\n",
    "\n",
    "Where:\n",
    "- $w$ = weights (defines direction of boundary)\n",
    "- $b$ = bias (defines position of boundary)\n",
    "- $w \\cdot x$ = dot product\n",
    "\n",
    "**Prediction:**\n",
    "- If $f(x) > 0$ ‚Üí Class 1\n",
    "- If $f(x) < 0$ ‚Üí Class 0\n",
    "- If $f(x) = 0$ ‚Üí On the boundary\n",
    "\n",
    "**Objective:** Maximize margin = Minimize $||w||$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear vs Non-Linear Problems\n",
    "\n",
    "### ‚úÖ **Linear SVM (Linearly Separable Data)**\n",
    "\n",
    "Perfect for data that can be separated by a straight line:\n",
    "\n",
    "```\n",
    "    Red          |         Blue\n",
    "     ‚Ä¢  ‚Ä¢        |          ‚Ä¢  ‚Ä¢\n",
    "    ‚Ä¢   ‚Ä¢       |         ‚Ä¢   ‚Ä¢\n",
    "   ‚Ä¢  ‚Ä¢        LINE      ‚Ä¢  ‚Ä¢\n",
    "```\n",
    "\n",
    "**Examples:**\n",
    "- Well-behaved datasets\n",
    "- High-dimensional data (often becomes linearly separable)\n",
    "\n",
    "### ‚ùå **Problem: Non-Linear Data**\n",
    "\n",
    "What if data looks like this?\n",
    "\n",
    "```\n",
    "       Blue\n",
    "    ‚Ä¢  ‚Ä¢  ‚Ä¢  ‚Ä¢\n",
    "   ‚Ä¢ Red Red  ‚Ä¢\n",
    "   ‚Ä¢  ‚Ä¢  ‚Ä¢   ‚Ä¢\n",
    "    ‚Ä¢  ‚Ä¢  ‚Ä¢  ‚Ä¢\n",
    "```\n",
    "\n",
    "No straight line can separate red from blue!\n",
    "\n",
    "### üöÄ **Solution: The Kernel Trick**\n",
    "\n",
    "**Key Idea:** Transform data into a higher dimension where it **becomes** linearly separable!\n",
    "\n",
    "#### **Example: 1D to 2D Transformation**\n",
    "\n",
    "**Before (1D - not separable):**\n",
    "```\n",
    "Red  Blue  Red\n",
    " ‚Ä¢    ‚Ä¢    ‚Ä¢\n",
    "------|------\n",
    "      x-axis\n",
    "```\n",
    "\n",
    "**After transformation (2D - separable!):**\n",
    "```\n",
    "   ‚Ä¢  Red\n",
    "        -------LINE-------\n",
    "‚Ä¢ Blue              ‚Ä¢ Red\n",
    "```\n",
    "\n",
    "By adding a dimension (e.g., $x^2$), points that were mixed are now separable!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Common Kernel Functions\n",
    "\n",
    "Kernels transform data without explicitly computing the transformation.\n",
    "\n",
    "### **1. Linear Kernel (No transformation)**\n",
    "$$K(x, y) = x \\cdot y$$\n",
    "\n",
    "**Use when:** Data is already linearly separable\n",
    "\n",
    "### **2. Polynomial Kernel**\n",
    "$$K(x, y) = (x \\cdot y + c)^d$$\n",
    "\n",
    "Where:\n",
    "- $d$ = degree (2, 3, 4...)\n",
    "- $c$ = constant\n",
    "\n",
    "**Use when:** You need curved decision boundaries\n",
    "\n",
    "### **3. RBF (Radial Basis Function) Kernel** üåü Most Popular\n",
    "$$K(x, y) = e^{-\\gamma ||x - y||^2}$$\n",
    "\n",
    "Where:\n",
    "- $\\gamma$ = controls flexibility\n",
    "- High $\\gamma$ = more wiggly boundary (risk overfitting)\n",
    "- Low $\\gamma$ = smoother boundary\n",
    "\n",
    "**Use when:** Complex, non-linear patterns\n",
    "\n",
    "### **4. Sigmoid Kernel**\n",
    "$$K(x, y) = \\tanh(\\alpha x \\cdot y + c)$$\n",
    "\n",
    "**Use when:** Similar to neural networks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Hyperparameter: C (Regularization)\n",
    "\n",
    "**C controls the trade-off between:**\n",
    "1. **Wide margin** (more generalization)\n",
    "2. **Correct classification** of training points\n",
    "\n",
    "### **Small C (e.g., 0.1)**\n",
    "```\n",
    "   ‚Ä¢  ‚Ä¢        |<--wide margin-->|      ‚Ä¢  ‚Ä¢\n",
    "  ‚Ä¢   ‚óè  X    |                 |    X  ‚óè   ‚Ä¢\n",
    " ‚Ä¢  ‚Ä¢         |                 |         ‚Ä¢  ‚Ä¢\n",
    "```\n",
    "- Wider margin\n",
    "- Allows some misclassifications (X)\n",
    "- Better generalization\n",
    "- Less overfitting\n",
    "\n",
    "### **Large C (e.g., 100)**\n",
    "```\n",
    "   ‚Ä¢  ‚Ä¢     |narrow|     ‚Ä¢  ‚Ä¢\n",
    "  ‚Ä¢   ‚óè     |      |     ‚óè   ‚Ä¢\n",
    " ‚Ä¢  ‚Ä¢       |      |       ‚Ä¢  ‚Ä¢\n",
    "```\n",
    "- Narrower margin\n",
    "- Tries to classify all training points correctly\n",
    "- Risk of overfitting\n",
    "- Complex boundary\n",
    "\n",
    "**Rule of Thumb:**\n",
    "- Start with C=1.0\n",
    "- If underfitting ‚Üí increase C\n",
    "- If overfitting ‚Üí decrease C\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing SVM with Code\n",
    "\n",
    "Let's create a simple example to see SVM in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example 1: Linear SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linearly separable data\n",
    "X_linear, y_linear = make_blobs(n_samples=50, centers=2, random_state=42, cluster_std=0.6)\n",
    "\n",
    "# Train Linear SVM\n",
    "svm_linear = SVC(kernel='linear', C=1.0)\n",
    "svm_linear.fit(X_linear, y_linear)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_linear[:, 0], X_linear[:, 1], c=y_linear, cmap='coolwarm', s=100, edgecolors='k')\n",
    "\n",
    "# Decision boundary\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = svm_linear.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "# Highlight support vectors\n",
    "ax.scatter(svm_linear.support_vectors_[:, 0], svm_linear.support_vectors_[:, 1],\n",
    "           s=300, linewidth=1.5, facecolors='none', edgecolors='k', label='Support Vectors')\n",
    "\n",
    "plt.title('Linear SVM: Maximum Margin Classifier', fontsize=14, pad=15)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Number of support vectors: {len(svm_linear.support_vectors_)}\")\n",
    "print(f\"üí° Notice: Only a few points (support vectors) define the boundary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example 2: Non-Linear SVM (RBF Kernel)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-linearly separable data (circles)\n",
    "X_circles, y_circles = make_circles(n_samples=100, factor=0.3, noise=0.1, random_state=42)\n",
    "\n",
    "# Train SVM with RBF kernel\n",
    "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm_rbf.fit(X_circles, y_circles)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Linear SVM (fails)\n",
    "svm_linear_circles = SVC(kernel='linear', C=1.0)\n",
    "svm_linear_circles.fit(X_circles, y_circles)\n",
    "\n",
    "axes[0].scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap='coolwarm', s=50, edgecolors='k')\n",
    "xx = np.linspace(-1.5, 1.5, 100)\n",
    "yy = np.linspace(-1.5, 1.5, 100)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = svm_linear_circles.decision_function(xy).reshape(XX.shape)\n",
    "axes[0].contour(XX, YY, Z, colors='k', levels=[0], alpha=0.5, linestyles=['-'])\n",
    "axes[0].set_title('‚ùå Linear SVM Fails on Circular Data', fontsize=13, pad=15)\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: RBF SVM (succeeds)\n",
    "axes[1].scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap='coolwarm', s=50, edgecolors='k')\n",
    "Z = svm_rbf.decision_function(xy).reshape(XX.shape)\n",
    "axes[1].contour(XX, YY, Z, colors='k', levels=[0], alpha=0.5, linestyles=['-'])\n",
    "axes[1].contourf(XX, YY, Z, levels=20, cmap='coolwarm', alpha=0.2)\n",
    "axes[1].scatter(svm_rbf.support_vectors_[:, 0], svm_rbf.support_vectors_[:, 1],\n",
    "               s=200, linewidth=1.5, facecolors='none', edgecolors='k', label='Support Vectors')\n",
    "axes[1].set_title('‚úÖ RBF Kernel SVM Handles Circular Data', fontsize=13, pad=15)\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight: Kernel trick allows SVM to create complex boundaries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. SVM vs Other Algorithms\n",
    "\n",
    "| Aspect | SVM | Logistic Regression | Decision Trees |\n",
    "|--------|-----|-------------------|---------------|\n",
    "| **Decision Boundary** | Maximum margin | Probability-based | Axis-aligned splits |\n",
    "| **Non-linear Data** | Excellent (kernels) | Poor | Good |\n",
    "| **Interpretability** | Low | High | High |\n",
    "| **Training Speed** | Slow (large data) | Fast | Fast |\n",
    "| **Memory** | Efficient (support vectors) | Efficient | Can be large |\n",
    "| **Overfitting** | Good with right C | Can overfit | Very prone |\n",
    "| **Feature Scaling** | Required | Required | Not needed |\n",
    "| **Probabilities** | Yes (calibrated) | Yes (natural) | Yes |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. When to Use SVM\n",
    "\n",
    "### ‚úÖ **Use SVM when:**\n",
    "1. **High-dimensional data** (many features)\n",
    "   - Text classification\n",
    "   - Gene expression data\n",
    "   - Image classification (with proper features)\n",
    "\n",
    "2. **Clear margin of separation** exists\n",
    "   - Well-defined classes\n",
    "   - Not too much class overlap\n",
    "\n",
    "3. **Non-linear relationships** (use RBF kernel)\n",
    "   - Complex decision boundaries\n",
    "   - Pattern recognition\n",
    "\n",
    "4. **Small to medium datasets**\n",
    "   - < 10,000 samples (depending on features)\n",
    "   - SVM scales poorly with very large datasets\n",
    "\n",
    "### ‚ùå **Don't use SVM when:**\n",
    "1. **Very large datasets** (millions of samples)\n",
    "   - Use linear models or neural networks\n",
    "   - Training becomes too slow\n",
    "\n",
    "2. **Need probability estimates** as primary output\n",
    "   - Logistic regression is better\n",
    "   - SVM probabilities are calibrated but not natural\n",
    "\n",
    "3. **Interpretability is crucial**\n",
    "   - Use logistic regression or decision trees\n",
    "   - SVM's decision function is complex\n",
    "\n",
    "4. **Noisy data with lots of overlap**\n",
    "   - Ensemble methods (Random Forest) may be better\n",
    "   - SVM can struggle with unclear boundaries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Practical Tips\n",
    "\n",
    "### **1. Always Scale Your Features!** ‚ö†Ô∏è\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "### **2. Start Simple**\n",
    "1. Try **Linear kernel** first\n",
    "2. If that fails, try **RBF kernel**\n",
    "3. Tune C and gamma\n",
    "\n",
    "### **3. Hyperparameter Tuning**\n",
    "- **C**: Start with 1.0, try [0.1, 1, 10, 100]\n",
    "- **gamma** (for RBF): Start with 'scale', try [0.001, 0.01, 0.1, 1]\n",
    "\n",
    "### **4. Use GridSearchCV**\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 0.001, 0.01, 0.1],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "```\n",
    "\n",
    "### **5. Check Support Vector Count**\n",
    "```python\n",
    "n_support = len(model.support_vectors_)\n",
    "print(f\"Support vectors: {n_support}/{len(X_train)}\")\n",
    "```\n",
    "- If too many (>50% of data) ‚Üí Model might be struggling\n",
    "- If very few ‚Üí Good! Model found clear boundary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Takeaways\n",
    "\n",
    "### üéØ **Core Concepts:**\n",
    "\n",
    "**1. Maximum Margin Principle**\n",
    "- SVM finds the boundary with the **largest margin**\n",
    "- More robust and generalizable\n",
    "\n",
    "**2. Support Vectors**\n",
    "- Only a **few points** (support vectors) matter\n",
    "- These define the decision boundary\n",
    "- Rest of the data can be ignored\n",
    "\n",
    "**3. Kernel Trick**\n",
    "- Transforms data to higher dimensions\n",
    "- Makes non-linear data linearly separable\n",
    "- RBF kernel is most popular\n",
    "\n",
    "**4. Hyperparameters**\n",
    "- **C**: Regularization (smaller = wider margin)\n",
    "- **gamma**: Kernel flexibility (higher = more complex)\n",
    "- **kernel**: Linear, RBF, Polynomial, etc.\n",
    "\n",
    "### üí° **Remember:**\n",
    "```\n",
    "SVM = Support Vector Machine\n",
    "\n",
    "Support Vectors ‚Üí Points closest to boundary\n",
    "Maximum Margin ‚Üí Widest separation\n",
    "Kernel Trick ‚Üí Handle non-linear data\n",
    "```\n",
    "\n",
    "### üéì **Analogy:**\n",
    "Think of SVM like finding the widest road between two cities:\n",
    "- The cities are your classes\n",
    "- The road is your decision boundary\n",
    "- The width is your margin\n",
    "- Support vectors are the buildings closest to the road\n",
    "\n",
    "You want the **widest possible road** (margin) while respecting the buildings (data points) on both sides!\n",
    "\n",
    "---\n",
    "\n",
    "**AI Tech Institute** | *Building Tomorrow's AI Engineers Today*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
