{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### K-Nearest Neighbors (KNN) - Interactive Deep Dive\n",
    "**Instructor:** Amir Charkhi | **Dataset:** Wine Quality (UCI)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "- How KNN makes predictions using neighbors\n",
    "- Distance metrics and their impact\n",
    "- Finding optimal K value\n",
    "- Feature scaling importance\n",
    "- Real-world application: Wine classification\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ The Big Idea: Vote by Your Neighbors\n",
    "\n",
    "Imagine you move to a new neighborhood and want to know if you'll like it:\n",
    "\n",
    "```\n",
    "You: üè†?       Ask K=3 nearest neighbors:\n",
    "              \n",
    "              üòä (Happy) - 50m away\n",
    "              üòä (Happy) - 80m away  \n",
    "              üòä (Happy) - 120m away\n",
    "              üò¢ (Unhappy) - 500m away\n",
    "              \n",
    "Vote: 3 Happy, 0 Unhappy ‚Üí You'll probably be happy! üòä\n",
    "```\n",
    "\n",
    "**KNN is that simple:**\n",
    "1. Find K closest training examples\n",
    "2. Let them vote\n",
    "3. Majority wins!\n",
    "\n",
    "**No training phase!** The model just memorizes data and compares at prediction time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Plotly for interactive visualizations\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Wine Quality Data\n",
    "\n",
    "**Dataset:** Wine Recognition Data\n",
    "\n",
    "**Context:** Chemical analysis of wines from 3 different cultivars in Italy:\n",
    "- 13 features: alcohol, acidity, color intensity, etc.\n",
    "- **Goal:** Classify wine into one of 3 types\n",
    "- **178 samples** from 3 wine regions\n",
    "\n",
    "**Real-world impact:** Wine quality control, authenticity verification üç∑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_wine()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='wine_class')\n",
    "\n",
    "# Create readable labels\n",
    "y_labels = y.map({0: 'Class 0', 1: 'Class 1', 2: 'Class 2'})\n",
    "\n",
    "# Display info\n",
    "info_df = pd.DataFrame({\n",
    "    'Metric': ['Total Samples', 'Features', 'Classes', 'Class 0 (Samples)', 'Class 1 (Samples)', 'Class 2 (Samples)'],\n",
    "    'Value': [\n",
    "        len(X),\n",
    "        X.shape[1],\n",
    "        len(np.unique(y)),\n",
    "        (y == 0).sum(),\n",
    "        (y == 1).sum(),\n",
    "        (y == 2).sum()\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"üç∑ Wine Classification Dataset\")\n",
    "print(\"=\"*50)\n",
    "for _, row in info_df.iterrows():\n",
    "    print(f\"{row['Metric']:.<30} {row['Value']}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview features\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive class distribution\n",
    "class_counts = y_labels.value_counts().sort_index()\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=class_counts.index,\n",
    "        y=class_counts.values,\n",
    "        text=class_counts.values,\n",
    "        textposition='auto',\n",
    "        marker_color=['#E74C3C', '#3498DB', '#2ECC71']\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Wine Class Distribution',\n",
    "    xaxis_title='Wine Class',\n",
    "    yaxis_title='Number of Samples',\n",
    "    template='plotly_white',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Analysis\n",
    "\n",
    "Let's explore how different chemical properties separate wine classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Create visualization dataframe\n",
    "viz_df = X[['alcohol', 'flavanoids', 'color_intensity', 'proline']].copy()\n",
    "viz_df['wine_class'] = y_labels\n",
    "\n",
    "# Convert categorical labels to numeric codes\n",
    "viz_df['wine_class_num'] = viz_df['wine_class'].astype('category').cat.codes\n",
    "\n",
    "# Parallel coordinates plot (continuous color only)\n",
    "fig = px.parallel_coordinates(\n",
    "    viz_df,\n",
    "    dimensions=['alcohol', 'flavanoids', 'color_intensity', 'proline'],\n",
    "    color='wine_class_num',\n",
    "    color_continuous_scale=['#E74C3C', '#3498DB', '#2ECC71'],  # red ‚Üí blue ‚Üí green\n",
    "    labels={'wine_class_num': 'Wine Class'},\n",
    "    title='Feature Patterns Across Wine Classes'\n",
    ")\n",
    "\n",
    "fig.update_layout(height=500)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Observation:** Different classes show distinct patterns! KNN will use these patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Key Features Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D scatter for top features\n",
    "fig = px.scatter_3d(\n",
    "    viz_df,\n",
    "    x='alcohol',\n",
    "    y='flavanoids',\n",
    "    z='proline',\n",
    "    color='wine_class',\n",
    "    title='3D Feature Space: Can Neighbors Help?',\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Rotate the plot!** Notice how classes cluster together. This is perfect for KNN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Prepare Data\n",
    "\n",
    "### üéØ Critical: Feature Scaling for KNN\n",
    "\n",
    "**Why is scaling ESSENTIAL for KNN?**\n",
    "\n",
    "KNN uses distance. Features with large ranges dominate!\n",
    "\n",
    "```\n",
    "Example (without scaling):\n",
    "  Alcohol: 12.5 vs 13.0    ‚Üí Difference: 0.5\n",
    "  Proline: 500 vs 1000     ‚Üí Difference: 500\n",
    "  \n",
    "Distance calculation dominated by Proline!\n",
    "Alcohol's contribution is negligible!\n",
    "\n",
    "After scaling (all features on same scale):\n",
    "  Alcohol: 0.2 vs 0.3      ‚Üí Difference: 0.1  \n",
    "  Proline: 0.4 vs 0.8      ‚Üí Difference: 0.4\n",
    "  \n",
    "Both features contribute fairly!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualize Scaling Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare before/after scaling\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Feature': X.columns[:6],\n",
    "    'Original Range': [X_train[col].max() - X_train[col].min() for col in X.columns[:6]],\n",
    "    'Scaled Range': [X_train_scaled[:, i].max() - X_train_scaled[:, i].min() for i in range(6)]\n",
    "})\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Before Scaling',\n",
    "    x=comparison_df['Feature'],\n",
    "    y=comparison_df['Original Range'],\n",
    "    marker_color='lightcoral'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='After Scaling',\n",
    "    x=comparison_df['Feature'],\n",
    "    y=comparison_df['Scaled Range'],\n",
    "    marker_color='lightgreen'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Feature Ranges: Before vs After Scaling',\n",
    "    yaxis_title='Range',\n",
    "    barmode='group',\n",
    "    template='plotly_white',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° After scaling:** All features have similar ranges ‚Üí Fair contribution to distance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Understanding Distance Metrics\n",
    "\n",
    "### üìè How to Measure \"Closeness\"?\n",
    "\n",
    "**Euclidean Distance (Default):** Straight-line distance\n",
    "```\n",
    "d = ‚àö[(x‚ÇÅ-x‚ÇÇ)¬≤ + (y‚ÇÅ-y‚ÇÇ)¬≤]\n",
    "\n",
    "Point A: (1, 2)\n",
    "Point B: (4, 6)  \n",
    "\n",
    "d = ‚àö[(1-4)¬≤ + (2-6)¬≤] = ‚àö[9 + 16] = 5\n",
    "```\n",
    "\n",
    "**Manhattan Distance:** City-block distance (taxi cab route)\n",
    "```\n",
    "d = |x‚ÇÅ-x‚ÇÇ| + |y‚ÇÅ-y‚ÇÇ|\n",
    "\n",
    "d = |1-4| + |2-6| = 3 + 4 = 7\n",
    "```\n",
    "\n",
    "**Minkowski Distance:** Generalization (p=1 ‚Üí Manhattan, p=2 ‚Üí Euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distance metrics\n",
    "distance_metrics = ['euclidean', 'manhattan', 'minkowski']\n",
    "results = []\n",
    "\n",
    "for metric in distance_metrics:\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    accuracy = knn.score(X_test_scaled, y_test)\n",
    "    results.append({'Metric': metric, 'Accuracy': accuracy})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=results_df['Metric'],\n",
    "        y=results_df['Accuracy'],\n",
    "        text=[f\"{x:.2%}\" for x in results_df['Accuracy']],\n",
    "        textposition='auto',\n",
    "        marker_color=['#E74C3C', '#3498DB', '#2ECC71']\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Distance Metric Comparison (K=5)',\n",
    "    xaxis_title='Distance Metric',\n",
    "    yaxis_title='Accuracy',\n",
    "    yaxis_range=[0, 1],\n",
    "    template='plotly_white',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Insight:** Euclidean typically works well for continuous features (like ours)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Finding Optimal K\n",
    "\n",
    "### üéØ The K Dilemma\n",
    "\n",
    "**Small K (e.g., K=1):**\n",
    "```\n",
    "‚úÖ Captures fine details\n",
    "‚ùå Sensitive to noise\n",
    "‚ùå Overfitting risk\n",
    "\n",
    "Example: One noisy neighbor can mislead!\n",
    "```\n",
    "\n",
    "**Large K (e.g., K=50):**\n",
    "```\n",
    "‚úÖ Robust to noise\n",
    "‚úÖ Smooth boundaries\n",
    "‚ùå Misses local patterns\n",
    "‚ùå Underfitting risk\n",
    "\n",
    "Example: Too many neighbors ‚Üí majority class always wins!\n",
    "```\n",
    "\n",
    "**Goal:** Find the \"sweet spot\" K!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different K values\n",
    "k_values = range(1, 31)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_scores.append(knn.score(X_train_scaled, y_train))\n",
    "    test_scores.append(knn.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive K vs Accuracy plot\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(k_values),\n",
    "    y=train_scores,\n",
    "    mode='lines+markers',\n",
    "    name='Training Accuracy',\n",
    "    line=dict(color='lightblue', width=3),\n",
    "    marker=dict(size=8)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(k_values),\n",
    "    y=test_scores,\n",
    "    mode='lines+markers',\n",
    "    name='Test Accuracy',\n",
    "    line=dict(color='orange', width=3),\n",
    "    marker=dict(size=8)\n",
    "))\n",
    "\n",
    "# Mark optimal K\n",
    "optimal_k = list(k_values)[np.argmax(test_scores)]\n",
    "optimal_accuracy = max(test_scores)\n",
    "\n",
    "fig.add_vline(\n",
    "    x=optimal_k,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    "    annotation_text=f\"Optimal K={optimal_k}\",\n",
    "    annotation_position=\"top\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Finding Optimal K: Bias-Variance Trade-off',\n",
    "    xaxis_title='K (Number of Neighbors)',\n",
    "    yaxis_title='Accuracy',\n",
    "    template='plotly_white',\n",
    "    height=500,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nüéØ Optimal K: {optimal_k}\")\n",
    "print(f\"üìä Test Accuracy: {optimal_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Key Observations:**\n",
    "\n",
    "- **K=1:** Perfect training accuracy (memorization!), but test accuracy lower\n",
    "- **Small K:** More variance, sensitive to noise\n",
    "- **Optimal K:** Best test performance (generalization)\n",
    "- **Large K:** Training and test converge (too simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with optimal K\n",
    "final_knn = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "final_knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = final_knn.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=cm,\n",
    "    x=['Class 0', 'Class 1', 'Class 2'],\n",
    "    y=['Class 0', 'Class 1', 'Class 2'],\n",
    "    text=cm,\n",
    "    texttemplate='%{text}',\n",
    "    textfont={\"size\": 16},\n",
    "    colorscale='Blues',\n",
    "    showscale=True\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'KNN Confusion Matrix (K={optimal_k})<br>Overall Accuracy: {accuracy:.2%}',\n",
    "    xaxis_title='Predicted Class',\n",
    "    yaxis_title='Actual Class',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Performance by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report visualization\n",
    "report = classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1', 'Class 2'], output_dict=True)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': ['Class 0', 'Class 1', 'Class 2'],\n",
    "    'Precision': [report['Class 0']['precision'], report['Class 1']['precision'], report['Class 2']['precision']],\n",
    "    'Recall': [report['Class 0']['recall'], report['Class 1']['recall'], report['Class 2']['recall']],\n",
    "    'F1-Score': [report['Class 0']['f1-score'], report['Class 1']['f1-score'], report['Class 2']['f1-score']]\n",
    "})\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for metric in ['Precision', 'Recall', 'F1-Score']:\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=metric,\n",
    "        x=metrics_df['Class'],\n",
    "        y=metrics_df[metric],\n",
    "        text=[f\"{x:.2%}\" for x in metrics_df[metric]],\n",
    "        textposition='auto'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Performance Metrics by Wine Class',\n",
    "    xaxis_title='Wine Class',\n",
    "    yaxis_title='Score',\n",
    "    yaxis_range=[0, 1.1],\n",
    "    barmode='group',\n",
    "    template='plotly_white',\n",
    "    height=450\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Decision Boundaries Visualization\n",
    "\n",
    "Let's see how KNN creates boundaries in 2D feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use two features for visualization\n",
    "feature_1, feature_2 = 'alcohol', 'proline'\n",
    "idx1 = list(X.columns).index(feature_1)\n",
    "idx2 = list(X.columns).index(feature_2)\n",
    "\n",
    "X_train_2d = X_train_scaled[:, [idx1, idx2]]\n",
    "X_test_2d = X_test_scaled[:, [idx1, idx2]]\n",
    "\n",
    "# Train KNN on 2D\n",
    "knn_2d = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "knn_2d.fit(X_train_2d, y_train)\n",
    "\n",
    "# Create mesh\n",
    "x_min, x_max = X_train_2d[:, 0].min() - 1, X_train_2d[:, 0].max() + 1\n",
    "y_min, y_max = X_train_2d[:, 1].min() - 1, X_train_2d[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "# Predict on mesh\n",
    "Z = knn_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive decision boundary\n",
    "fig = go.Figure()\n",
    "\n",
    "# Decision regions\n",
    "fig.add_trace(go.Contour(\n",
    "    x=xx[0],\n",
    "    y=yy[:, 0],\n",
    "    z=Z,\n",
    "    colorscale=[\n",
    "        [0, 'rgba(231, 76, 60, 0.3)'],\n",
    "        [0.5, 'rgba(52, 152, 219, 0.3)'],\n",
    "        [1, 'rgba(46, 204, 113, 0.3)']\n",
    "    ],\n",
    "    showscale=False,\n",
    "    hoverinfo='skip',\n",
    "    contours=dict(start=0, end=2, size=1)\n",
    "))\n",
    "\n",
    "# Training points\n",
    "for class_val, class_name, color in [(0, 'Class 0', '#E74C3C'), (1, 'Class 1', '#3498DB'), (2, 'Class 2', '#2ECC71')]:\n",
    "    mask = y_train == class_val\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_train_2d[mask, 0],\n",
    "        y=X_train_2d[mask, 1],\n",
    "        mode='markers',\n",
    "        name=class_name,\n",
    "        marker=dict(color=color, size=10, line=dict(width=1, color='white'))\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'KNN Decision Boundaries (K={optimal_k})<br>{feature_1} vs {feature_2}',\n",
    "    xaxis_title=f'{feature_1} (scaled)',\n",
    "    yaxis_title=f'{feature_2} (scaled)',\n",
    "    template='plotly_white',\n",
    "    height=600,\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Observations:**\n",
    "- Boundaries are **irregular** (not straight lines like logistic regression)\n",
    "- Boundaries adapt to local data structure\n",
    "- \"Majority vote\" creates these regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Visualize Prediction Process\n",
    "\n",
    "Let's see how KNN makes a prediction for a single test point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a test point\n",
    "test_point_idx = 0\n",
    "test_point = X_test_2d[test_point_idx:test_point_idx+1]\n",
    "true_label = y_test.iloc[test_point_idx]\n",
    "\n",
    "# Find K nearest neighbors\n",
    "distances, indices = knn_2d.kneighbors(test_point)\n",
    "neighbor_labels = y_train.iloc[indices[0]]\n",
    "\n",
    "# Make prediction\n",
    "prediction = knn_2d.predict(test_point)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the prediction process\n",
    "fig = go.Figure()\n",
    "\n",
    "# All training points (faded)\n",
    "for class_val, color in [(0, '#E74C3C'), (1, '#3498DB'), (2, '#2ECC71')]:\n",
    "    mask = y_train == class_val\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_train_2d[mask, 0],\n",
    "        y=X_train_2d[mask, 1],\n",
    "        mode='markers',\n",
    "        name=f'Class {class_val} (train)',\n",
    "        marker=dict(color=color, size=6, opacity=0.3),\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "# K nearest neighbors (highlighted)\n",
    "neighbors_2d = X_train_2d[indices[0]]\n",
    "for i, (neighbor, label, dist) in enumerate(zip(neighbors_2d, neighbor_labels, distances[0])):\n",
    "    color = {0: '#E74C3C', 1: '#3498DB', 2: '#2ECC71'}[label]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[neighbor[0]],\n",
    "        y=[neighbor[1]],\n",
    "        mode='markers',\n",
    "        name=f'Neighbor {i+1} (Class {label}, d={dist:.2f})',\n",
    "        marker=dict(color=color, size=15, line=dict(width=2, color='black'))\n",
    "    ))\n",
    "    \n",
    "    # Draw line to test point\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[test_point[0, 0], neighbor[0]],\n",
    "        y=[test_point[0, 1], neighbor[1]],\n",
    "        mode='lines',\n",
    "        line=dict(color='gray', width=1, dash='dash'),\n",
    "        showlegend=False,\n",
    "        hoverinfo='skip'\n",
    "    ))\n",
    "\n",
    "# Test point\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[test_point[0, 0]],\n",
    "    y=[test_point[0, 1]],\n",
    "    mode='markers',\n",
    "    name=f'Test Point (True: Class {true_label}, Pred: Class {prediction})',\n",
    "    marker=dict(\n",
    "        color='yellow',\n",
    "        size=20,\n",
    "        symbol='star',\n",
    "        line=dict(width=2, color='black')\n",
    "    )\n",
    "))\n",
    "\n",
    "# Vote count\n",
    "vote_counts = neighbor_labels.value_counts().to_dict()\n",
    "vote_text = ', '.join([f\"Class {k}: {v} votes\" for k, v in sorted(vote_counts.items())])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'KNN Prediction Process (K={optimal_k})<br>{vote_text}<br>Prediction: Class {prediction}',\n",
    "    xaxis_title=f'{feature_1} (scaled)',\n",
    "    yaxis_title=f'{feature_2} (scaled)',\n",
    "    template='plotly_white',\n",
    "    height=600,\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° This is how KNN works:**\n",
    "1. Find K=5 closest neighbors (connected by gray lines)\n",
    "2. Each neighbor \"votes\" for its class\n",
    "3. Majority class wins!\n",
    "4. No complex math, just distance and voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Computational Considerations\n",
    "\n",
    "### ‚ö° KNN Trade-offs\n",
    "\n",
    "**Training Time:** Very fast! ‚úÖ\n",
    "- Just stores data\n",
    "- No model \"learning\"\n",
    "- O(1) complexity\n",
    "\n",
    "**Prediction Time:** Can be slow! ‚ö†Ô∏è\n",
    "- Calculate distance to ALL training points\n",
    "- O(n √ó d) where n=samples, d=features\n",
    "- Problem for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Compare prediction times\n",
    "timing_results = []\n",
    "\n",
    "for k in [1, 5, 15, 50]:\n",
    "    knn_temp = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_temp.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Time predictions\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = knn_temp.predict(X_test_scaled)\n",
    "    avg_time = (time.time() - start) / 100\n",
    "    \n",
    "    timing_results.append({'K': k, 'Avg Prediction Time (ms)': avg_time * 1000})\n",
    "\n",
    "timing_df = pd.DataFrame(timing_results)\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=timing_df['K'].astype(str),\n",
    "        y=timing_df['Avg Prediction Time (ms)'],\n",
    "        text=[f\"{x:.2f} ms\" for x in timing_df['Avg Prediction Time (ms)']],\n",
    "        textposition='auto',\n",
    "        marker_color='lightcoral'\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Prediction Time vs K',\n",
    "    xaxis_title='K (Number of Neighbors)',\n",
    "    yaxis_title='Avg Time (milliseconds)',\n",
    "    template='plotly_white',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Key Takeaways\n",
    "\n",
    "### ‚úÖ What We Learned:\n",
    "\n",
    "**1. KNN Core Concepts:**\n",
    "- **Lazy learning:** No training phase, just memorize data\n",
    "- **Instance-based:** Uses actual training examples\n",
    "- **Non-parametric:** No assumptions about data distribution\n",
    "- **Majority voting:** Prediction based on neighbors\n",
    "\n",
    "**2. Key Hyperparameters:**\n",
    "- **K (neighbors):**\n",
    "  - Small K ‚Üí Complex boundary, overfitting risk\n",
    "  - Large K ‚Üí Simple boundary, underfitting risk\n",
    "  - Odd K avoids ties in binary classification\n",
    "- **Distance metric:**\n",
    "  - Euclidean (default, works well)\n",
    "  - Manhattan (for grid-like spaces)\n",
    "  - Minkowski (generalization)\n",
    "\n",
    "**3. Critical Requirements:**\n",
    "- ‚ö†Ô∏è **MUST scale features!**\n",
    "- Clean data (outliers strongly impact)\n",
    "- Consider curse of dimensionality\n",
    "\n",
    "**4. When to Use KNN:**\n",
    "- ‚úÖ Small to medium datasets (< 10K samples)\n",
    "- ‚úÖ Low dimensional data (< 20 features)\n",
    "- ‚úÖ Non-linear boundaries\n",
    "- ‚úÖ Need simple, interpretable model\n",
    "- ‚úÖ Continuous numeric features\n",
    "\n",
    "**5. When NOT to Use KNN:**\n",
    "- ‚ùå Large datasets (slow predictions)\n",
    "- ‚ùå High dimensional data (curse of dimensionality)\n",
    "- ‚ùå Need fast predictions\n",
    "- ‚ùå Categorical features (distance unclear)\n",
    "- ‚ùå Unbalanced classes\n",
    "\n",
    "**6. Advantages:**\n",
    "- Simple to understand\n",
    "- No training time\n",
    "- Naturally handles multi-class\n",
    "- Adapts to new data easily\n",
    "\n",
    "**7. Disadvantages:**\n",
    "- Slow predictions\n",
    "- Memory intensive (stores all data)\n",
    "- Sensitive to irrelevant features\n",
    "- Requires feature scaling\n",
    "\n",
    "---\n",
    "\n",
    "### üç∑ Real-World Application:\n",
    "\n",
    "Our KNN model achieved **{:.1%} accuracy** on wine classification:\n",
    "- Successfully distinguished 3 wine types\n",
    "- Optimal K={} neighbors\n",
    "- Clear decision boundaries based on chemical properties\n",
    "\n",
    "This demonstrates KNN's effectiveness for:\n",
    "- Quality control\n",
    "- Product authentication\n",
    "- Multi-class problems with clear clusters\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Best Practices:\n",
    "\n",
    "1. **Always scale features** - KNN is distance-based\n",
    "2. **Try odd K values** - Avoid tie-breaking issues\n",
    "3. **Use cross-validation** - Find optimal K\n",
    "4. **Consider dataset size** - KNN doesn't scale well\n",
    "5. **Remove irrelevant features** - They add noise to distance\n",
    "6. **Use KD-trees or Ball-trees** - For faster neighbor search (if needed)\n",
    "\n",
    "---\n",
    "\n",
    "**AI Tech Institute** | *Building Tomorrow's AI Engineers Today*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
