{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 10: Classification Model Zoo\n",
    "**Instructor:** Amir Charkhi | **Goal:** Compare All Classification Algorithms\n",
    "\n",
    "### Learning Objectives\n",
    "- Train and compare 8 different classification algorithms\n",
    "- Understand strengths and weaknesses of each\n",
    "- Learn when to use which algorithm\n",
    "- Make data-driven model selection decisions\n",
    "\n",
    "### The 8 Models We'll Compare:\n",
    "1. **Logistic Regression** - Simple baseline\n",
    "2. **Ridge Classifier** - Regularized linear model\n",
    "3. **Linear SVC** - Maximum margin (linear)\n",
    "4. **SVM with RBF Kernel** - Non-linear patterns\n",
    "5. **K-Nearest Neighbors (KNN)** - Instance-based learning\n",
    "6. **Decision Tree** - Interpretable but overfits\n",
    "7. **Random Forest** - Ensemble of trees\n",
    "8. **Gradient Boosting** - Sequential boosting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "We'll need all our classification algorithms and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Core libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn: Data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"‚úÖ Data preparation tools imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn: Classification models\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "print(\"‚úÖ All 8 classification algorithms imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn: Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Evaluation metrics imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ CLASSIFICATION MODEL ZOO - All libraries ready!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load and Prepare Dataset\n",
    "\n",
    "We'll use the **Online Shoppers Purchasing Intention** dataset.\n",
    "\n",
    "**Goal:** Predict if a visitor will make a purchase (Revenue: True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv'\n",
    "\n",
    "print(\"üì• Loading Online Shoppers dataset...\")\n",
    "df_raw = pd.read_csv(url)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded: {df_raw.shape[0]:,} rows √ó {df_raw.shape[1]} columns\")\n",
    "print(f\"\\nüìä Target distribution:\")\n",
    "print(df_raw['Revenue'].value_counts())\n",
    "print(f\"\\nüí° Class imbalance: {df_raw['Revenue'].value_counts()[True]/len(df_raw)*100:.1f}% positive class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "print(\"üßπ Preprocessing data...\")\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Convert target to binary\n",
    "df['Revenue'] = df['Revenue'].astype(int)\n",
    "\n",
    "# Encode Month\n",
    "month_map = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'June': 6,\n",
    "             'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}\n",
    "df['Month'] = df['Month'].map(month_map)\n",
    "\n",
    "# One-hot encode VisitorType\n",
    "visitor_dummies = pd.get_dummies(df['VisitorType'], prefix='Visitor', drop_first=True)\n",
    "df = pd.concat([df, visitor_dummies.astype(int)], axis=1)\n",
    "df = df.drop(columns=['VisitorType'])\n",
    "\n",
    "# Convert Weekend\n",
    "df['Weekend'] = df['Weekend'].astype(int)\n",
    "\n",
    "print(\"‚úÖ Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "feature_cols = [col for col in df.columns if col != 'Revenue']\n",
    "X = df[feature_cols].copy()\n",
    "y = df['Revenue'].copy()\n",
    "\n",
    "print(f\"üìä Features: {len(feature_cols)} columns\")\n",
    "print(f\"üìä Total samples: {len(X):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Train-Test Split\n",
    "\n",
    "‚ö†Ô∏è **Important:** Use stratification to maintain class balance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"‚úÇÔ∏è Data Split:\")\n",
    "print(f\"   Training:   {len(X_train):>6,} samples ({len(X_train)/len(X)*100:.0f}%)\")\n",
    "print(f\"   Testing:    {len(X_test):>6,} samples ({len(X_test)/len(X)*100:.0f}%)\")\n",
    "print(f\"\\n‚úÖ Class balance maintained:\")\n",
    "print(f\"   Train: {y_train.mean():.3f} positive class\")\n",
    "print(f\"   Test:  {y_test.mean():.3f} positive class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Scaling\n",
    "\n",
    "**Required for:** Logistic Regression, SVC, KNN  \n",
    "**Not required for:** Tree-based models\n",
    "\n",
    "We'll scale for consistency and to support all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚öñÔ∏è Features scaled using StandardScaler\")\n",
    "print(f\"   Mean ‚âà {X_train_scaled.mean():.6f}\")\n",
    "print(f\"   Std  ‚âà {X_train_scaled.std():.6f}\")\n",
    "print(\"\\n‚úÖ Scaled features ready for all models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Helper Function: Evaluate Models\n",
    "\n",
    "We'll create a reusable function to evaluate all models consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate a classification model.\n",
    "    Returns metrics dictionary.\n",
    "    \"\"\"\n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get probabilities if available\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        y_decision = model.decision_function(X_test)\n",
    "        auc = roc_auc_score(y_test, y_decision)\n",
    "    else:\n",
    "        auc = None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'ROC-AUC': auc,\n",
    "        'Train Time (s)': train_time\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Accuracy:  {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['Precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['Recall']:.4f}\")\n",
    "    print(f\"F1-Score:  {metrics['F1-Score']:.4f}\")\n",
    "    if auc is not None:\n",
    "        print(f\"ROC-AUC:   {metrics['ROC-AUC']:.4f}\")\n",
    "    print(f\"Train Time: {train_time:.3f}s\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"‚úÖ Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Train All Models\n",
    "\n",
    "Now we'll train all 8 classifiers one by one!\n",
    "\n",
    "### Model 1: Logistic Regression\n",
    "**Type:** Linear model  \n",
    "**Pros:** Fast, interpretable, probability estimates  \n",
    "**Best for:** Baseline, interpretability needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_metrics = evaluate_model(lr_model, X_train_scaled, y_train, X_test_scaled, y_test, \n",
    "                             \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Ridge Classifier\n",
    "**Type:** Regularized linear model  \n",
    "**Pros:** Very fast, handles multicollinearity  \n",
    "**Cons:** No probability estimates  \n",
    "**Best for:** Large datasets, speed matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Classifier\n",
    "ridge_model = RidgeClassifier(random_state=42)\n",
    "ridge_metrics = evaluate_model(ridge_model, X_train_scaled, y_train, X_test_scaled, y_test,\n",
    "                                \"Ridge Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Linear SVC\n",
    "**Type:** Support Vector Machine (linear)  \n",
    "**Pros:** Maximum margin, good for high dimensions  \n",
    "**Cons:** Sensitive to C parameter  \n",
    "**Best for:** Linearly separable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC\n",
    "linear_svc_model = LinearSVC(random_state=42, max_iter=2000, dual=False)\n",
    "linear_svc_metrics = evaluate_model(linear_svc_model, X_train_scaled, y_train, \n",
    "                                     X_test_scaled, y_test, \"Linear SVC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: SVM with RBF Kernel ‚≠ê\n",
    "**Type:** Support Vector Machine (non-linear)  \n",
    "**Pros:** Handles non-linear patterns, powerful  \n",
    "**Cons:** Slower than linear, needs tuning  \n",
    "**Best for:** Complex decision boundaries\n",
    "\n",
    "üí° **Reminder:** See `theory_svm.ipynb` for kernel trick explanation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with RBF Kernel\n",
    "svm_rbf_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_rbf_metrics = evaluate_model(svm_rbf_model, X_train_scaled, y_train, \n",
    "                                  X_test_scaled, y_test, \"SVM (RBF Kernel)\")\n",
    "\n",
    "print(f\"\\nüí° Support vectors: {len(svm_rbf_model.support_vectors_)}/{len(X_train)} \"\n",
    "      f\"({len(svm_rbf_model.support_vectors_)/len(X_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: K-Nearest Neighbors ‚≠ê\n",
    "**Type:** Instance-based learning  \n",
    "**Pros:** Simple, no training, non-parametric  \n",
    "**Cons:** Slow prediction, memory intensive  \n",
    "**Best for:** Small datasets, as baseline\n",
    "\n",
    "üí° **Reminder:** See `theory_knn.ipynb` for choosing K!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors\n",
    "# Using K=5 as a good starting point\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_metrics = evaluate_model(knn_model, X_train_scaled, y_train, \n",
    "                              X_test_scaled, y_test, \"K-Nearest Neighbors (K=5)\")\n",
    "\n",
    "print(f\"\\nüí° Using K=5 neighbors for each prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Decision Tree\n",
    "**Type:** Tree-based (single tree)  \n",
    "**Pros:** Interpretable, no scaling needed  \n",
    "**Cons:** Prone to overfitting  \n",
    "**Best for:** Understanding feature interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree (with some regularization)\n",
    "dt_model = DecisionTreeClassifier(max_depth=10, min_samples_split=20, random_state=42)\n",
    "dt_metrics = evaluate_model(dt_model, X_train, y_train, X_test, y_test, \n",
    "                             \"Decision Tree\")\n",
    "\n",
    "print(f\"\\nüí° Tree depth: {dt_model.get_depth()}, Leaves: {dt_model.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: Random Forest üåü\n",
    "**Type:** Ensemble (bagging)  \n",
    "**Pros:** Excellent performance, robust, feature importance  \n",
    "**Cons:** Less interpretable, larger model  \n",
    "**Best for:** Strong out-of-box performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_metrics = evaluate_model(rf_model, X_train, y_train, X_test, y_test, \n",
    "                             \"Random Forest\")\n",
    "\n",
    "print(f\"\\nüí° Ensemble of {rf_model.n_estimators} decision trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8: Gradient Boosting üåü\n",
    "**Type:** Ensemble (boosting)  \n",
    "**Pros:** Often best performance, powerful  \n",
    "**Cons:** Slower training, more hyperparameters  \n",
    "**Best for:** When you need maximum accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \n",
    "                                      max_depth=5, random_state=42)\n",
    "gb_metrics = evaluate_model(gb_model, X_train, y_train, X_test, y_test, \n",
    "                             \"Gradient Boosting\")\n",
    "\n",
    "print(f\"\\nüí° Sequential ensemble with {gb_model.n_estimators} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Complete Comparison\n",
    "\n",
    "Let's see all models side-by-side!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "all_metrics = [\n",
    "    lr_metrics,\n",
    "    ridge_metrics,\n",
    "    linear_svc_metrics,\n",
    "    svm_rbf_metrics,\n",
    "    knn_metrics,\n",
    "    dt_metrics,\n",
    "    rf_metrics,\n",
    "    gb_metrics\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "# Sort by F1-Score (best metric for imbalanced data)\n",
    "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"üèÜ CLASSIFICATION MODEL COMPARISON - TEST SET PERFORMANCE\")\n",
    "print(\"=\"*90)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Highlight best model\n",
    "best_model = comparison_df.iloc[0]\n",
    "print(f\"\\nü•á BEST MODEL: {best_model['Model']}\")\n",
    "print(f\"   F1-Score: {best_model['F1-Score']:.4f}\")\n",
    "print(f\"   ROC-AUC:  {best_model['ROC-AUC']:.4f}\")\n",
    "print(f\"   Accuracy: {best_model['Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Visual Comparison\n",
    "\n",
    "Let's visualize the performance of all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. F1-Score comparison\n",
    "axes[0, 0].barh(comparison_df['Model'], comparison_df['F1-Score'], color='purple', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('F1-Score', fontsize=11)\n",
    "axes[0, 0].set_title('F1-Score Comparison (Higher is Better)', fontsize=12, pad=15)\n",
    "axes[0, 0].invert_yaxis()\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 2. ROC-AUC comparison\n",
    "comparison_with_auc = comparison_df[comparison_df['ROC-AUC'].notna()]\n",
    "axes[0, 1].barh(comparison_with_auc['Model'], comparison_with_auc['ROC-AUC'], \n",
    "                color='seagreen', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('ROC-AUC', fontsize=11)\n",
    "axes[0, 1].set_title('ROC-AUC Comparison', fontsize=12, pad=15)\n",
    "axes[0, 1].invert_yaxis()\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Precision vs Recall\n",
    "axes[1, 0].scatter(comparison_df['Recall'], comparison_df['Precision'], \n",
    "                   s=200, alpha=0.6, c=range(len(comparison_df)), cmap='viridis')\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    axes[1, 0].annotate(row['Model'], \n",
    "                        (row['Recall'], row['Precision']),\n",
    "                        fontsize=8, ha='right', va='bottom')\n",
    "axes[1, 0].set_xlabel('Recall', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Precision', fontsize=11)\n",
    "axes[1, 0].set_title('Precision vs Recall Tradeoff', fontsize=12, pad=15)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Training Time\n",
    "sorted_by_time = comparison_df.sort_values('Train Time (s)')\n",
    "axes[1, 1].barh(sorted_by_time['Model'], sorted_by_time['Train Time (s)'], \n",
    "                color='coral', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Training Time (seconds)', fontsize=11)\n",
    "axes[1, 1].set_title('Training Time Comparison', fontsize=12, pad=15)\n",
    "axes[1, 1].invert_yaxis()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   - Tree-based models (RF, GB) typically perform best\")\n",
    "print(\"   - KNN is slow to train but simple\")\n",
    "print(\"   - Linear models are fastest\")\n",
    "print(\"   - Precision-Recall tradeoff varies by model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model Categories\n",
    "\n",
    "Let's group models by type and compare categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model categories\n",
    "categories = {\n",
    "    'Logistic Regression': 'Linear',\n",
    "    'Ridge Classifier': 'Linear',\n",
    "    'Linear SVC': 'Linear',\n",
    "    'SVM (RBF Kernel)': 'Non-Linear',\n",
    "    'K-Nearest Neighbors (K=5)': 'Instance-Based',\n",
    "    'Decision Tree': 'Tree-Based',\n",
    "    'Random Forest': 'Ensemble (Bagging)',\n",
    "    'Gradient Boosting': 'Ensemble (Boosting)'\n",
    "}\n",
    "\n",
    "comparison_df['Category'] = comparison_df['Model'].map(categories)\n",
    "\n",
    "# Average performance by category\n",
    "category_performance = comparison_df.groupby('Category').agg({\n",
    "    'Accuracy': 'mean',\n",
    "    'F1-Score': 'mean',\n",
    "    'ROC-AUC': 'mean',\n",
    "    'Train Time (s)': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\nüìä AVERAGE PERFORMANCE BY MODEL CATEGORY:\")\n",
    "print(\"=\"*70)\n",
    "print(category_performance)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize category performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# F1-Score by category\n",
    "category_performance['F1-Score'].sort_values().plot(kind='barh', ax=axes[0], \n",
    "                                                      color='steelblue', alpha=0.7)\n",
    "axes[0].set_xlabel('Average F1-Score', fontsize=11)\n",
    "axes[0].set_title('Average F1-Score by Model Category', fontsize=12, pad=15)\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Training time by category\n",
    "category_performance['Train Time (s)'].sort_values().plot(kind='barh', ax=axes[1], \n",
    "                                                            color='coral', alpha=0.7)\n",
    "axes[1].set_xlabel('Average Training Time (s)', fontsize=11)\n",
    "axes[1].set_title('Average Training Time by Category', fontsize=12, pad=15)\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. When to Use Each Model\n",
    "\n",
    "### üéØ Decision Framework:\n",
    "\n",
    "#### **Need Interpretability?**\n",
    "- ‚úÖ **Logistic Regression** - Clear coefficients\n",
    "- ‚úÖ **Decision Tree** - Visual rules\n",
    "- ‚ùå Avoid: SVM, Neural Networks, Ensembles\n",
    "\n",
    "#### **Need Speed (Real-time)?**\n",
    "- ‚úÖ **Linear Models** (Logistic, Ridge, Linear SVC)\n",
    "- ‚úÖ **Decision Tree** (single tree)\n",
    "- ‚ùå Avoid: KNN, Large Ensembles\n",
    "\n",
    "#### **Need Best Performance?**\n",
    "- ‚úÖ **Gradient Boosting** - Often wins competitions\n",
    "- ‚úÖ **Random Forest** - Great out-of-box\n",
    "- ‚úÖ **SVM (RBF)** - For smaller datasets\n",
    "\n",
    "#### **Small Dataset (<10K samples)?**\n",
    "- ‚úÖ **SVM** - Works well with limited data\n",
    "- ‚úÖ **KNN** - Simple baseline\n",
    "- ‚úÖ **Logistic Regression** - Reliable\n",
    "\n",
    "#### **Large Dataset (>100K samples)?**\n",
    "- ‚úÖ **Linear Models** - Scale well\n",
    "- ‚úÖ **Random Forest** (with subsampling)\n",
    "- ‚ùå Avoid: KNN, SVM with RBF\n",
    "\n",
    "#### **High-Dimensional Data (many features)?**\n",
    "- ‚úÖ **Linear SVC** - Good for sparse data\n",
    "- ‚úÖ **Logistic Regression** with regularization\n",
    "- ‚úÖ **Random Forest** with feature selection\n",
    "- ‚ùå Avoid: KNN (curse of dimensionality)\n",
    "\n",
    "#### **Imbalanced Classes?**\n",
    "- ‚úÖ **Random Forest** - Use `class_weight='balanced'`\n",
    "- ‚úÖ **Gradient Boosting** - Robust to imbalance\n",
    "- ‚úÖ **Logistic Regression** with class weights\n",
    "\n",
    "#### **Non-Linear Patterns?**\n",
    "- ‚úÖ **SVM (RBF)** - Kernel trick\n",
    "- ‚úÖ **Tree-based models** - Natural non-linearity\n",
    "- ‚ùå Avoid: Linear models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Selection Summary\n",
    "\n",
    "### üìã Quick Reference:\n",
    "\n",
    "| Model | Best For | Avoid When |\n",
    "|-------|----------|------------|\n",
    "| **Logistic Regression** | Baseline, interpretability, linear data | Non-linear patterns |\n",
    "| **Ridge Classifier** | Speed, multicollinearity | Need probabilities |\n",
    "| **Linear SVC** | High dimensions, linear separation | Non-linear data |\n",
    "| **SVM (RBF)** | Complex patterns, small data | Large datasets, need speed |\n",
    "| **KNN** | Simple baseline, no assumptions | Large data, high dimensions |\n",
    "| **Decision Tree** | Interpretability, feature interactions | Production (overfits) |\n",
    "| **Random Forest** | Strong performance, robustness | Need interpretability |\n",
    "| **Gradient Boosting** | Maximum accuracy, competitions | Need speed, interpretability |\n",
    "\n",
    "---\n",
    "\n",
    "### üèÜ For This Dataset (Online Shoppers):\n",
    "\n",
    "**Top 3 Models:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 3\n",
    "top_3 = comparison_df.head(3)[['Model', 'F1-Score', 'ROC-AUC', 'Accuracy']]\n",
    "\n",
    "print(\"\\nüèÜ TOP 3 MODELS FOR THIS DATASET:\")\n",
    "print(\"=\"*70)\n",
    "print(top_3.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüí° Recommendation:\")\n",
    "print(f\"   Deploy: {top_3.iloc[0]['Model']}\")\n",
    "print(f\"   Reason: Best F1-Score ({top_3.iloc[0]['F1-Score']:.4f}) for imbalanced classes\")\n",
    "print(f\"   Backup: {top_3.iloc[1]['Model']} (nearly as good)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Feature Importance (Top 3 Models)\n",
    "\n",
    "Let's see which features matter most for our best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from tree-based models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models_to_plot = [\n",
    "    ('Decision Tree', dt_model),\n",
    "    ('Random Forest', rf_model),\n",
    "    ('Gradient Boosting', gb_model)\n",
    "]\n",
    "\n",
    "for idx, (name, model) in enumerate(models_to_plot):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False).head(10)\n",
    "    \n",
    "    axes[idx].barh(importance_df['Feature'], importance_df['Importance'], alpha=0.7)\n",
    "    axes[idx].set_xlabel('Importance', fontsize=10)\n",
    "    axes[idx].set_title(f'{name}\\nTop 10 Features', fontsize=11, pad=15)\n",
    "    axes[idx].invert_yaxis()\n",
    "    axes[idx].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Common Important Features:\")\n",
    "print(\"   - PageValues: Higher values = more likely to purchase\")\n",
    "print(\"   - ProductRelated_Duration: Time on product pages matters\")\n",
    "print(\"   - ExitRates/BounceRates: Negative indicators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Key Takeaways\n",
    "\n",
    "### ‚úÖ What We Learned:\n",
    "\n",
    "**1. Model Diversity Matters**\n",
    "- 8 different approaches to the same problem\n",
    "- Each has unique strengths and weaknesses\n",
    "- No single \"best\" algorithm for all problems\n",
    "\n",
    "**2. Performance Patterns**\n",
    "- **Ensemble methods** (RF, GB) typically top performers\n",
    "- **Linear models** fast but limited for complex patterns\n",
    "- **SVM** powerful but slower\n",
    "- **KNN** simple but computationally expensive\n",
    "\n",
    "**3. Trade-offs to Consider**\n",
    "- **Accuracy vs Speed:** GB best accuracy, Linear fastest\n",
    "- **Accuracy vs Interpretability:** DT interpretable, RF accurate\n",
    "- **Training vs Prediction:** KNN instant training, slow prediction\n",
    "\n",
    "**4. For This Problem (E-Commerce Purchase Prediction)**\n",
    "- Tree-based models work best (non-linear patterns)\n",
    "- PageValues is the most important feature\n",
    "- Class imbalance (~16% purchases) handled well by RF/GB\n",
    "\n",
    "**5. General Guidelines**\n",
    "- Start with **Logistic Regression** (baseline)\n",
    "- Try **Random Forest** (strong out-of-box)\n",
    "- If you need max performance: **Gradient Boosting**\n",
    "- If you need speed: **Linear models**\n",
    "- If you need interpretability: **Logistic Regression** or **Decision Tree**\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Model Selection Flowchart:\n",
    "\n",
    "```\n",
    "START\n",
    "  |\n",
    "  ‚îú‚îÄ Need interpretability? ‚Üí Logistic Regression or Decision Tree\n",
    "  |\n",
    "  ‚îú‚îÄ Need real-time speed? ‚Üí Linear models (Logistic, Ridge, SVC)\n",
    "  |\n",
    "  ‚îú‚îÄ Small dataset (<10K)? ‚Üí SVM, KNN, or Logistic Regression\n",
    "  |\n",
    "  ‚îú‚îÄ Large dataset (>100K)? ‚Üí Linear models or Random Forest\n",
    "  |\n",
    "  ‚îú‚îÄ High dimensions? ‚Üí Linear SVC or Logistic Regression (regularized)\n",
    "  |\n",
    "  ‚îú‚îÄ Need best accuracy? ‚Üí Gradient Boosting or Random Forest\n",
    "  |\n",
    "  ‚îî‚îÄ Unsure? ‚Üí Start with Random Forest (reliable)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Resources:\n",
    "\n",
    "**Theory Notebooks:**\n",
    "- `classification_fundamentals.ipynb` - Encoding & probabilities\n",
    "- `theory_svm.ipynb` - SVM concepts & kernel trick\n",
    "- `theory_knn.ipynb` - KNN intuition & choosing K\n",
    "\n",
    "**What's Next:**\n",
    "- Hyperparameter tuning (Week 10 Session 2)\n",
    "- Threshold optimization\n",
    "- Model deployment (Week 11)\n",
    "- Neural networks (Week 11)\n",
    "\n",
    "---\n",
    "\n",
    "**AI Tech Institute** | *Building Tomorrow's AI Engineers Today*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
