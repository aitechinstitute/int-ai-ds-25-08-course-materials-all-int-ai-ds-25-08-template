{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Understanding K-Nearest Neighbors (KNN)\n",
    "**Instructor:** Amir Charkhi\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand how KNN works (the simplest ML algorithm!)\n",
    "- Learn about distance metrics and choosing K\n",
    "- Understand the bias-variance tradeoff in KNN\n",
    "- Know when to use KNN vs other algorithms\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Core Idea: \"You Are Your Neighbors\"\n",
    "\n",
    "### üéØ **The Intuition:**\n",
    "\n",
    "Imagine you move to a new neighborhood. How do you predict if you'll like it?\n",
    "\n",
    "**KNN's Answer:** Look at your **K nearest neighbors**!\n",
    "- If most neighbors are happy ‚Üí You'll probably be happy\n",
    "- If most neighbors are unhappy ‚Üí You'll probably be unhappy\n",
    "\n",
    "### üìä **Simple Example:**\n",
    "\n",
    "You want to classify a **new point** (?):\n",
    "\n",
    "```\n",
    "    Blue  Blue      ?      Red  Red\n",
    "      ‚Ä¢    ‚Ä¢               ‚Ä¢    ‚Ä¢\n",
    "    Blue               Red\n",
    "      ‚Ä¢                 ‚Ä¢\n",
    "```\n",
    "\n",
    "**Question:** Is the new point (?) Blue or Red?\n",
    "\n",
    "**KNN Solution (K=3):**\n",
    "1. Find the 3 nearest points to ?\n",
    "2. Count their colors: 2 Blue, 1 Red\n",
    "3. **Majority vote:** Blue wins!\n",
    "4. **Prediction:** ? is Blue\n",
    "\n",
    "### üéì **That's it!** KNN is that simple.\n",
    "\n",
    "No complex math, no training, no optimization. Just:\n",
    "1. **Measure distances**\n",
    "2. **Find K neighbors**\n",
    "3. **Take a vote**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How KNN Works: Step by Step\n",
    "\n",
    "### **Step 1: Store All Training Data** üíæ\n",
    "Unlike other algorithms, KNN doesn't \"learn\" or \"train\". It just **remembers** all the training data!\n",
    "\n",
    "```python\n",
    "# That's it! No training needed\n",
    "model.fit(X_train, y_train)  # Just stores the data\n",
    "```\n",
    "\n",
    "### **Step 2: Calculate Distances** üìè\n",
    "When you want to predict a new point:\n",
    "1. Calculate distance from new point to **every** training point\n",
    "2. Most common: **Euclidean distance** (straight-line distance)\n",
    "\n",
    "**2D Euclidean Distance:**\n",
    "$$d = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$$\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "New point: (2, 3)\n",
    "Point A:   (1, 1)\n",
    "\n",
    "Distance = ‚àö[(2-1)¬≤ + (3-1)¬≤] = ‚àö[1 + 4] = ‚àö5 ‚âà 2.24\n",
    "```\n",
    "\n",
    "### **Step 3: Find K Nearest Neighbors** üîç\n",
    "Sort all distances and pick the K smallest ones.\n",
    "\n",
    "**Example (K=3):**\n",
    "```\n",
    "Point    Distance    Class\n",
    "A        2.24        Blue\n",
    "B        3.16        Blue  ‚Üê K=3 neighbors\n",
    "C        4.12        Red\n",
    "D        5.83        Red\n",
    "E        7.21        Blue\n",
    "```\n",
    "\n",
    "### **Step 4: Vote!** üó≥Ô∏è\n",
    "\n",
    "**Classification:** Majority vote\n",
    "- K=3 neighbors: 2 Blue, 1 Red\n",
    "- **Prediction: Blue**\n",
    "\n",
    "**Regression:** Average of neighbors\n",
    "- K=3 neighbors: values [5, 7, 6]\n",
    "- **Prediction: (5+7+6)/3 = 6**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choosing K: The Most Important Decision\n",
    "\n",
    "### **K = Number of neighbors to consider**\n",
    "\n",
    "### üî¥ **K = 1 (Too Small)**\n",
    "```\n",
    "Blue  Blue    ?    Red  Red\n",
    "  ‚Ä¢     ‚Ä¢          ‚Ä¢    ‚Ä¢\n",
    "        ‚Ä¢          ‚Ä¢\n",
    "       ‚Üëclosest\n",
    "```\n",
    "\n",
    "**Problem:**\n",
    "- Very sensitive to noise\n",
    "- One outlier can ruin prediction\n",
    "- **High variance** (overfitting)\n",
    "- Complex, jagged decision boundaries\n",
    "\n",
    "### üü¢ **K = 3-5 (Just Right)**\n",
    "```\n",
    "Blue  Blue    ?    Red  Red\n",
    "  ‚Ä¢     ‚Ä¢          ‚Ä¢    ‚Ä¢\n",
    "  ‚Üë    ‚Üë ‚Üë K=3\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Balanced bias-variance\n",
    "- Robust to noise\n",
    "- Good generalization\n",
    "- Smooth decision boundaries\n",
    "\n",
    "### üîµ **K = Too Large (e.g., K = total points)**\n",
    "```\n",
    "Blue  Blue    ?    Red  Red\n",
    "  ‚Ä¢     ‚Ä¢          ‚Ä¢    ‚Ä¢\n",
    "  ‚Üë    ‚Üë  ‚Üë  ‚Üë    ‚Üë  (all points)\n",
    "```\n",
    "\n",
    "**Problem:**\n",
    "- Too smooth, loses local patterns\n",
    "- Always predicts majority class\n",
    "- **High bias** (underfitting)\n",
    "- Ignores nearby information\n",
    "\n",
    "---\n",
    "\n",
    "### üìä **Effect of K on Decision Boundary:**\n",
    "\n",
    "```\n",
    "K=1              K=5              K=20\n",
    "(Overfits)       (Good)           (Underfits)\n",
    "\n",
    "  ‚ï±‚ï≤             ___               ___________\n",
    " ‚ï±  ‚ï≤           ‚ï±   ‚ï≤             ‚ï±           ‚ï≤\n",
    "‚ï±    ‚ï≤         ‚ï±     ‚ï≤           ‚ï±             ‚ï≤\n",
    "Jagged         Smooth            Too smooth\n",
    "```\n",
    "\n",
    "### ‚úÖ **How to Choose K:**\n",
    "\n",
    "1. **Rule of Thumb:** K = ‚àö(n), where n = number of training samples\n",
    "   - 100 samples ‚Üí K ‚âà 10\n",
    "   - 1000 samples ‚Üí K ‚âà 31\n",
    "\n",
    "2. **Use Cross-Validation:**\n",
    "   - Try K = [1, 3, 5, 7, 9, 11, 15, 20]\n",
    "   - Pick K with best validation score\n",
    "\n",
    "3. **Keep K Odd** (for classification):\n",
    "   - Avoids ties: K=3 ‚úì, K=4 ‚úó\n",
    "   - Exception: multi-class problems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distance Metrics\n",
    "\n",
    "How do we measure \"closeness\"?\n",
    "\n",
    "### **1. Euclidean Distance** üìè (Most Common)\n",
    "**Straight-line distance**\n",
    "\n",
    "$$d = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$$\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Point A: (1, 2, 3)\n",
    "Point B: (4, 6, 8)\n",
    "\n",
    "d = ‚àö[(4-1)¬≤ + (6-2)¬≤ + (8-3)¬≤]\n",
    "  = ‚àö[9 + 16 + 25]\n",
    "  = ‚àö50 ‚âà 7.07\n",
    "```\n",
    "\n",
    "**Use when:** Features are continuous, scale matters\n",
    "\n",
    "### **2. Manhattan Distance** üèôÔ∏è\n",
    "**City-block distance** (like walking on a grid)\n",
    "\n",
    "$$d = \\sum_{i=1}^{n} |x_i - y_i|$$\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Point A: (1, 2)\n",
    "Point B: (4, 6)\n",
    "\n",
    "d = |4-1| + |6-2| = 3 + 4 = 7\n",
    "```\n",
    "\n",
    "**Visual:**\n",
    "```\n",
    "    B\n",
    "    ‚Ä¢\n",
    "    |\n",
    "    |‚îÄ‚îÄ (walk along grid)\n",
    "    |\n",
    "    ‚Ä¢‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Ä¢\n",
    "    A\n",
    "```\n",
    "\n",
    "**Use when:** Features are independent, outliers present\n",
    "\n",
    "### **3. Minkowski Distance** üî¢\n",
    "**Generalized distance** (includes both above)\n",
    "\n",
    "$$d = \\left(\\sum_{i=1}^{n} |x_i - y_i|^p\\right)^{1/p}$$\n",
    "\n",
    "Where:\n",
    "- p=1 ‚Üí Manhattan distance\n",
    "- p=2 ‚Üí Euclidean distance\n",
    "- p=‚àû ‚Üí Chebyshev distance (max difference)\n",
    "\n",
    "### **4. Cosine Similarity** üìê\n",
    "**Measures angle, not distance**\n",
    "\n",
    "$$\\text{similarity} = \\frac{x \\cdot y}{||x|| \\cdot ||y||}$$\n",
    "\n",
    "**Use when:** \n",
    "- Text data (word vectors)\n",
    "- Recommendation systems\n",
    "- Direction matters more than magnitude\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing KNN with Code\n",
    "\n",
    "Let's see KNN in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example 1: Simple KNN Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple dataset\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, \n",
    "                           n_informative=2, n_clusters_per_class=1, \n",
    "                           random_state=42, class_sep=1.5)\n",
    "\n",
    "# Train KNN with K=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Create mesh for decision boundary\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.4, cmap='coolwarm')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=100, edgecolors='k', linewidth=1.5)\n",
    "plt.title('KNN Decision Boundary (K=5)', fontsize=14, pad=15)\n",
    "plt.xlabel('Feature 1', fontsize=11)\n",
    "plt.ylabel('Feature 2', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Notice: Decision boundary is smooth but not linear!\")\n",
    "print(\"   KNN can handle non-linear patterns naturally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example 2: Effect of Different K Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different K values\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "k_values = [1, 5, 20]\n",
    "\n",
    "for idx, k in enumerate(k_values):\n",
    "    # Train KNN\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X, y)\n",
    "    \n",
    "    # Predict on mesh\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.4, cmap='coolwarm')\n",
    "    axes[idx].scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=50, edgecolors='k')\n",
    "    axes[idx].set_title(f'K = {k}', fontsize=13, pad=15)\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"   K=1:  Jagged, complex boundary (overfitting)\")\n",
    "print(\"   K=5:  Smooth, balanced boundary (just right)\")\n",
    "print(\"   K=20: Very smooth, simple boundary (underfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example 3: Finding Optimal K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Try different K values\n",
    "k_range = range(1, 31)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_scores.append(knn.score(X_train, y_train))\n",
    "    test_scores.append(knn.score(X_test, y_test))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, train_scores, marker='o', label='Training Accuracy', linewidth=2)\n",
    "plt.plot(k_range, test_scores, marker='s', label='Testing Accuracy', linewidth=2)\n",
    "plt.xlabel('K (Number of Neighbors)', fontsize=11)\n",
    "plt.ylabel('Accuracy', fontsize=11)\n",
    "plt.title('Finding Optimal K: Training vs Testing Accuracy', fontsize=13, pad=15)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark best K\n",
    "best_k = k_range[np.argmax(test_scores)]\n",
    "best_score = max(test_scores)\n",
    "plt.axvline(best_k, color='red', linestyle='--', linewidth=2, alpha=0.7, label=f'Best K={best_k}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüèÜ Best K: {best_k}\")\n",
    "print(f\"üìä Test Accuracy: {best_score:.3f}\")\n",
    "print(f\"\\nüí° Notice:\")\n",
    "print(f\"   - As K increases, training accuracy decreases\")\n",
    "print(f\"   - Test accuracy peaks at K={best_k}, then declines\")\n",
    "print(f\"   - This is the bias-variance tradeoff in action!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. The Curse of Dimensionality\n",
    "\n",
    "### ‚ö†Ô∏è **Big Problem with KNN:**\n",
    "\n",
    "KNN performs poorly in **high-dimensional spaces** (many features).\n",
    "\n",
    "### **Why?**\n",
    "\n",
    "In high dimensions, **all points are equally far apart!**\n",
    "\n",
    "#### **Example:**\n",
    "\n",
    "**2D (2 features):**\n",
    "```\n",
    "‚Ä¢  ‚Ä¢  ?  ‚Ä¢  ‚Ä¢\n",
    "  Neighbors are close!\n",
    "```\n",
    "\n",
    "**100D (100 features):**\n",
    "```\n",
    "All distances ‚âà same\n",
    "No clear \"neighbors\"\n",
    "```\n",
    "\n",
    "### **What Happens:**\n",
    "1. In 2D: distance range [1, 10]\n",
    "2. In 10D: distance range [8, 12] ‚Üê Compressed!\n",
    "3. In 100D: distance range [9.9, 10.1] ‚Üê Nearly identical!\n",
    "\n",
    "### **Result:**\n",
    "- \"Nearest\" neighbors aren't actually near\n",
    "- K-nearest becomes meaningless\n",
    "- Performance degrades\n",
    "\n",
    "### ‚úÖ **Solutions:**\n",
    "1. **Dimensionality Reduction:** Use PCA to reduce features\n",
    "2. **Feature Selection:** Keep only important features\n",
    "3. **Use Different Algorithm:** Try tree-based or neural networks\n",
    "\n",
    "### **Rule of Thumb:**\n",
    "- KNN works well: < 20 features\n",
    "- KNN struggles: > 50 features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. KNN: Pros and Cons\n",
    "\n",
    "### ‚úÖ **Advantages:**\n",
    "\n",
    "**1. Simple and Intuitive**\n",
    "- Easiest ML algorithm to understand\n",
    "- No complex math\n",
    "- Great for learning\n",
    "\n",
    "**2. No Training Time**\n",
    "- Just stores data\n",
    "- Fast to \"train\"\n",
    "- Good for frequently updating data\n",
    "\n",
    "**3. Non-parametric**\n",
    "- No assumptions about data distribution\n",
    "- Flexible decision boundaries\n",
    "- Can handle complex patterns\n",
    "\n",
    "**4. Naturally Handles Multi-class**\n",
    "- Works for any number of classes\n",
    "- No modifications needed\n",
    "\n",
    "**5. Works for Both Classification and Regression**\n",
    "- Classification: voting\n",
    "- Regression: averaging\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå **Disadvantages:**\n",
    "\n",
    "**1. Slow Prediction (Main Problem!)**\n",
    "- Must calculate distance to ALL training points\n",
    "- 1 million samples = 1 million calculations per prediction\n",
    "- Not suitable for real-time applications\n",
    "\n",
    "**2. Memory Intensive**\n",
    "- Stores entire training dataset\n",
    "- Large datasets = large memory\n",
    "\n",
    "**3. Curse of Dimensionality**\n",
    "- Performance degrades with many features\n",
    "- Distances become meaningless\n",
    "\n",
    "**4. Sensitive to Feature Scaling** ‚ö†Ô∏è\n",
    "```\n",
    "Feature 1: [1, 2, 3]        (range 1-3)\n",
    "Feature 2: [1000, 2000, 3000]  (range 1000-3000)\n",
    "\n",
    "Distance dominated by Feature 2!\n",
    "Must scale features!\n",
    "```\n",
    "\n",
    "**5. Sensitive to Irrelevant Features**\n",
    "- All features contribute to distance\n",
    "- Noise features hurt performance\n",
    "- Need good feature selection\n",
    "\n",
    "**6. Imbalanced Classes**\n",
    "- Majority class dominates voting\n",
    "- Need to use weighted KNN\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. KNN vs Other Algorithms\n",
    "\n",
    "| Aspect | KNN | SVM | Decision Trees | Logistic Regression |\n",
    "|--------|-----|-----|---------------|--------------------|\n",
    "| **Training Time** | Instant ‚ö° | Slow | Fast | Fast |\n",
    "| **Prediction Time** | Slow üêå | Fast | Fast | Fast |\n",
    "| **Memory Usage** | High üíæ | Low | Medium | Low |\n",
    "| **Interpretability** | Medium | Low | High ‚úì | High ‚úì |\n",
    "| **Non-linear Data** | Good ‚úì | Good ‚úì | Excellent ‚úì | Poor |\n",
    "| **High Dimensions** | Poor ‚ùå | Good ‚úì | Good ‚úì | Good ‚úì |\n",
    "| **Feature Scaling** | Required ‚úì | Required ‚úì | Not needed | Required ‚úì |\n",
    "| **Handles Noise** | Medium | Good | Poor | Good |\n",
    "| **Simplicity** | Very Simple ‚úì | Complex | Simple | Simple |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. When to Use KNN\n",
    "\n",
    "### ‚úÖ **Use KNN when:**\n",
    "\n",
    "**1. Small to Medium Datasets**\n",
    "- < 10,000 samples\n",
    "- Prediction speed not critical\n",
    "\n",
    "**2. Low Dimensional Data**\n",
    "- < 20 features\n",
    "- Or after dimensionality reduction\n",
    "\n",
    "**3. Non-linear Decision Boundaries**\n",
    "- Complex patterns\n",
    "- No clear linear separation\n",
    "\n",
    "**4. As a Baseline**\n",
    "- Quick first model\n",
    "- Compare against more complex models\n",
    "\n",
    "**5. Recommendation Systems**\n",
    "- \"Users similar to you also liked...\"\n",
    "- Item similarity\n",
    "\n",
    "**6. Anomaly Detection**\n",
    "- Points far from all neighbors = anomalies\n",
    "- Good for outlier detection\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå **Don't use KNN when:**\n",
    "\n",
    "**1. Large Datasets**\n",
    "- > 100,000 samples\n",
    "- Real-time predictions needed\n",
    "- Use tree-based or linear models\n",
    "\n",
    "**2. High-Dimensional Data**\n",
    "- > 50 features\n",
    "- Text data with many words\n",
    "- Use SVM, trees, or neural networks\n",
    "\n",
    "**3. Need Fast Predictions**\n",
    "- Production systems\n",
    "- Real-time applications\n",
    "- Use pre-trained models\n",
    "\n",
    "**4. Interpretability is Critical**\n",
    "- Need to explain decisions\n",
    "- Regulatory requirements\n",
    "- Use logistic regression or decision trees\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Practical Tips\n",
    "\n",
    "### **1. Always Scale Features!** ‚ö†Ô∏è Critical!\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "### **2. Choose K Carefully**\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_neighbors': [1, 3, 5, 7, 9, 11, 15, 20]}\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "best_k = grid.best_params_['n_neighbors']\n",
    "```\n",
    "\n",
    "### **3. Try Different Distance Metrics**\n",
    "```python\n",
    "# Euclidean (default)\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "\n",
    "# Manhattan\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "\n",
    "# Minkowski (generalized)\n",
    "knn_minkowski = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=3)\n",
    "```\n",
    "\n",
    "### **4. Handle Imbalanced Data**\n",
    "```python\n",
    "# Use weighted voting\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "# Closer neighbors have more influence!\n",
    "```\n",
    "\n",
    "### **5. Reduce Dimensionality First**\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10)  # Reduce to 10 features\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_pca, y_train)\n",
    "```\n",
    "\n",
    "### **6. For Large Datasets: Use Approximations**\n",
    "```python\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Use ball tree or kd tree for faster search\n",
    "knn = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Takeaways\n",
    "\n",
    "### üéØ **Core Concepts:**\n",
    "\n",
    "**1. Simple Principle**\n",
    "- \"You are the average of your K nearest neighbors\"\n",
    "- No training, just memory\n",
    "- Decision by voting (classification) or averaging (regression)\n",
    "\n",
    "**2. Choosing K**\n",
    "- Small K ‚Üí Complex boundary, overfitting\n",
    "- Large K ‚Üí Simple boundary, underfitting\n",
    "- Use cross-validation to find optimal K\n",
    "- Rule of thumb: K = ‚àön\n",
    "\n",
    "**3. Distance Matters**\n",
    "- Euclidean distance most common\n",
    "- Different metrics for different problems\n",
    "- **Always scale features!**\n",
    "\n",
    "**4. Curse of Dimensionality**\n",
    "- KNN fails in high dimensions\n",
    "- Use dimensionality reduction\n",
    "- Or choose different algorithm\n",
    "\n",
    "**5. Trade-offs**\n",
    "- ‚úÖ Simple, no training\n",
    "- ‚ùå Slow predictions, memory intensive\n",
    "\n",
    "---\n",
    "\n",
    "### üí° **Remember:**\n",
    "\n",
    "```\n",
    "KNN = K-Nearest Neighbors\n",
    "\n",
    "1. Find K nearest points\n",
    "2. Take majority vote (classification)\n",
    "   OR take average (regression)\n",
    "3. That's your prediction!\n",
    "```\n",
    "\n",
    "### üéì **Analogy:**\n",
    "\n",
    "KNN is like asking your 5 closest friends for advice:\n",
    "- 3 say \"Yes\" and 2 say \"No\"\n",
    "- You go with the majority: \"Yes\"\n",
    "\n",
    "**The algorithm is literally that simple!**\n",
    "\n",
    "But remember:\n",
    "- Make sure your \"friends\" (neighbors) are actually close\n",
    "- Don't ask too few (K too small) or too many (K too large)\n",
    "- Make sure you're measuring \"closeness\" correctly (distance metric)\n",
    "\n",
    "---\n",
    "\n",
    "### üìä **Quick Reference Card:**\n",
    "\n",
    "| Question | Answer |\n",
    "|----------|--------|\n",
    "| How does KNN work? | Finds K nearest points, takes majority vote |\n",
    "| What's the best K? | Use cross-validation, try ‚àön as starting point |\n",
    "| Do I need to scale? | YES! Always! |\n",
    "| Training time? | Instant (just stores data) |\n",
    "| Prediction time? | Slow (calculates all distances) |\n",
    "| Good for high dimensions? | NO (curse of dimensionality) |\n",
    "| When to use? | Small datasets, low dimensions, as baseline |\n",
    "\n",
    "---\n",
    "\n",
    "**AI Tech Institute** | *Building Tomorrow's AI Engineers Today*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
