{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 05 ¬∑ Notebook 01 ‚Äì Causal Inference Fundamentals\n",
    "**Instructor:** Amir Charkhi  |  **Goal:** Master the foundations of causal reasoning for data-driven decisions.\n",
    "\n",
    "> Format: short theory ‚Üí quick practice ‚Üí build understanding ‚Üí mini-challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Objectives\n",
    "- Distinguish between correlation and causation\n",
    "- Identify and handle confounding variables\n",
    "- Apply Simpson's Paradox to real scenarios\n",
    "- Implement basic causal inference techniques\n",
    "- Build intuition for causal reasoning in AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Causal Inference?\n",
    "Causal inference is the science of **determining whether one event causes another**, not just whether they occur together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries for causal analysis\n",
    "import numpy as np              # Numerical operations\n",
    "import pandas as pd             # Data manipulation\n",
    "import matplotlib.pyplot as plt # Visualization\n",
    "import seaborn as sns          # Statistical plots\n",
    "from scipy import stats        # Statistical tests\n",
    "\n",
    "# Set style for nice plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (8, 4)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world example: Ice cream sales and crime rates\n",
    "np.random.seed(42)\n",
    "days = 30\n",
    "\n",
    "# Generate temperature data\n",
    "temperature = np.random.normal(25, 8, days)\n",
    "\n",
    "print(\"üå°Ô∏è Summer Temperature & City Data:\")\n",
    "print(f\"Average temperature: {np.mean(temperature):.1f}¬∞C\")\n",
    "print(f\"Temperature range: {min(temperature):.1f} - {max(temperature):.1f}¬∞C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Correlation Trap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Spurious Correlations\n",
    "**What looks related** may not be causally connected - third factors often create illusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ice cream sales increase with temperature\n",
    "ice_cream = 50 + 3 * temperature + np.random.normal(0, 10, days)\n",
    "\n",
    "# Crime also increases with temperature (heat effect)\n",
    "crime = 20 + 1.5 * temperature + np.random.normal(0, 5, days)\n",
    "\n",
    "print(\"üìä Correlation Analysis:\")\n",
    "print(f\"Ice cream ‚Üî Crime correlation: {np.corrcoef(ice_cream, crime)[0,1]:.3f}\")\n",
    "print(\"Strong correlation! But does ice cream cause crime?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the spurious relationship\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Plot 1: Ice cream vs Crime\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(ice_cream, crime, alpha=0.6)\n",
    "plt.xlabel('Ice Cream Sales')\n",
    "plt.ylabel('Crime Rate')\n",
    "plt.title('Spurious Correlation')\n",
    "z = np.polyfit(ice_cream, crime, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(ice_cream, p(ice_cream), \"r--\", alpha=0.8)\n",
    "\n",
    "# Plot 2: The true cause\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(temperature, ice_cream, alpha=0.6, label='Ice Cream')\n",
    "plt.scatter(temperature, crime, alpha=0.6, label='Crime')\n",
    "plt.xlabel('Temperature (¬∞C)')\n",
    "plt.ylabel('Values')\n",
    "plt.title('The Real Cause: Temperature')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Understanding Confounders\n",
    "**Confounders** are variables that influence both your suspected cause AND the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confounder example: Education level\n",
    "n_people = 100\n",
    "education_years = np.random.uniform(8, 20, n_people)\n",
    "\n",
    "# Education affects both income and health\n",
    "income = 20000 + 3000 * education_years + np.random.normal(0, 5000, n_people)\n",
    "health_score = 40 + 2 * education_years + np.random.normal(0, 5, n_people)\n",
    "\n",
    "print(\"üéì Education as a Confounder:\")\n",
    "print(f\"Income ‚Üî Health correlation: {np.corrcoef(income, health_score)[0,1]:.3f}\")\n",
    "print(\"But income doesn't directly cause health here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 ‚Äì Identify the Confounder (easy)**  \n",
    "Find the hidden confounder in each scenario:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n",
    "scenarios = [\n",
    "    \"Coffee drinkers have more heart disease (hint: age)\",\n",
    "    \"Countries with more TVs have longer life expectancy (hint: wealth)\", \n",
    "    \"Shoe size correlates with reading ability in children (hint: ?)\",\n",
    "    \"Hospital stays correlate with death rates (hint: ?)\"\n",
    "]\n",
    "\n",
    "# Identify the confounder for scenarios 3 and 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "confounders = [\n",
    "    \"Age - older people drink more coffee AND have more heart disease\",\n",
    "    \"Wealth - richer countries have more TVs AND better healthcare\",\n",
    "    \"Age - older children have bigger feet AND read better\",\n",
    "    \"Illness severity - sicker people stay longer AND die more often\"\n",
    "]\n",
    "\n",
    "for i, (scenario, confounder) in enumerate(zip(scenarios, confounders)):\n",
    "    print(f\"{i+1}. {scenario}\")\n",
    "    print(f\"   ‚Üí Confounder: {confounder}\\n\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simpson's Paradox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 When Aggregated Data Lies\n",
    "Simpson's Paradox: A trend that appears in groups of data **reverses** when groups are combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# University admissions example\n",
    "# Department A: Easy, more men apply\n",
    "dept_a_men = {'applied': 450, 'admitted': 270}  # 60% admission\n",
    "dept_a_women = {'applied': 50, 'admitted': 40}  # 80% admission\n",
    "\n",
    "# Department B: Hard, more women apply  \n",
    "dept_b_men = {'applied': 50, 'admitted': 15}    # 30% admission\n",
    "dept_b_women = {'applied': 450, 'admitted': 158} # 35% admission\n",
    "\n",
    "print(\"üéì Admissions by Department:\")\n",
    "print(f\"Dept A - Men: {270/450:.1%}, Women: {40/50:.1%}\")\n",
    "print(f\"Dept B - Men: {15/50:.1%}, Women: {158/450:.1%}\")\n",
    "print(\"Women have higher rates in BOTH departments!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But look at overall rates!\n",
    "total_men_applied = 450 + 50\n",
    "total_men_admitted = 270 + 15\n",
    "total_women_applied = 50 + 450\n",
    "total_women_admitted = 40 + 158\n",
    "\n",
    "print(\"\\nüìä Overall Admission Rates:\")\n",
    "print(f\"Men overall: {total_men_admitted/total_men_applied:.1%}\")\n",
    "print(f\"Women overall: {total_women_admitted/total_women_applied:.1%}\")\n",
    "print(\"‚ö†Ô∏è PARADOX: Men have higher overall rate despite lower rates in each dept!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Simpson's Paradox\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Department-specific rates\n",
    "departments = ['Dept A\\nMen', 'Dept A\\nWomen', 'Dept B\\nMen', 'Dept B\\nWomen']\n",
    "rates = [60, 80, 30, 35]\n",
    "colors = ['lightblue', 'pink', 'lightblue', 'pink']\n",
    "\n",
    "ax1.bar(departments, rates, color=colors)\n",
    "ax1.set_ylabel('Admission Rate (%)')\n",
    "ax1.set_title('By Department: Women Win Both')\n",
    "ax1.set_ylim(0, 100)\n",
    "\n",
    "# Overall rates\n",
    "overall_labels = ['Men Overall', 'Women Overall']\n",
    "overall_rates = [total_men_admitted/total_men_applied * 100,\n",
    "                total_women_admitted/total_women_applied * 100]\n",
    "overall_colors = ['lightblue', 'pink']\n",
    "\n",
    "ax2.bar(overall_labels, overall_rates, color=overall_colors)\n",
    "ax2.set_ylabel('Admission Rate (%)')\n",
    "ax2.set_title('Overall: Men Win (Paradox!)')\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Why Simpson's Paradox Matters\n",
    "This paradox appears in:\n",
    "- Medical trials (treatment effectiveness)\n",
    "- Hiring decisions (discrimination analysis)  \n",
    "- ML model performance (across subgroups)\n",
    "- A/B testing (segment analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 ‚Äì Detect Simpson's Paradox (medium)**  \n",
    "Analyze treatment effectiveness data for a paradox:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n",
    "# Treatment outcomes by age group\n",
    "young_treatment = {'patients': 100, 'recovered': 85}  # 85% recovery\n",
    "young_control = {'patients': 300, 'recovered': 240}   # 80% recovery\n",
    "\n",
    "old_treatment = {'patients': 300, 'recovered': 180}   # 60% recovery  \n",
    "old_control = {'patients': 100, 'recovered': 55}      # 55% recovery\n",
    "\n",
    "# Calculate and compare rates\n",
    "# Does treatment work in each group?\n",
    "# Does treatment work overall?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "print(\"üíä Treatment Effectiveness Analysis:\")\n",
    "print(\"\\nBy Age Group:\")\n",
    "print(f\"Young - Treatment: {85/100:.1%}, Control: {240/300:.1%}\")\n",
    "print(f\"Old - Treatment: {180/300:.1%}, Control: {55/100:.1%}\")\n",
    "print(\"‚úÖ Treatment beats control in BOTH age groups!\")\n",
    "\n",
    "# Calculate overall rates\n",
    "total_treatment_patients = 100 + 300  \n",
    "total_treatment_recovered = 85 + 180\n",
    "total_control_patients = 300 + 100\n",
    "total_control_recovered = 240 + 55\n",
    "\n",
    "print(\"\\nOverall Rates:\")\n",
    "print(f\"Treatment overall: {total_treatment_recovered/total_treatment_patients:.1%}\")\n",
    "print(f\"Control overall: {total_control_recovered/total_control_patients:.1%}\")\n",
    "print(\"‚ö†Ô∏è PARADOX: Control beats treatment overall!\")\n",
    "\n",
    "print(\"\\nüìù Explanation:\")\n",
    "print(\"More old patients got treatment (harder cases)\")\n",
    "print(\"More young patients got control (easier cases)\")\n",
    "print(\"This allocation created the paradox!\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Causal Inference Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Controlling for Confounders\n",
    "Remove the influence of confounding variables to isolate causal effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with confounding\n",
    "n_customers = 200\n",
    "age = np.random.uniform(20, 60, n_customers)\n",
    "\n",
    "# Age affects both premium membership and spending\n",
    "premium_prob = 1 / (1 + np.exp(-(age - 40) / 10))  # Older ‚Üí premium\n",
    "is_premium = np.random.binomial(1, premium_prob)\n",
    "\n",
    "# Spending depends on age AND premium status\n",
    "spending = 100 + 2*age + 50*is_premium + np.random.normal(0, 20, n_customers)\n",
    "\n",
    "print(\"üí∞ Customer Spending Analysis:\")\n",
    "print(f\"Premium customers spend: ${spending[is_premium==1].mean():.2f}\")\n",
    "print(f\"Regular customers spend: ${spending[is_premium==0].mean():.2f}\")\n",
    "print(f\"Naive difference: ${spending[is_premium==1].mean() - spending[is_premium==0].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control for age using stratification\n",
    "age_groups = pd.cut(age, bins=3, labels=['Young', 'Middle', 'Old'])\n",
    "\n",
    "print(\"\\nüìä Controlled Analysis (by age group):\")\n",
    "controlled_effects = []\n",
    "\n",
    "for group in ['Young', 'Middle', 'Old']:\n",
    "    mask = age_groups == group\n",
    "    premium_spend = spending[mask & (is_premium==1)].mean()\n",
    "    regular_spend = spending[mask & (is_premium==0)].mean()\n",
    "    effect = premium_spend - regular_spend\n",
    "    \n",
    "    if not np.isnan(effect):\n",
    "        controlled_effects.append(effect)\n",
    "        print(f\"{group}: Premium effect = ${effect:.2f}\")\n",
    "\n",
    "print(f\"\\nTrue causal effect (controlled): ${np.mean(controlled_effects):.2f}\")\n",
    "print(\"(Should be close to $50 - our true effect)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Matching Methods\n",
    "Find similar units in treatment and control groups for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple matching example\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create feature matrix for matching\n",
    "X = np.column_stack([age])\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# For each premium customer, find closest regular customer\n",
    "matched_pairs = []\n",
    "premium_indices = np.where(is_premium == 1)[0]\n",
    "regular_indices = np.where(is_premium == 0)[0]\n",
    "\n",
    "for p_idx in premium_indices[:20]:  # Match first 20\n",
    "    # Find closest regular customer by age\n",
    "    distances = np.abs(age[p_idx] - age[regular_indices])\n",
    "    closest_idx = regular_indices[np.argmin(distances)]\n",
    "    \n",
    "    matched_pairs.append({\n",
    "        'premium_spend': spending[p_idx],\n",
    "        'regular_spend': spending[closest_idx],\n",
    "        'age_diff': abs(age[p_idx] - age[closest_idx])\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze matched pairs\n",
    "matched_df = pd.DataFrame(matched_pairs)\n",
    "matched_effect = matched_df['premium_spend'].mean() - matched_df['regular_spend'].mean()\n",
    "\n",
    "print(\"üë• Matched Pairs Analysis:\")\n",
    "print(f\"Average age difference in pairs: {matched_df['age_diff'].mean():.1f} years\")\n",
    "print(f\"Matched estimate of premium effect: ${matched_effect:.2f}\")\n",
    "print(f\"True effect: $50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3 ‚Äì Causal Analysis (hard)**  \n",
    "Determine if training actually improves employee performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn\n",
    "# Employee performance data\n",
    "np.random.seed(42)\n",
    "n_employees = 150\n",
    "\n",
    "# Prior performance affects training selection\n",
    "prior_performance = np.random.uniform(40, 90, n_employees)\n",
    "training_prob = (prior_performance - 40) / 100  # Better employees more likely to get training\n",
    "got_training = np.random.binomial(1, training_prob)\n",
    "\n",
    "# New performance: prior + training effect + noise\n",
    "true_training_effect = 10\n",
    "new_performance = prior_performance + true_training_effect * got_training + np.random.normal(0, 5, n_employees)\n",
    "\n",
    "# Naive analysis (wrong!)\n",
    "# Your code here...\n",
    "\n",
    "# Controlled analysis (correct!)\n",
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "print(\"üìà Employee Training Analysis:\")\n",
    "\n",
    "# Naive analysis (biased)\n",
    "trained_perf = new_performance[got_training==1].mean()\n",
    "untrained_perf = new_performance[got_training==0].mean()\n",
    "naive_effect = trained_perf - untrained_perf\n",
    "\n",
    "print(f\"\\n‚ùå Naive Analysis:\")\n",
    "print(f\"Trained employees: {trained_perf:.1f}\")\n",
    "print(f\"Untrained employees: {untrained_perf:.1f}\")\n",
    "print(f\"Naive effect: {naive_effect:.1f}\")\n",
    "print(\"(Overestimated because better employees got training!)\")\n",
    "\n",
    "# Control for prior performance\n",
    "performance_groups = pd.cut(prior_performance, bins=4, \n",
    "                           labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "print(f\"\\n‚úÖ Controlled Analysis:\")\n",
    "controlled_effects = []\n",
    "\n",
    "for group in ['Low', 'Medium', 'High', 'Very High']:\n",
    "    mask = performance_groups == group\n",
    "    if sum(mask & (got_training==1)) > 0 and sum(mask & (got_training==0)) > 0:\n",
    "        trained = new_performance[mask & (got_training==1)].mean()\n",
    "        untrained = new_performance[mask & (got_training==0)].mean()\n",
    "        effect = trained - untrained\n",
    "        controlled_effects.append(effect)\n",
    "        print(f\"{group} performers: Training effect = {effect:.1f}\")\n",
    "\n",
    "print(f\"\\nControlled estimate: {np.mean(controlled_effects):.1f}\")\n",
    "print(f\"True effect: {true_training_effect}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(prior_performance[got_training==0], new_performance[got_training==0], \n",
    "           alpha=0.5, label='No Training')\n",
    "plt.scatter(prior_performance[got_training==1], new_performance[got_training==1], \n",
    "           alpha=0.5, label='Training')\n",
    "plt.xlabel('Prior Performance')\n",
    "plt.ylabel('New Performance')\n",
    "plt.title('Performance Change')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "methods = ['Naive\\n(Biased)', 'Controlled\\n(Correct)', 'True\\nEffect']\n",
    "effects = [naive_effect, np.mean(controlled_effects), true_training_effect]\n",
    "colors = ['red', 'green', 'blue']\n",
    "plt.bar(methods, effects, color=colors, alpha=0.7)\n",
    "plt.ylabel('Estimated Training Effect')\n",
    "plt.title('Method Comparison')\n",
    "plt.axhline(y=true_training_effect, color='blue', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. When Causation Matters in AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Real AI Applications\n",
    "Causal inference is crucial for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications = {\n",
    "    \"Recommendation Systems\": \"Did recommendation cause purchase or would user buy anyway?\",\n",
    "    \"Medical AI\": \"Will treatment help this specific patient?\",\n",
    "    \"Hiring Algorithms\": \"Does degree cause performance or just correlate?\",\n",
    "    \"Marketing Attribution\": \"Which ad actually drove the conversion?\",\n",
    "    \"Policy Decisions\": \"Will this intervention reduce churn?\"\n",
    "}\n",
    "\n",
    "for app, question in applications.items():\n",
    "    print(f\"ü§ñ {app}:\")\n",
    "    print(f\"   {question}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Building Causal Intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive example: Marketing campaign\n",
    "np.random.seed(42)\n",
    "n_customers = 1000\n",
    "\n",
    "# Customer features\n",
    "customer_value = np.random.exponential(100, n_customers)\n",
    "engagement = np.random.uniform(0, 1, n_customers)\n",
    "\n",
    "# Campaign targeting (biased toward high-value customers)\n",
    "campaign_prob = customer_value / (customer_value + 100)\n",
    "got_campaign = np.random.binomial(1, campaign_prob)\n",
    "\n",
    "# Purchase outcome\n",
    "base_purchase_prob = 0.1 + 0.3 * (engagement > 0.5)\n",
    "campaign_effect = 0.1  # True effect\n",
    "\n",
    "purchase_prob = base_purchase_prob + campaign_effect * got_campaign\n",
    "purchased = np.random.binomial(1, purchase_prob)\n",
    "\n",
    "print(\"üìß Email Campaign Analysis:\")\n",
    "print(f\"Campaign group purchase rate: {purchased[got_campaign==1].mean():.1%}\")\n",
    "print(f\"No campaign purchase rate: {purchased[got_campaign==0].mean():.1%}\")\n",
    "print(\"But is the difference causal? Let's check!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper causal analysis\n",
    "# Create value groups\n",
    "value_groups = pd.qcut(customer_value, q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "effects = []\n",
    "for group in ['Low', 'Medium', 'High', 'Very High']:\n",
    "    mask = value_groups == group\n",
    "    campaign_rate = purchased[mask & (got_campaign==1)].mean()\n",
    "    no_campaign_rate = purchased[mask & (got_campaign==0)].mean()\n",
    "    \n",
    "    if not (np.isnan(campaign_rate) or np.isnan(no_campaign_rate)):\n",
    "        effect = campaign_rate - no_campaign_rate\n",
    "        effects.append(effect)\n",
    "        print(f\"{group} value customers: {effect:.1%} lift\")\n",
    "\n",
    "print(f\"\\nTrue causal effect: {np.mean(effects):.1%} (should be ~10%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mini-Challenges\n",
    "- **M1 (easy):** Identify three real-world examples of correlation without causation\n",
    "- **M2 (medium):** Create a dataset exhibiting Simpson's Paradox with 3 groups\n",
    "- **M3 (hard):** Implement propensity score matching for causal inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn - try the challenges!\n",
    "# M1: List your examples\n",
    "\n",
    "# M2: Create paradox data\n",
    "# Hint: Make success rates that reverse when aggregated\n",
    "\n",
    "# M3: Advanced matching\n",
    "# Hint: Use logistic regression for propensity scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solutions</b></summary>\n",
    "\n",
    "```python\n",
    "# M1 - Correlation without causation examples\n",
    "print(\"üîç Correlation ‚â† Causation Examples:\")\n",
    "examples = [\n",
    "    \"Ice cream sales ‚Üî Drowning deaths (Confounder: Summer/heat)\",\n",
    "    \"Number of firefighters ‚Üî Fire damage (Confounder: Fire size)\",\n",
    "    \"Hospital quality ‚Üî Death rates (Confounder: Patient severity)\",\n",
    "    \"Organic food sales ‚Üî Autism diagnoses (Confounder: Time/awareness)\",\n",
    "    \"Nicolas Cage movies ‚Üî Pool drownings (Pure coincidence!)\"\n",
    "]\n",
    "\n",
    "for i, ex in enumerate(examples[:3], 1):\n",
    "    print(f\"{i}. {ex}\")\n",
    "\n",
    "# M2 - Create Simpson's Paradox\n",
    "print(\"\\nüé≠ Simpson's Paradox Dataset:\")\n",
    "\n",
    "# Three departments with different standards\n",
    "dept_x = {'male_admit': 20, 'male_total': 100,   # 20% rate\n",
    "         'female_admit': 18, 'female_total': 60}  # 30% rate\n",
    "\n",
    "dept_y = {'male_admit': 30, 'male_total': 60,    # 50% rate\n",
    "         'female_admit': 70, 'female_total': 100} # 70% rate\n",
    "\n",
    "dept_z = {'male_admit': 6, 'male_total': 40,     # 15% rate\n",
    "         'female_admit': 12, 'female_total': 40}  # 30% rate\n",
    "\n",
    "# Check each department\n",
    "for dept_name, dept in [('X', dept_x), ('Y', dept_y), ('Z', dept_z)]:\n",
    "    m_rate = dept['male_admit'] / dept['male_total']\n",
    "    f_rate = dept['female_admit'] / dept['female_total']\n",
    "    print(f\"Dept {dept_name}: M={m_rate:.1%}, F={f_rate:.1%} (F wins)\")\n",
    "\n",
    "# Check overall\n",
    "total_m_admit = 20 + 30 + 6\n",
    "total_m_total = 100 + 60 + 40\n",
    "total_f_admit = 18 + 70 + 12\n",
    "total_f_total = 60 + 100 + 40\n",
    "\n",
    "print(f\"\\nOverall: M={total_m_admit/total_m_total:.1%}, \"\n",
    "      f\"F={total_f_admit/total_f_total:.1%} (Same!)\")\n",
    "\n",
    "# M3 - Propensity Score Matching\n",
    "print(\"\\nüéØ Propensity Score Matching:\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Generate data\n",
    "n = 500\n",
    "X1 = np.random.normal(50, 10, n)\n",
    "X2 = np.random.normal(30, 5, n)\n",
    "\n",
    "# Treatment assignment (biased)\n",
    "treat_prob = 1 / (1 + np.exp(-(-2 + 0.05*X1 + 0.1*X2)))\n",
    "treatment = np.random.binomial(1, treat_prob)\n",
    "\n",
    "# Outcome with treatment effect = 5\n",
    "outcome = 20 + 0.5*X1 + 0.3*X2 + 5*treatment + np.random.normal(0, 3, n)\n",
    "\n",
    "# Calculate propensity scores\n",
    "X = np.column_stack([X1, X2])\n",
    "ps_model = LogisticRegression()\n",
    "ps_model.fit(X, treatment)\n",
    "propensity = ps_model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Match on propensity scores\n",
    "treated_idx = np.where(treatment == 1)[0]\n",
    "control_idx = np.where(treatment == 0)[0]\n",
    "\n",
    "matched_effects = []\n",
    "for t_idx in treated_idx[:50]:  # Match 50 treated units\n",
    "    # Find closest control unit by propensity score\n",
    "    ps_distances = np.abs(propensity[t_idx] - propensity[control_idx])\n",
    "    closest = control_idx[np.argmin(ps_distances)]\n",
    "    \n",
    "    effect = outcome[t_idx] - outcome[closest]\n",
    "    matched_effects.append(effect)\n",
    "\n",
    "print(f\"Propensity-matched effect: {np.mean(matched_effects):.2f}\")\n",
    "print(f\"True effect: 5.0\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up & Next Steps\n",
    "‚úÖ You understand the difference between correlation and causation  \n",
    "‚úÖ You can identify and control for confounding variables  \n",
    "‚úÖ You recognize Simpson's Paradox in data  \n",
    "‚úÖ You can apply basic matching methods for causal inference  \n",
    "‚úÖ You have intuition for when causal analysis matters in AI  \n",
    "\n",
    "**Next:** Practical Causal Analysis - Apply these concepts to real business problems!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
