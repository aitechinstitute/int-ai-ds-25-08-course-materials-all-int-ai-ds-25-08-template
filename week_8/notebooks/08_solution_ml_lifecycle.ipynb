{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Lifecycle - SOLUTION\n",
    "## California Housing Price Prediction\n",
    "\n",
    "**AI Tech Institute**  \n",
    "*Full ML Pipeline: From Data to Deployment*\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the complete ML workflow from data loading to model evaluation\n",
    "- Learn proper data splitting to avoid data leakage\n",
    "- Compare linear and tree-based models\n",
    "- Master cross-validation and hyperparameter tuning\n",
    "- Apply best practices for model evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "**What you need to do:**  \n",
    "Import all necessary libraries for data manipulation, visualization, and machine learning.\n",
    "\n",
    "**Required imports:**\n",
    "- NumPy and Pandas for data handling\n",
    "- Matplotlib and Seaborn for visualization\n",
    "- Scikit-learn for dataset, preprocessing, models, and evaluation\n",
    "\n",
    "**üí° Hint:** Import `train_test_split`, `LinearRegression`, `DecisionTreeRegressor`, `cross_val_score`, `GridSearchCV`, and regression metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import sklearn components\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load the Dataset\n",
    "\n",
    "**What you need to do:**  \n",
    "Load the California Housing dataset using sklearn's built-in dataset.\n",
    "\n",
    "**Theory:**  \n",
    "The California Housing dataset contains information from the 1990 census with features like median income, house age, and location. The target variable is the median house value.\n",
    "\n",
    "**üí° Hint:** Use `fetch_california_housing()` and convert to a pandas DataFrame. Set `as_frame=True` for easy handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Create DataFrame with features\n",
    "df = housing.frame\n",
    "\n",
    "# Display basic information\n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Features: {list(housing.feature_names)}\")\n",
    "print(f\"Target: {housing.target_names[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Initial Data Inspection\n",
    "\n",
    "**What you need to do:**  \n",
    "Perform a quick inspection of the dataset before any splitting.\n",
    "\n",
    "**Tasks:**\n",
    "- Display the first few rows\n",
    "- Check dataset shape\n",
    "- Display feature names and target variable\n",
    "- Check for missing values\n",
    "\n",
    "**üí° Hint:** Use `.head()`, `.shape`, `.info()`, and `.isnull().sum()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing)\n",
    "print(f\"\\n‚úÖ Total missing values: {missing.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Train-Validation-Test Split\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL: Split BEFORE detailed EDA to prevent data leakage!**\n",
    "\n",
    "**What you need to do:**  \n",
    "Split the data into three sets:\n",
    "- **Training set (60%)**: For model training\n",
    "- **Validation set (20%)**: For model selection and hyperparameter tuning\n",
    "- **Test set (20%)**: For final, unbiased evaluation (DO NOT TOUCH until the very end!)\n",
    "\n",
    "**Theory:**  \n",
    "The test set represents unseen data in production. It must remain completely isolated from all training decisions to give an honest estimate of model performance.\n",
    "\n",
    "**üí° Hint:** Use `train_test_split()` twice. First split into train+val (80%) and test (20%), then split train+val into train (75% of 80% = 60% total) and validation (25% of 80% = 20% total). Set `random_state=42` for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# First split: separate test set (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: separate train and validation from temp (75/25 split = 60/20 of total)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Display split sizes\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Data Split Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training set:   {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set:       {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Total:          {len(X):,} samples\")\n",
    "print(\"\\nüîí Test set is now locked and will not be used until final evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Perform EDA ONLY on the training set to avoid data leakage!**\n",
    "\n",
    "**What you need to do:**  \n",
    "Analyze the training data to understand patterns, distributions, and relationships.\n",
    "\n",
    "**Tasks:**\n",
    "1. Display summary statistics for all features\n",
    "2. Visualize target variable distribution (histogram)\n",
    "3. Create a correlation heatmap\n",
    "4. Identify the top 3 features most correlated with the target\n",
    "5. Create scatter plots for top correlated features vs target\n",
    "6. Check for outliers using box plots\n",
    "\n",
    "**üí° Hint:** Use `.describe()`, `plt.hist()`, `sns.heatmap()`, and `sns.scatterplot()` on training data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Training Set - Summary Statistics:\")\n",
    "print(\"=\"*80)\n",
    "print(X_train.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Target Variable - Summary Statistics:\")\n",
    "print(\"=\"*80)\n",
    "print(y_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y_train, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Median House Value (100k $)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Target Variable', fontsize=14, fontweight='bold')\n",
    "plt.axvline(y_train.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {y_train.mean():.2f}')\n",
    "plt.axvline(y_train.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {y_train.median():.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(y_train, vert=True)\n",
    "plt.ylabel('Median House Value (100k $)', fontsize=12)\n",
    "plt.title('Box Plot of Target Variable', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Target variable statistics:\")\n",
    "print(f\"   Mean: ${y_train.mean():.2f} (100k)\")\n",
    "print(f\"   Median: ${y_train.median():.2f} (100k)\")\n",
    "print(f\"   Std: ${y_train.std():.2f} (100k)\")\n",
    "print(f\"   Range: ${y_train.min():.2f} to ${y_train.max():.2f} (100k)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "# Combine training features and target for correlation analysis\n",
    "train_data = X_train.copy()\n",
    "train_data['MedHouseVal'] = y_train\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = train_data.corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8},\n",
    "            fmt='.2f')\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify top correlated features with target\n",
    "target_corr = corr_matrix['MedHouseVal'].drop('MedHouseVal').sort_values(ascending=False)\n",
    "print(\"\\nüìà Feature Correlations with Target (MedHouseVal):\")\n",
    "print(\"=\"*80)\n",
    "for feature, corr in target_corr.items():\n",
    "    print(f\"{feature:20s}: {corr:+.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Top 3 most correlated features: {list(target_corr.head(3).index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for top correlated features\n",
    "top_features = list(target_corr.head(3).index)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    axes[idx].scatter(X_train[feature], y_train, alpha=0.3, s=10)\n",
    "    axes[idx].set_xlabel(feature, fontsize=12)\n",
    "    axes[idx].set_ylabel('Median House Value', fontsize=12)\n",
    "    axes[idx].set_title(f'{feature} vs Target\\n(Correlation: {target_corr[feature]:.3f})', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(X_train[feature], y_train, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[idx].plot(X_train[feature], p(X_train[feature]), \"r--\", linewidth=2, alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Scatter plots created for top 3 features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Baseline Model: Linear Regression\n",
    "\n",
    "**Theory:**  \n",
    "Linear Regression assumes a linear relationship between features and target. It's fast, interpretable, and serves as an excellent baseline. The model learns coefficients (weights) for each feature to minimize the sum of squared errors.\n",
    "\n",
    "**What you need to do:**  \n",
    "Train a Linear Regression model and evaluate it on the validation set.\n",
    "\n",
    "**Tasks:**\n",
    "1. Initialize the Linear Regression model\n",
    "2. Train (fit) the model on training data\n",
    "3. Make predictions on validation set\n",
    "4. Calculate and display:\n",
    "   - Mean Absolute Error (MAE)\n",
    "   - Mean Squared Error (MSE)\n",
    "   - Root Mean Squared Error (RMSE)\n",
    "   - R¬≤ Score\n",
    "\n",
    "**üí° Hint:** Use `.fit()`, `.predict()`, and metrics from `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "print(\"Training Linear Regression model...\")\n",
    "lr_model.fit(X_train, y_train)\n",
    "print(\"‚úÖ Model trained successfully!\\n\")\n",
    "\n",
    "# Display feature coefficients\n",
    "print(\"Feature Coefficients:\")\n",
    "print(\"=\"*80)\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': lr_model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "print(coef_df.to_string(index=False))\n",
    "print(f\"\\nIntercept: {lr_model.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on training and validation sets\n",
    "y_train_pred_lr = lr_model.predict(X_train)\n",
    "y_val_pred_lr = lr_model.predict(X_val)\n",
    "\n",
    "# Calculate metrics for training set\n",
    "train_mae_lr = mean_absolute_error(y_train, y_train_pred_lr)\n",
    "train_mse_lr = mean_squared_error(y_train, y_train_pred_lr)\n",
    "train_rmse_lr = np.sqrt(train_mse_lr)\n",
    "train_r2_lr = r2_score(y_train, y_train_pred_lr)\n",
    "\n",
    "# Calculate metrics for validation set\n",
    "val_mae_lr = mean_absolute_error(y_val, y_val_pred_lr)\n",
    "val_mse_lr = mean_squared_error(y_val, y_val_pred_lr)\n",
    "val_rmse_lr = np.sqrt(val_mse_lr)\n",
    "val_r2_lr = r2_score(y_val, y_val_pred_lr)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LINEAR REGRESSION - PERFORMANCE METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Metric':<25} {'Training Set':>15} {'Validation Set':>15} {'Difference':>15}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'MAE':<25} {train_mae_lr:>15.4f} {val_mae_lr:>15.4f} {abs(train_mae_lr-val_mae_lr):>15.4f}\")\n",
    "print(f\"{'MSE':<25} {train_mse_lr:>15.4f} {val_mse_lr:>15.4f} {abs(train_mse_lr-val_mse_lr):>15.4f}\")\n",
    "print(f\"{'RMSE':<25} {train_rmse_lr:>15.4f} {val_rmse_lr:>15.4f} {abs(train_rmse_lr-val_rmse_lr):>15.4f}\")\n",
    "print(f\"{'R¬≤ Score':<25} {train_r2_lr:>15.4f} {val_r2_lr:>15.4f} {abs(train_r2_lr-val_r2_lr):>15.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"   - The model explains {val_r2_lr*100:.2f}% of variance in validation data\")\n",
    "print(f\"   - Average prediction error: ${val_mae_lr*100:.2f}k\")\n",
    "print(f\"   - RMSE: ${val_rmse_lr*100:.2f}k (penalizes larger errors more)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Cross-Validation for Linear Regression\n",
    "\n",
    "**Theory:**  \n",
    "Cross-validation provides a more robust estimate of model performance by training and evaluating the model multiple times on different subsets of data. K-Fold CV splits data into K folds, trains on K-1 folds, and validates on the remaining fold, rotating through all combinations.\n",
    "\n",
    "**What you need to do:**  \n",
    "Perform 5-fold cross-validation on the training set to get a better estimate of model performance.\n",
    "\n",
    "**Tasks:**\n",
    "1. Use `cross_val_score()` with 5 folds\n",
    "2. Calculate RMSE for each fold (use `scoring='neg_mean_squared_error'` and take square root)\n",
    "3. Display mean and standard deviation of CV scores\n",
    "\n",
    "**üí° Hint:** `cross_val_score()` returns negative MSE, so you need to negate and take the square root. Use `scoring='neg_root_mean_squared_error'` if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "print(\"Performing 5-Fold Cross-Validation for Linear Regression...\\n\")\n",
    "\n",
    "# Calculate negative MSE for each fold\n",
    "cv_scores_mse = cross_val_score(lr_model, X_train, y_train, \n",
    "                                 cv=5, \n",
    "                                 scoring='neg_mean_squared_error',\n",
    "                                 n_jobs=-1)\n",
    "\n",
    "# Convert to RMSE (negate and take square root)\n",
    "cv_scores_rmse = np.sqrt(-cv_scores_mse)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*80)\n",
    "print(\"LINEAR REGRESSION - CROSS-VALIDATION RESULTS (5-Fold)\")\n",
    "print(\"=\"*80)\n",
    "for i, score in enumerate(cv_scores_rmse, 1):\n",
    "    print(f\"Fold {i}: RMSE = {score:.4f}\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f\"Mean RMSE:   {cv_scores_rmse.mean():.4f}\")\n",
    "print(f\"Std RMSE:    {cv_scores_rmse.std():.4f}\")\n",
    "print(f\"Min RMSE:    {cv_scores_rmse.min():.4f}\")\n",
    "print(f\"Max RMSE:    {cv_scores_rmse.max():.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüí° Cross-Validation Insights:\")\n",
    "print(f\"   - Average RMSE across 5 folds: {cv_scores_rmse.mean():.4f}\")\n",
    "print(f\"   - Standard deviation: {cv_scores_rmse.std():.4f}\")\n",
    "print(f\"   - Low std suggests model performance is stable across different data splits\")\n",
    "\n",
    "# Store for comparison\n",
    "lr_cv_rmse_mean = cv_scores_rmse.mean()\n",
    "lr_cv_rmse_std = cv_scores_rmse.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Tree-Based Model: Decision Tree Regressor\n",
    "\n",
    "**Theory:**  \n",
    "Decision Trees partition the feature space into regions through recursive binary splits. They can capture non-linear relationships and interactions between features without requiring feature scaling. However, they tend to overfit if not properly regularized.\n",
    "\n",
    "**What you need to do:**  \n",
    "Train a Decision Tree Regressor and compare its performance to Linear Regression.\n",
    "\n",
    "**Tasks:**\n",
    "1. Initialize a Decision Tree Regressor with `random_state=42`\n",
    "2. Train on training data\n",
    "3. Evaluate on validation set\n",
    "4. Calculate the same metrics as Linear Regression\n",
    "5. Compare performance to Linear Regression\n",
    "\n",
    "**üí° Hint:** Without constraints, Decision Trees can perfectly memorize training data. We'll tune this in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Decision Tree model (no constraints)\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "print(\"Training Decision Tree Regressor (default parameters)...\")\n",
    "dt_model.fit(X_train, y_train)\n",
    "print(\"‚úÖ Model trained successfully!\\n\")\n",
    "\n",
    "# Display tree information\n",
    "print(f\"Tree Depth: {dt_model.get_depth()}\")\n",
    "print(f\"Number of Leaves: {dt_model.get_n_leaves()}\")\n",
    "print(f\"Number of Features Used: {dt_model.n_features_in_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred_dt = dt_model.predict(X_train)\n",
    "y_val_pred_dt = dt_model.predict(X_val)\n",
    "\n",
    "# Calculate metrics for training set\n",
    "train_mae_dt = mean_absolute_error(y_train, y_train_pred_dt)\n",
    "train_mse_dt = mean_squared_error(y_train, y_train_pred_dt)\n",
    "train_rmse_dt = np.sqrt(train_mse_dt)\n",
    "train_r2_dt = r2_score(y_train, y_train_pred_dt)\n",
    "\n",
    "# Calculate metrics for validation set\n",
    "val_mae_dt = mean_absolute_error(y_val, y_val_pred_dt)\n",
    "val_mse_dt = mean_squared_error(y_val, y_val_pred_dt)\n",
    "val_rmse_dt = np.sqrt(val_mse_dt)\n",
    "val_r2_dt = r2_score(y_val, y_val_pred_dt)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DECISION TREE (DEFAULT) - PERFORMANCE METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Metric':<25} {'Training Set':>15} {'Validation Set':>15} {'Difference':>15}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'MAE':<25} {train_mae_dt:>15.4f} {val_mae_dt:>15.4f} {abs(train_mae_dt-val_mae_dt):>15.4f}\")\n",
    "print(f\"{'MSE':<25} {train_mse_dt:>15.4f} {val_mse_dt:>15.4f} {abs(train_mse_dt-val_mse_dt):>15.4f}\")\n",
    "print(f\"{'RMSE':<25} {train_rmse_dt:>15.4f} {val_rmse_dt:>15.4f} {abs(train_rmse_dt-val_rmse_dt):>15.4f}\")\n",
    "print(f\"{'R¬≤ Score':<25} {train_r2_dt:>15.4f} {val_r2_dt:>15.4f} {abs(train_r2_dt-val_r2_dt):>15.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare with Linear Regression\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: Decision Tree vs Linear Regression (Validation Set)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Metric':<25} {'Linear Reg':>15} {'Decision Tree':>15} {'Improvement':>15}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'MAE':<25} {val_mae_lr:>15.4f} {val_mae_dt:>15.4f} {val_mae_lr-val_mae_dt:>+15.4f}\")\n",
    "print(f\"{'RMSE':<25} {val_rmse_lr:>15.4f} {val_rmse_dt:>15.4f} {val_rmse_lr-val_rmse_dt:>+15.4f}\")\n",
    "print(f\"{'R¬≤ Score':<25} {val_r2_lr:>15.4f} {val_r2_dt:>15.4f} {val_r2_dt-val_r2_lr:>+15.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Overfitting Warning:\")\n",
    "print(f\"   - Training R¬≤ = {train_r2_dt:.4f} vs Validation R¬≤ = {val_r2_dt:.4f}\")\n",
    "print(f\"   - Large gap suggests overfitting - the tree memorized training data!\")\n",
    "print(f\"   - Hyperparameter tuning should help constrain the tree complexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Cross-Validation for Decision Tree\n",
    "\n",
    "**What you need to do:**  \n",
    "Perform 5-fold cross-validation on the Decision Tree model.\n",
    "\n",
    "**üí° Hint:** If CV scores vary significantly from validation scores, the model may be overfitting. This motivates hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "print(\"Performing 5-Fold Cross-Validation for Decision Tree...\\n\")\n",
    "\n",
    "cv_scores_mse_dt = cross_val_score(dt_model, X_train, y_train,\n",
    "                                    cv=5,\n",
    "                                    scoring='neg_mean_squared_error',\n",
    "                                    n_jobs=-1)\n",
    "\n",
    "cv_scores_rmse_dt = np.sqrt(-cv_scores_mse_dt)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*80)\n",
    "print(\"DECISION TREE (DEFAULT) - CROSS-VALIDATION RESULTS (5-Fold)\")\n",
    "print(\"=\"*80)\n",
    "for i, score in enumerate(cv_scores_rmse_dt, 1):\n",
    "    print(f\"Fold {i}: RMSE = {score:.4f}\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f\"Mean RMSE:   {cv_scores_rmse_dt.mean():.4f}\")\n",
    "print(f\"Std RMSE:    {cv_scores_rmse_dt.std():.4f}\")\n",
    "print(f\"Min RMSE:    {cv_scores_rmse_dt.min():.4f}\")\n",
    "print(f\"Max RMSE:    {cv_scores_rmse_dt.max():.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare CV results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-VALIDATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Model':<30} {'Mean CV RMSE':>20} {'Std':>15}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Linear Regression':<30} {lr_cv_rmse_mean:>20.4f} {lr_cv_rmse_std:>15.4f}\")\n",
    "print(f\"{'Decision Tree (default)':<30} {cv_scores_rmse_dt.mean():>20.4f} {cv_scores_rmse_dt.std():>15.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store for comparison\n",
    "dt_cv_rmse_mean = cv_scores_rmse_dt.mean()\n",
    "dt_cv_rmse_std = cv_scores_rmse_dt.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Hyperparameter Tuning: Decision Tree\n",
    "\n",
    "**Theory:**  \n",
    "Hyperparameter tuning finds the optimal model configuration that balances bias and variance. For Decision Trees, key hyperparameters include:\n",
    "- `max_depth`: Maximum tree depth (prevents overfitting)\n",
    "- `min_samples_split`: Minimum samples required to split a node\n",
    "- `min_samples_leaf`: Minimum samples required at leaf nodes\n",
    "- `max_features`: Number of features to consider for each split\n",
    "\n",
    "**What you need to do:**  \n",
    "Use GridSearchCV to find the best hyperparameters for the Decision Tree.\n",
    "\n",
    "**Tasks:**\n",
    "1. Define a parameter grid with:\n",
    "   - `max_depth`: [3, 5, 7, 10, None]\n",
    "   - `min_samples_split`: [2, 5, 10]\n",
    "   - `min_samples_leaf`: [1, 2, 4]\n",
    "2. Use GridSearchCV with 5-fold CV\n",
    "3. Fit on training data\n",
    "4. Display best parameters and best CV score\n",
    "5. Evaluate the best model on validation set\n",
    "\n",
    "**üí° Hint:** Use `scoring='neg_mean_squared_error'` and set `n_jobs=-1` to use all CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(\"Starting Hyperparameter Tuning with GridSearchCV...\")\n",
    "print(f\"Testing {len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf'])} combinations\")\n",
    "print(\"This may take a few moments...\\n\")\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=DecisionTreeRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n‚úÖ Hyperparameter tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best parameters\n",
    "print(\"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nBest Parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "best_cv_rmse = np.sqrt(-grid_search.best_score_)\n",
    "print(f\"\\nBest Cross-Validation RMSE: {best_cv_rmse:.4f}\")\n",
    "\n",
    "# Get the best model\n",
    "best_dt_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"\\nBest Tree Information:\")\n",
    "print(f\"  Tree Depth: {best_dt_model.get_depth()}\")\n",
    "print(f\"  Number of Leaves: {best_dt_model.get_n_leaves()}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred_dt_tuned = best_dt_model.predict(X_val)\n",
    "y_train_pred_dt_tuned = best_dt_model.predict(X_train)\n",
    "\n",
    "# Calculate metrics\n",
    "train_mae_dt_tuned = mean_absolute_error(y_train, y_train_pred_dt_tuned)\n",
    "train_rmse_dt_tuned = np.sqrt(mean_squared_error(y_train, y_train_pred_dt_tuned))\n",
    "train_r2_dt_tuned = r2_score(y_train, y_train_pred_dt_tuned)\n",
    "\n",
    "val_mae_dt_tuned = mean_absolute_error(y_val, y_val_pred_dt_tuned)\n",
    "val_rmse_dt_tuned = np.sqrt(mean_squared_error(y_val, y_val_pred_dt_tuned))\n",
    "val_r2_dt_tuned = r2_score(y_val, y_val_pred_dt_tuned)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DECISION TREE (TUNED) - PERFORMANCE METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Metric':<25} {'Training Set':>15} {'Validation Set':>15} {'Difference':>15}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'MAE':<25} {train_mae_dt_tuned:>15.4f} {val_mae_dt_tuned:>15.4f} {abs(train_mae_dt_tuned-val_mae_dt_tuned):>15.4f}\")\n",
    "print(f\"{'RMSE':<25} {train_rmse_dt_tuned:>15.4f} {val_rmse_dt_tuned:>15.4f} {abs(train_rmse_dt_tuned-val_rmse_dt_tuned):>15.4f}\")\n",
    "print(f\"{'R¬≤ Score':<25} {train_r2_dt_tuned:>15.4f} {val_r2_dt_tuned:>15.4f} {abs(train_r2_dt_tuned-val_r2_dt_tuned):>15.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüí° Improvement from Tuning:\")\n",
    "print(f\"   - Validation RMSE: {val_rmse_dt:.4f} ‚Üí {val_rmse_dt_tuned:.4f} (Œî = {val_rmse_dt-val_rmse_dt_tuned:+.4f})\")\n",
    "print(f\"   - Validation R¬≤: {val_r2_dt:.4f} ‚Üí {val_r2_dt_tuned:.4f} (Œî = {val_r2_dt_tuned-val_r2_dt:+.4f})\")\n",
    "print(f\"   - Train-Val gap reduced: {abs(train_r2_dt-val_r2_dt):.4f} ‚Üí {abs(train_r2_dt_tuned-val_r2_dt_tuned):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Model Comparison\n",
    "\n",
    "**What you need to do:**  \n",
    "Create a summary comparison of all models tested.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create a DataFrame or table comparing:\n",
    "   - Linear Regression\n",
    "   - Decision Tree (default)\n",
    "   - Decision Tree (tuned)\n",
    "2. Include metrics: RMSE, MAE, R¬≤\n",
    "3. Identify which model performs best on validation data\n",
    "\n",
    "**üí° Hint:** Store all results in a dictionary and convert to a pandas DataFrame for clean visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "results = {\n",
    "    'Model': ['Linear Regression', 'Decision Tree (Default)', 'Decision Tree (Tuned)'],\n",
    "    'Training RMSE': [train_rmse_lr, train_rmse_dt, train_rmse_dt_tuned],\n",
    "    'Validation RMSE': [val_rmse_lr, val_rmse_dt, val_rmse_dt_tuned],\n",
    "    'Training MAE': [train_mae_lr, train_mae_dt, train_mae_dt_tuned],\n",
    "    'Validation MAE': [val_mae_lr, val_mae_dt, val_mae_dt_tuned],\n",
    "    'Training R¬≤': [train_r2_lr, train_r2_dt, train_r2_dt_tuned],\n",
    "    'Validation R¬≤': [val_r2_lr, val_r2_dt, val_r2_dt_tuned],\n",
    "    'CV RMSE (Mean)': [lr_cv_rmse_mean, dt_cv_rmse_mean, best_cv_rmse],\n",
    "    'Overfitting Gap': [\n",
    "        abs(train_r2_lr - val_r2_lr),\n",
    "        abs(train_r2_dt - val_r2_dt),\n",
    "        abs(train_r2_dt_tuned - val_r2_dt_tuned)\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"=\"*120)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*120)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Identify best model\n",
    "best_model_idx = comparison_df['Validation RMSE'].idxmin()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "best_model_rmse = comparison_df.loc[best_model_idx, 'Validation RMSE']\n",
    "best_model_r2 = comparison_df.loc[best_model_idx, 'Validation R¬≤']\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   - Validation RMSE: {best_model_rmse:.4f}\")\n",
    "print(f\"   - Validation R¬≤: {best_model_r2:.4f}\")\n",
    "print(f\"   - This model will be used for final test set evaluation\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# RMSE Comparison\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, comparison_df['Training RMSE'], width, label='Training', alpha=0.8)\n",
    "axes[0].bar(x + width/2, comparison_df['Validation RMSE'], width, label='Validation', alpha=0.8)\n",
    "axes[0].set_xlabel('Model', fontsize=12)\n",
    "axes[0].set_ylabel('RMSE', fontsize=12)\n",
    "axes[0].set_title('RMSE Comparison: Training vs Validation', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# R¬≤ Comparison\n",
    "axes[1].bar(x - width/2, comparison_df['Training R¬≤'], width, label='Training', alpha=0.8)\n",
    "axes[1].bar(x + width/2, comparison_df['Validation R¬≤'], width, label='Validation', alpha=0.8)\n",
    "axes[1].set_xlabel('Model', fontsize=12)\n",
    "axes[1].set_ylabel('R¬≤ Score', fontsize=12)\n",
    "axes[1].set_title('R¬≤ Score Comparison: Training vs Validation', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Final Evaluation on Test Set\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL: This is your ONE AND ONLY test set evaluation!**\n",
    "\n",
    "**Theory:**  \n",
    "The test set provides an unbiased estimate of how your model will perform on completely unseen data in production. This is your final report card. If you used the test set during development, this number would be artificially optimistic.\n",
    "\n",
    "**What you need to do:**  \n",
    "Evaluate your best model (from validation performance) on the held-out test set.\n",
    "\n",
    "**Tasks:**\n",
    "1. Select your best model based on validation performance\n",
    "2. Make predictions on the test set\n",
    "3. Calculate final metrics: RMSE, MAE, R¬≤\n",
    "4. Compare test set performance to validation performance\n",
    "5. Create a scatter plot: Actual vs Predicted values\n",
    "6. Display residuals distribution\n",
    "\n",
    "**üí° Hint:** If test performance is significantly worse than validation, your model may have overfit to the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model (Decision Tree Tuned based on validation performance)\n",
    "final_model = best_dt_model\n",
    "final_model_name = best_model_name\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üîì UNLOCKING TEST SET FOR FINAL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSelected Model: {final_model_name}\")\n",
    "print(\"\\nMaking predictions on test set...\\n\")\n",
    "\n",
    "# Make predictions on test set\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Calculate final metrics\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL TEST SET RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Metric':<25} {'Training':>15} {'Validation':>15} {'Test':>15}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'MAE':<25} {train_mae_dt_tuned:>15.4f} {val_mae_dt_tuned:>15.4f} {test_mae:>15.4f}\")\n",
    "print(f\"{'RMSE':<25} {train_rmse_dt_tuned:>15.4f} {val_rmse_dt_tuned:>15.4f} {test_rmse:>15.4f}\")\n",
    "print(f\"{'R¬≤ Score':<25} {train_r2_dt_tuned:>15.4f} {val_r2_dt_tuned:>15.4f} {test_r2:>15.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Performance comparison\n",
    "val_test_diff = abs(val_rmse_dt_tuned - test_rmse)\n",
    "print(f\"\\nüìä Performance Analysis:\")\n",
    "print(f\"   - Test RMSE: ${test_rmse*100:.2f}k\")\n",
    "print(f\"   - Test R¬≤: {test_r2:.4f} (model explains {test_r2*100:.2f}% of variance)\")\n",
    "print(f\"   - Validation vs Test RMSE difference: {val_test_diff:.4f}\")\n",
    "\n",
    "if val_test_diff < 0.05:\n",
    "    print(f\"   ‚úÖ Excellent! Model generalizes well to unseen data\")\n",
    "elif val_test_diff < 0.10:\n",
    "    print(f\"   ‚úÖ Good! Model performance is consistent\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Warning: Larger gap may indicate some overfitting to validation set\")\n",
    "\n",
    "print(f\"\\nüí° Final Interpretation:\")\n",
    "print(f\"   - On average, predictions are ${test_mae*100:.2f}k off from actual prices\")\n",
    "print(f\"   - Model can be deployed with expected RMSE of ${test_rmse*100:.2f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot: Actual vs Predicted\n",
    "axes[0].scatter(y_test, y_test_pred, alpha=0.5, s=20)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Values', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Values', fontsize=12)\n",
    "axes[0].set_title('Actual vs Predicted Values (Test Set)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add R¬≤ annotation\n",
    "axes[0].text(0.05, 0.95, f'R¬≤ = {test_r2:.4f}\\nRMSE = {test_rmse:.4f}',\n",
    "             transform=axes[0].transAxes, fontsize=11,\n",
    "             verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - y_test_pred\n",
    "axes[1].scatter(y_test_pred, residuals, alpha=0.5, s=20)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Values', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals (Actual - Predicted)', fontsize=12)\n",
    "axes[1].set_title('Residual Plot (Test Set)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze residuals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Histogram of residuals\n",
    "axes[0].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(residuals.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {residuals.mean():.4f}')\n",
    "axes[0].axvline(residuals.median(), color='green', linestyle='--', linewidth=2,\n",
    "                label=f'Median: {residuals.median():.4f}')\n",
    "axes[0].set_xlabel('Residuals', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Residuals', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot for normality check\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot (Normality Check)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESIDUAL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mean of Residuals:     {residuals.mean():>10.4f} (should be close to 0)\")\n",
    "print(f\"Std of Residuals:      {residuals.std():>10.4f}\")\n",
    "print(f\"Min Residual:          {residuals.min():>10.4f}\")\n",
    "print(f\"Max Residual:          {residuals.max():>10.4f}\")\n",
    "print(f\"Median Abs Residual:   {np.abs(residuals).median():>10.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüí° Residual Insights:\")\n",
    "print(f\"   - Residuals should be randomly distributed around 0\")\n",
    "print(f\"   - If Q-Q plot follows the red line, residuals are normally distributed\")\n",
    "print(f\"   - Patterns in residuals suggest the model is missing important features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Key Takeaways & Next Steps\n",
    "\n",
    "**What you should have learned:**\n",
    "1. ‚úÖ Proper data splitting prevents data leakage\n",
    "2. ‚úÖ EDA helps understand data before modeling\n",
    "3. ‚úÖ Start with simple baselines (Linear Regression)\n",
    "4. ‚úÖ Cross-validation provides robust performance estimates\n",
    "5. ‚úÖ Hyperparameter tuning improves model performance\n",
    "6. ‚úÖ Test set evaluation gives final, unbiased performance\n",
    "\n",
    "**Reflection Questions:**\n",
    "- Which model performed better and why?\n",
    "- How did hyperparameter tuning affect Decision Tree performance?\n",
    "- What's the difference between validation and test set performance?\n",
    "- Which features were most important for prediction?\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Extension Activities\n",
    "\n",
    "**This notebook structure is ready for plug-and-play with other models!**\n",
    "\n",
    "Try replacing the Decision Tree with:\n",
    "- **Random Forest Regressor** (ensemble of trees)\n",
    "- **Gradient Boosting Regressor** (sequential boosting)\n",
    "- **XGBoost Regressor** (optimized gradient boosting)\n",
    "- **LightGBM Regressor** (fast gradient boosting)\n",
    "- **Support Vector Regressor** (SVR)\n",
    "\n",
    "For each new model:\n",
    "1. Follow the same workflow (sections 8-10)\n",
    "2. Use appropriate hyperparameters for that model\n",
    "3. Compare results in section 11\n",
    "4. Update final evaluation if it becomes the best model\n",
    "\n",
    "---\n",
    "\n",
    "**AI Tech Institute** | *Building Tomorrow's AI Engineers Today*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéì MACHINE LEARNING LIFECYCLE - COMPLETION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úÖ Dataset: California Housing ({len(df):,} samples)\")\n",
    "print(f\"‚úÖ Train/Val/Test Split: {len(X_train):,} / {len(X_val):,} / {len(X_test):,}\")\n",
    "print(f\"‚úÖ Models Trained: Linear Regression, Decision Tree (Default & Tuned)\")\n",
    "print(f\"‚úÖ Best Model: {final_model_name}\")\n",
    "print(f\"‚úÖ Final Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"‚úÖ Final Test R¬≤: {test_r2:.4f}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ Congratulations! You've completed the full ML lifecycle!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
