{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 8 - Theory: Linear Models Foundation\n",
    "**Instructor:** Amir Charkhi | **Goal:** Master the mathematics & intuition behind linear models\n",
    "\n",
    "> Understanding what happens under the hood\n",
    "\n",
    "## üìö What You'll Learn Today\n",
    "\n",
    "**Theory Topics:**\n",
    "1. ‚úÖ Linear Regression Mathematics\n",
    "2. ‚úÖ Loss Functions & Optimization\n",
    "3. ‚úÖ Assumptions of Linear Models\n",
    "4. ‚úÖ Logistic Regression Theory\n",
    "5. ‚úÖ Maximum Likelihood Estimation\n",
    "6. ‚úÖ Regularization Theory (Ridge, Lasso, ElasticNet)\n",
    "7. ‚úÖ Bias-Variance Tradeoff\n",
    "\n",
    "**Why Study Theory?**\n",
    "- Understand when models work (and when they don't)\n",
    "- Debug problems effectively\n",
    "- Make better modeling decisions\n",
    "- Build intuition for advanced methods\n",
    "- Communicate results confidently\n",
    "\n",
    "**Time**: 75 minutes | **Prerequisites**: Basic calculus, linear algebra\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"üìê THEORY OF LINEAR MODELS\")\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Linear Regression Theory\n",
    "\n",
    "### 1.1: The Model\n",
    "\n",
    "**General Form:**\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon$$\n",
    "\n",
    "**Matrix Form:**\n",
    "$$\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{y}$ = target vector (n √ó 1)\n",
    "- $\\mathbf{X}$ = feature matrix (n √ó p)\n",
    "- $\\boldsymbol{\\beta}$ = coefficients (p √ó 1)\n",
    "- $\\boldsymbol{\\epsilon}$ = error/noise (n √ó 1)\n",
    "\n",
    "**Key Idea:** We assume a *linear relationship* between features and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the linear assumption\n",
    "np.random.seed(42)\n",
    "x = np.linspace(0, 10, 100)\n",
    "y_perfect = 2 * x + 1\n",
    "y_noisy = y_perfect + np.random.normal(0, 2, 100)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(x, y_noisy, alpha=0.5, s=30, label='Observed data (y)')\n",
    "plt.plot(x, y_perfect, 'r-', linewidth=2, label='True relationship (no noise)')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Linear Relationship with Noise', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: The Goal - Minimize Error\n",
    "\n",
    "We want to find $\\boldsymbol{\\beta}$ that minimizes prediction errors.\n",
    "\n",
    "**Residual (error) for one point:**\n",
    "$$e_i = y_i - \\hat{y}_i = y_i - (\\beta_0 + \\beta_1 x_i)$$\n",
    "\n",
    "**Loss Function - Mean Squared Error (MSE):**\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n} e_i^2$$\n",
    "\n",
    "**Why square the errors?**\n",
    "- Penalizes large errors more\n",
    "- Makes math tractable (differentiable)\n",
    "- Positive values (can't cancel out)\n",
    "\n",
    "**Matrix form:**\n",
    "$$MSE = \\frac{1}{n}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize residuals (errors)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = x.reshape(-1, 1)\n",
    "model = LinearRegression()\n",
    "model.fit(X, y_noisy)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(x, y_noisy, alpha=0.5, s=30, label='Data')\n",
    "plt.plot(x, y_pred, 'r-', linewidth=2, label='Fitted line')\n",
    "\n",
    "# Draw residuals for first 20 points\n",
    "for i in range(0, 100, 5):\n",
    "    plt.plot([x[i], x[i]], [y_noisy[i], y_pred[i]], 'g--', alpha=0.5, linewidth=1)\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Residuals: Distance from Points to Line', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"üí° We minimize the sum of squared green lines!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: The Solution - Normal Equation\n",
    "\n",
    "**Closed-form solution** (when it exists):\n",
    "\n",
    "$$\\boldsymbol{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "**Derivation (intuition):**\n",
    "1. Take derivative of MSE with respect to $\\boldsymbol{\\beta}$\n",
    "2. Set to zero (find minimum)\n",
    "3. Solve for $\\boldsymbol{\\beta}$\n",
    "\n",
    "**When does it fail?**\n",
    "- $\\mathbf{X}^T\\mathbf{X}$ is not invertible (singular matrix)\n",
    "- Features are perfectly correlated (multicollinearity)\n",
    "- More features than samples (p > n)\n",
    "\n",
    "**Solution:** Use gradient descent or regularization instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual implementation of Normal Equation\n",
    "def normal_equation(X, y):\n",
    "    \"\"\"Solve linear regression using normal equation\"\"\"\n",
    "    X_with_intercept = np.c_[np.ones(len(X)), X]  # Add column of 1s for intercept\n",
    "    beta = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y\n",
    "    return beta\n",
    "\n",
    "# Compare with sklearn\n",
    "beta_manual = normal_equation(X, y_noisy)\n",
    "\n",
    "print(f\"Manual solution: Œ≤‚ÇÄ={beta_manual[0]:.3f}, Œ≤‚ÇÅ={beta_manual[1]:.3f}\")\n",
    "print(f\"Sklearn solution: Œ≤‚ÇÄ={model.intercept_:.3f}, Œ≤‚ÇÅ={model.coef_[0]:.3f}\")\n",
    "print(f\"\\n‚úÖ Same results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4: Key Assumptions\n",
    "\n",
    "Linear regression makes important assumptions:\n",
    "\n",
    "#### 1. **Linearity**\n",
    "- Relationship between X and y is linear\n",
    "- Check: Residual plots should show no pattern\n",
    "\n",
    "#### 2. **Independence**\n",
    "- Observations are independent\n",
    "- Errors are uncorrelated\n",
    "- Violated in: Time series, spatial data\n",
    "\n",
    "#### 3. **Homoscedasticity**\n",
    "- Constant variance of errors\n",
    "- $Var(\\epsilon_i) = \\sigma^2$ for all i\n",
    "- Check: Residual plot should have constant spread\n",
    "\n",
    "#### 4. **Normality**\n",
    "- Errors are normally distributed: $\\epsilon \\sim N(0, \\sigma^2)$\n",
    "- Important for: Confidence intervals, hypothesis tests\n",
    "- Less critical for: Predictions\n",
    "\n",
    "#### 5. **No Multicollinearity**\n",
    "- Features are not highly correlated\n",
    "- Causes unstable coefficient estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize assumption violations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Good: Assumptions met\n",
    "x_good = np.linspace(0, 10, 100)\n",
    "y_good = 2*x_good + np.random.normal(0, 1, 100)\n",
    "model_good = LinearRegression().fit(x_good.reshape(-1,1), y_good)\n",
    "residuals_good = y_good - model_good.predict(x_good.reshape(-1,1))\n",
    "\n",
    "axes[0,0].scatter(x_good, residuals_good, alpha=0.5, s=20)\n",
    "axes[0,0].axhline(0, color='red', linestyle='--')\n",
    "axes[0,0].set_title('‚úÖ Good: Random scatter', fontweight='bold')\n",
    "axes[0,0].set_xlabel('Fitted values')\n",
    "axes[0,0].set_ylabel('Residuals')\n",
    "\n",
    "# Bad: Non-linearity\n",
    "x_curve = np.linspace(0, 10, 100)\n",
    "y_curve = x_curve**2 + np.random.normal(0, 5, 100)\n",
    "model_curve = LinearRegression().fit(x_curve.reshape(-1,1), y_curve)\n",
    "residuals_curve = y_curve - model_curve.predict(x_curve.reshape(-1,1))\n",
    "\n",
    "axes[0,1].scatter(x_curve, residuals_curve, alpha=0.5, s=20, color='orange')\n",
    "axes[0,1].axhline(0, color='red', linestyle='--')\n",
    "axes[0,1].set_title('‚ö†Ô∏è Bad: Curved pattern (non-linear)', fontweight='bold')\n",
    "axes[0,1].set_xlabel('Fitted values')\n",
    "axes[0,1].set_ylabel('Residuals')\n",
    "\n",
    "# Bad: Heteroscedasticity\n",
    "x_hetero = np.linspace(1, 10, 100)\n",
    "y_hetero = 2*x_hetero + np.random.normal(0, x_hetero*0.5, 100)\n",
    "model_hetero = LinearRegression().fit(x_hetero.reshape(-1,1), y_hetero)\n",
    "residuals_hetero = y_hetero - model_hetero.predict(x_hetero.reshape(-1,1))\n",
    "\n",
    "axes[1,0].scatter(x_hetero, residuals_hetero, alpha=0.5, s=20, color='red')\n",
    "axes[1,0].axhline(0, color='red', linestyle='--')\n",
    "axes[1,0].set_title('‚ö†Ô∏è Bad: Funnel shape (heteroscedasticity)', fontweight='bold')\n",
    "axes[1,0].set_xlabel('Fitted values')\n",
    "axes[1,0].set_ylabel('Residuals')\n",
    "\n",
    "# Normality check with Q-Q plot\n",
    "stats.probplot(residuals_good, dist=\"norm\", plot=axes[1,1])\n",
    "axes[1,1].set_title('‚úÖ Q-Q Plot: Checking normality', fontweight='bold')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Logistic Regression Theory\n",
    "\n",
    "### 2.1: From Regression to Classification\n",
    "\n",
    "**Problem:** Linear regression outputs can be outside [0,1]\n",
    "- Can't interpret as probabilities\n",
    "- Unbounded predictions\n",
    "\n",
    "**Solution:** Use a transformation!\n",
    "\n",
    "**The Logistic (Sigmoid) Function:**\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "**Properties:**\n",
    "- Maps any real number to (0, 1)\n",
    "- Smooth and differentiable\n",
    "- $\\sigma(0) = 0.5$ (decision boundary)\n",
    "- $\\sigma(\\infty) = 1$, $\\sigma(-\\infty) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z = np.linspace(-10, 10, 200)\n",
    "y_sigmoid = sigmoid(z)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z, y_sigmoid, linewidth=3, color='blue')\n",
    "plt.axhline(0.5, color='red', linestyle='--', alpha=0.5, label='Decision boundary')\n",
    "plt.axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "plt.axhline(0, color='black', linestyle='-', alpha=0.3, linewidth=0.5)\n",
    "plt.axhline(1, color='black', linestyle='-', alpha=0.3, linewidth=0.5)\n",
    "plt.xlabel('z (linear combination)', fontsize=12)\n",
    "plt.ylabel('œÉ(z) = P(y=1|x)', fontsize=12)\n",
    "plt.title('Sigmoid Function: Transforming Linear Output to Probability', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Output is now between 0 and 1 - a valid probability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: The Logistic Regression Model\n",
    "\n",
    "**Model equation:**\n",
    "$$P(y=1|\\mathbf{x}) = \\sigma(\\mathbf{x}^T\\boldsymbol{\\beta}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p)}}$$\n",
    "\n",
    "**Decision rule:**\n",
    "- If $P(y=1|\\mathbf{x}) > 0.5$ ‚Üí predict class 1\n",
    "- If $P(y=1|\\mathbf{x}) \\leq 0.5$ ‚Üí predict class 0\n",
    "\n",
    "**The linear part (logit/log-odds):**\n",
    "$$\\log\\left(\\frac{P(y=1|\\mathbf{x})}{1-P(y=1|\\mathbf{x})}\\right) = \\mathbf{x}^T\\boldsymbol{\\beta}$$\n",
    "\n",
    "The log-odds is linear in the features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Maximum Likelihood Estimation\n",
    "\n",
    "**Problem:** Can't use MSE for classification!\n",
    "\n",
    "**Solution:** Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "**Likelihood function** (probability of observing our data):\n",
    "$$L(\\boldsymbol{\\beta}) = \\prod_{i=1}^{n} P(y_i|\\mathbf{x}_i, \\boldsymbol{\\beta})$$\n",
    "\n",
    "**For binary classification:**\n",
    "$$L(\\boldsymbol{\\beta}) = \\prod_{i=1}^{n} [h(\\mathbf{x}_i)]^{y_i}[1-h(\\mathbf{x}_i)]^{1-y_i}$$\n",
    "\n",
    "where $h(\\mathbf{x}_i) = P(y=1|\\mathbf{x}_i)$\n",
    "\n",
    "**Log-Likelihood** (easier to work with):\n",
    "$$\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} [y_i \\log h(\\mathbf{x}_i) + (1-y_i) \\log(1-h(\\mathbf{x}_i))]$$\n",
    "\n",
    "**Loss function** (Cross-Entropy):\n",
    "$$J(\\boldsymbol{\\beta}) = -\\frac{1}{n} \\ell(\\boldsymbol{\\beta})$$\n",
    "\n",
    "**Goal:** Maximize likelihood = Minimize cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-entropy loss\n",
    "p_pred = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# Loss when true label is 1\n",
    "loss_y1 = -np.log(p_pred)\n",
    "# Loss when true label is 0\n",
    "loss_y0 = -np.log(1 - p_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(p_pred, loss_y1, label='True label = 1', linewidth=2)\n",
    "plt.plot(p_pred, loss_y0, label='True label = 0', linewidth=2)\n",
    "plt.xlabel('Predicted Probability P(y=1)', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Cross-Entropy Loss Function', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0, 5])\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Wrong predictions are heavily penalized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4: Optimization - Gradient Descent\n",
    "\n",
    "**No closed-form solution** for logistic regression!\n",
    "\n",
    "**Gradient Descent Algorithm:**\n",
    "1. Start with random $\\boldsymbol{\\beta}$\n",
    "2. Compute gradient: $\\nabla J(\\boldsymbol{\\beta})$\n",
    "3. Update: $\\boldsymbol{\\beta} \\leftarrow \\boldsymbol{\\beta} - \\alpha \\nabla J(\\boldsymbol{\\beta})$\n",
    "4. Repeat until convergence\n",
    "\n",
    "**Gradient for logistic regression:**\n",
    "$$\\nabla J(\\boldsymbol{\\beta}) = \\frac{1}{n}\\mathbf{X}^T(h(\\mathbf{X}) - \\mathbf{y})$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = learning rate\n",
    "- $h(\\mathbf{X})$ = predicted probabilities\n",
    "- $\\mathbf{y}$ = true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient descent\n",
    "def loss_surface(b0, b1):\n",
    "    \"\"\"Simplified 2D loss surface\"\"\"\n",
    "    return (b0-2)**2 + (b1+1)**2\n",
    "\n",
    "# Create grid\n",
    "b0 = np.linspace(-1, 5, 100)\n",
    "b1 = np.linspace(-4, 2, 100)\n",
    "B0, B1 = np.meshgrid(b0, b1)\n",
    "Z = loss_surface(B0, B1)\n",
    "\n",
    "# Gradient descent path\n",
    "path_b0 = [0, 0.5, 1.0, 1.5, 1.8, 1.95, 2.0]\n",
    "path_b1 = [-3, -2.5, -2.0, -1.5, -1.2, -1.05, -1.0]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contour(B0, B1, Z, levels=20, cmap='viridis')\n",
    "plt.plot(path_b0, path_b1, 'r.-', linewidth=2, markersize=10, label='Gradient descent path')\n",
    "plt.scatter([2], [-1], s=200, c='red', marker='*', label='Minimum', zorder=5)\n",
    "plt.xlabel('Œ≤‚ÇÄ', fontsize=12)\n",
    "plt.ylabel('Œ≤‚ÇÅ', fontsize=12)\n",
    "plt.title('Gradient Descent: Finding the Minimum', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.colorbar(label='Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Regularization Theory\n",
    "\n",
    "### 3.1: The Overfitting Problem\n",
    "\n",
    "**Issue:** Complex models can fit training data *too well*\n",
    "- High variance\n",
    "- Poor generalization\n",
    "- Sensitive to noise\n",
    "\n",
    "**Solution:** Add a penalty for model complexity!\n",
    "\n",
    "**Regularized loss:**\n",
    "$$J(\\boldsymbol{\\beta}) = \\text{Loss}(\\mathbf{y}, \\hat{\\mathbf{y}}) + \\lambda \\cdot \\text{Penalty}(\\boldsymbol{\\beta})$$\n",
    "\n",
    "Where:\n",
    "- $\\lambda$ = regularization strength (hyperparameter)\n",
    "- Larger $\\lambda$ ‚Üí simpler model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Ridge Regression (L2 Regularization)\n",
    "\n",
    "**Loss function:**\n",
    "$$J(\\boldsymbol{\\beta}) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda\\sum_{j=1}^{p}\\beta_j^2$$\n",
    "\n",
    "**Matrix form:**\n",
    "$$J(\\boldsymbol{\\beta}) = ||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||^2 + \\lambda||\\boldsymbol{\\beta}||^2$$\n",
    "\n",
    "**Closed-form solution:**\n",
    "$$\\boldsymbol{\\beta}_{ridge} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "**Properties:**\n",
    "- Shrinks coefficients toward zero\n",
    "- Never exactly zero\n",
    "- All features kept in model\n",
    "- Handles multicollinearity well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Lasso Regression (L1 Regularization)\n",
    "\n",
    "**Loss function:**\n",
    "$$J(\\boldsymbol{\\beta}) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda\\sum_{j=1}^{p}|\\beta_j|$$\n",
    "\n",
    "**Properties:**\n",
    "- Can set coefficients exactly to zero\n",
    "- Performs **feature selection**\n",
    "- Creates sparse models\n",
    "- No closed-form solution (needs optimization)\n",
    "\n",
    "**When to use:**\n",
    "- Many features, few important\n",
    "- Want interpretable model\n",
    "- Need automatic feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: ElasticNet (L1 + L2)\n",
    "\n",
    "**Combines both penalties:**\n",
    "$$J(\\boldsymbol{\\beta}) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda_1\\sum_{j=1}^{p}|\\beta_j| + \\lambda_2\\sum_{j=1}^{p}\\beta_j^2$$\n",
    "\n",
    "**Alternative form:**\n",
    "$$J(\\boldsymbol{\\beta}) = \\text{MSE} + \\lambda[\\alpha||\\boldsymbol{\\beta}||_1 + (1-\\alpha)||\\boldsymbol{\\beta}||^2]$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha \\in [0,1]$ controls L1 vs L2 mix\n",
    "- $\\alpha=0$ ‚Üí Pure Ridge\n",
    "- $\\alpha=1$ ‚Üí Pure Lasso\n",
    "\n",
    "**Benefits:**\n",
    "- Feature selection (like Lasso)\n",
    "- Handles correlated features (like Ridge)\n",
    "- More stable than Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regularization penalties\n",
    "beta_range = np.linspace(-3, 3, 100)\n",
    "l1_penalty = np.abs(beta_range)\n",
    "l2_penalty = beta_range**2\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(beta_range, l1_penalty, label='L1: |Œ≤|', linewidth=2)\n",
    "plt.plot(beta_range, l2_penalty, label='L2: Œ≤¬≤', linewidth=2)\n",
    "plt.axvline(0, color='black', linestyle='--', alpha=0.3)\n",
    "plt.axhline(0, color='black', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('Œ≤ (coefficient value)', fontsize=12)\n",
    "plt.ylabel('Penalty', fontsize=12)\n",
    "plt.title('L1 vs L2 Penalty Functions', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° L1 is linear (can hit zero), L2 is quadratic (asymptotic to zero)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geometric intuition: Constraint regions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# L1 constraint (diamond)\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "l1_x = np.cos(theta)\n",
    "l1_y = np.sin(theta)\n",
    "l1_diamond_x = np.sign(l1_x) * np.abs(l1_x)\n",
    "l1_diamond_y = np.sign(l1_y) * (1 - np.abs(l1_x))\n",
    "\n",
    "# Plot L1\n",
    "axes[0].plot(l1_diamond_x, l1_diamond_y, 'b-', linewidth=2)\n",
    "axes[0].fill(l1_diamond_x, l1_diamond_y, alpha=0.2, color='blue')\n",
    "axes[0].scatter([0], [1], s=200, c='red', marker='*', zorder=5, label='Optimal (Œ≤‚ÇÇ‚â†0, Œ≤‚ÇÅ=0)')\n",
    "axes[0].set_xlabel('Œ≤‚ÇÅ', fontsize=12)\n",
    "axes[0].set_ylabel('Œ≤‚ÇÇ', fontsize=12)\n",
    "axes[0].set_title('L1 (Lasso): Diamond-shaped constraint\\nHits axes ‚Üí Sparse solution', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim([-1.5, 1.5])\n",
    "axes[0].set_ylim([-1.5, 1.5])\n",
    "\n",
    "# L2 constraint (circle)\n",
    "circle_x = np.cos(theta)\n",
    "circle_y = np.sin(theta)\n",
    "\n",
    "axes[1].plot(circle_x, circle_y, 'g-', linewidth=2)\n",
    "axes[1].fill(circle_x, circle_y, alpha=0.2, color='green')\n",
    "axes[1].scatter([0.7], [0.7], s=200, c='red', marker='*', zorder=5, label='Optimal (Œ≤‚ÇÇ‚â†0, Œ≤‚ÇÅ‚â†0)')\n",
    "axes[1].set_xlabel('Œ≤‚ÇÅ', fontsize=12)\n",
    "axes[1].set_ylabel('Œ≤‚ÇÇ', fontsize=12)\n",
    "axes[1].set_title('L2 (Ridge): Circular constraint\\nRarely hits axes ‚Üí Dense solution', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim([-1.5, 1.5])\n",
    "axes[1].set_ylim([-1.5, 1.5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5: Comparison Table\n",
    "\n",
    "| Property | Ridge (L2) | Lasso (L1) | ElasticNet |\n",
    "|----------|-----------|-----------|------------|\n",
    "| **Penalty** | $\\lambda\\sum\\beta_j^2$ | $\\lambda\\sum|\\beta_j|$ | Both |\n",
    "| **Feature Selection** | ‚ùå No | ‚úÖ Yes | ‚úÖ Yes |\n",
    "| **Coefficient Values** | Shrink to ~0 | Exactly 0 | Mix |\n",
    "| **Multicollinearity** | ‚úÖ Handles well | ‚ö†Ô∏è Picks one | ‚úÖ Handles well |\n",
    "| **Computational Cost** | Fast (closed form) | Slower (iterative) | Slower |\n",
    "| **Best When** | Many correlated features | Many irrelevant features | Combination needed |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Bias-Variance Tradeoff\n",
    "\n",
    "### 4.1: Core Concept\n",
    "\n",
    "**Total Error Decomposition:**\n",
    "$$\\text{Expected Test Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "\n",
    "**Bias:**\n",
    "- Error from wrong assumptions\n",
    "- Model too simple (underfitting)\n",
    "- High bias ‚Üí systematic errors\n",
    "\n",
    "**Variance:**\n",
    "- Error from sensitivity to training data\n",
    "- Model too complex (overfitting)\n",
    "- High variance ‚Üí inconsistent predictions\n",
    "\n",
    "**Irreducible Error:**\n",
    "- Noise in the data\n",
    "- Cannot be reduced by better models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bias-variance tradeoff\n",
    "complexity = np.linspace(0, 10, 100)\n",
    "bias = 5 / (1 + complexity)\n",
    "variance = complexity / 2\n",
    "total_error = bias + variance + 0.5  # +0.5 is irreducible error\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(complexity, bias, label='Bias¬≤', linewidth=2)\n",
    "plt.plot(complexity, variance, label='Variance', linewidth=2)\n",
    "plt.plot(complexity, total_error, label='Total Error', linewidth=3, linestyle='--', color='red')\n",
    "plt.axhline(0.5, color='gray', linestyle=':', alpha=0.5, label='Irreducible Error')\n",
    "\n",
    "# Mark optimal point\n",
    "optimal_idx = np.argmin(total_error)\n",
    "plt.scatter([complexity[optimal_idx]], [total_error[optimal_idx]], \n",
    "           s=200, c='red', marker='*', zorder=5, label='Optimal Complexity')\n",
    "\n",
    "plt.xlabel('Model Complexity', fontsize=12)\n",
    "plt.ylabel('Error', fontsize=12)\n",
    "plt.title('Bias-Variance Tradeoff', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Regularization & Bias-Variance\n",
    "\n",
    "**Effect of regularization parameter $\\lambda$:**\n",
    "\n",
    "**Low $\\lambda$ (weak regularization):**\n",
    "- Low bias, high variance\n",
    "- Complex model\n",
    "- Risk of overfitting\n",
    "\n",
    "**High $\\lambda$ (strong regularization):**\n",
    "- High bias, low variance\n",
    "- Simple model\n",
    "- Risk of underfitting\n",
    "\n",
    "**Sweet spot:**\n",
    "- Balance bias and variance\n",
    "- Found via cross-validation\n",
    "- Minimizes test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regularization effect\n",
    "lambda_range = np.logspace(-3, 3, 100)\n",
    "train_error = 0.1 + 0.2 * np.exp(-lambda_range)\n",
    "test_error = 0.3 + 0.2 * np.exp(-lambda_range) + 0.15 * lambda_range / 10\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(lambda_range, train_error, label='Training Error', linewidth=2)\n",
    "plt.semilogx(lambda_range, test_error, label='Test Error', linewidth=2)\n",
    "\n",
    "# Mark optimal lambda\n",
    "optimal_idx = np.argmin(test_error)\n",
    "plt.scatter([lambda_range[optimal_idx]], [test_error[optimal_idx]], \n",
    "           s=200, c='red', marker='*', zorder=5, label='Optimal Œª')\n",
    "\n",
    "# Annotate regions\n",
    "plt.text(0.01, 0.4, 'Overfitting\\n(High Variance)', ha='center', fontsize=10, \n",
    "        bbox=dict(boxstyle='round', facecolor='red', alpha=0.2))\n",
    "plt.text(100, 0.4, 'Underfitting\\n(High Bias)', ha='center', fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='blue', alpha=0.2))\n",
    "\n",
    "plt.xlabel('Regularization Parameter (Œª)', fontsize=12)\n",
    "plt.ylabel('Error', fontsize=12)\n",
    "plt.title('Effect of Regularization on Error', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Key Theoretical Insights\n",
    "\n",
    "### Linear Regression\n",
    "‚úÖ **Goal:** Minimize MSE using Normal Equation or Gradient Descent  \n",
    "‚úÖ **Assumptions:** Linearity, Independence, Homoscedasticity, Normality  \n",
    "‚úÖ **Limitation:** Only captures linear relationships\n",
    "\n",
    "### Logistic Regression\n",
    "‚úÖ **Goal:** Maximize likelihood (minimize cross-entropy)  \n",
    "‚úÖ **Key:** Sigmoid transforms linear output to probability  \n",
    "‚úÖ **Optimization:** Gradient descent (no closed-form solution)  \n",
    "‚úÖ **Limitation:** Linear decision boundary\n",
    "\n",
    "### Regularization\n",
    "‚úÖ **Ridge (L2):** Shrinks all coefficients, keeps all features  \n",
    "‚úÖ **Lasso (L1):** Sets some coefficients to zero, feature selection  \n",
    "‚úÖ **ElasticNet:** Best of both worlds  \n",
    "‚úÖ **Purpose:** Control model complexity, reduce overfitting\n",
    "\n",
    "### Bias-Variance Tradeoff\n",
    "‚úÖ **Bias:** Error from simplifying assumptions  \n",
    "‚úÖ **Variance:** Error from sensitivity to training data  \n",
    "‚úÖ **Goal:** Find optimal complexity via cross-validation  \n",
    "‚úÖ **Tool:** Regularization helps balance the tradeoff\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "1. ‚úÖ Practice with real datasets\n",
    "2. ‚úÖ Experiment with regularization parameters\n",
    "3. ‚úÖ Check model assumptions\n",
    "4. ‚úÖ Use cross-validation for model selection\n",
    "5. ‚úÖ Visualize decision boundaries (for classification)\n",
    "\n",
    "**Remember:** Theory guides practice. Understanding *why* helps you know *when* to apply each technique!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
