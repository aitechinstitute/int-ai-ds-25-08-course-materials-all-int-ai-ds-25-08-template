{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 8 - Lab 01: Linear Regression Fundamentals\n",
    "**Instructor:** Amir Charkhi | **Type:** Hands-On Practice\n",
    "\n",
    "> Master Linear and Polynomial Regression\n",
    "\n",
    "## üéØ Lab Objectives\n",
    "\n",
    "In this lab, you'll practice:\n",
    "- Building and interpreting Linear Regression models\n",
    "- Understanding coefficients and feature importance\n",
    "- Detecting and handling overfitting\n",
    "- Implementing Polynomial Regression\n",
    "- Using regularization (Ridge, Lasso)\n",
    "\n",
    "**Time**: 35-45 minutes  \n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ (Intermediate)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Quick Reference\n",
    "\n",
    "**Linear Regression:**\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Simple Linear Regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Coefficients\n",
    "coefficients = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "# Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Regularization\n",
    "ridge = Ridge(alpha=1.0)\n",
    "lasso = Lasso(alpha=1.0)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Run this cell first!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing, make_regression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "print(\"‚úÖ Setup complete! Let's master Linear Regression!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Exercise 1: Simple Linear Regression\n",
    "\n",
    "Let's start with the basics and build our first regression model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y = pd.Series(housing.target, name='MedHouseVal')\n",
    "\n",
    "print(f\"Dataset loaded: {len(X)} samples, {X.shape[1]} features\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")\n",
    "\n",
    "# TODO 1.1: Explore the data\n",
    "# - Display first 5 rows\n",
    "# - Show summary statistics\n",
    "# - Display target variable statistics (min, max, mean)\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ Task 1.1 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Visualize Feature Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1.2: Create visualizations\n",
    "# Requirements:\n",
    "#   1. Create a correlation heatmap\n",
    "#   2. Create scatter plot: MedInc vs target (most correlated feature)\n",
    "#   3. Identify the top 3 features most correlated with target\n",
    "\n",
    "# Your code here:\n",
    "# Hint: Combine X and y into a single DataFrame for correlation\n",
    "df_combined = pd.concat([X, y], axis=1)\n",
    "\n",
    "# 1. Correlation heatmap\n",
    "\n",
    "\n",
    "# 2. Scatter plot for top feature\n",
    "\n",
    "\n",
    "# 3. Print top 3 correlated features\n",
    "correlations = # Calculate correlations with target\n",
    "print(\"\\nTop 3 features by correlation:\")\n",
    "# Your code here\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ Task 1.2 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Build Your First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1.3: Train a Linear Regression model\n",
    "# Steps:\n",
    "#   1. Split data (80/20, random_state=42)\n",
    "#   2. Create and train LinearRegression model\n",
    "#   3. Make predictions on test set\n",
    "#   4. Calculate MAE, RMSE, and R¬≤ score\n",
    "\n",
    "# Your code here:\n",
    "X_train, X_test, y_train, y_test = # Split the data\n",
    "\n",
    "model = # Create model\n",
    "# Train model\n",
    "\n",
    "y_pred = # Make predictions\n",
    "\n",
    "# Calculate metrics\n",
    "mae = # Mean Absolute Error\n",
    "rmse = # Root Mean Squared Error\n",
    "r2 = # R¬≤ Score\n",
    "\n",
    "# Validation (Don't modify)\n",
    "print(\"Model Performance:\")\n",
    "print(f\"  MAE:  ${mae*100:.2f}k\")\n",
    "print(f\"  RMSE: ${rmse*100:.2f}k\")\n",
    "print(f\"  R¬≤:   {r2:.4f}\")\n",
    "\n",
    "if r2 > 0.5:\n",
    "    print(f\"\\n‚úÖ Good start! Model explains {r2*100:.1f}% of variance\")\n",
    "    print(\"üéâ Task 1.3 Complete!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Check your code - R¬≤ should be above 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Exercise 2: Understanding Coefficients\n",
    "\n",
    "Let's interpret what the model learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Analyze Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2.1: Extract and visualize coefficients\n",
    "# Requirements:\n",
    "#   1. Get model coefficients and create a DataFrame\n",
    "#   2. Sort by absolute coefficient value\n",
    "#   3. Create a horizontal bar plot\n",
    "#   4. Print the intercept\n",
    "\n",
    "# Your code here:\n",
    "coefficients = # Get coefficients\n",
    "intercept = # Get intercept\n",
    "\n",
    "# Create DataFrame of coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': # Your coefficients here\n",
    "})\n",
    "\n",
    "# Sort by absolute value\n",
    "\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Create horizontal bar plot\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nIntercept: {intercept:.4f}\")\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"  Positive coefficient = feature increases prediction\")\n",
    "print(\"  Negative coefficient = feature decreases prediction\")\n",
    "print(\"  Larger absolute value = stronger effect\")\n",
    "print(\"\\n‚úÖ Task 2.1 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Manual Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2.2: Make a manual prediction using the equation\n",
    "# Linear Regression: y = intercept + coef1*x1 + coef2*x2 + ... + coefn*xn\n",
    "\n",
    "# Get the first test sample\n",
    "sample = X_test.iloc[0]\n",
    "actual = y_test.iloc[0]\n",
    "model_pred = model.predict(sample.values.reshape(1, -1))[0]\n",
    "\n",
    "print(\"Sample features:\")\n",
    "print(sample)\n",
    "\n",
    "# Your code here:\n",
    "# Calculate prediction manually: intercept + sum of (coefficient * feature value)\n",
    "manual_pred = # Start with intercept, then add each (coef * feature value)\n",
    "\n",
    "\n",
    "# Validation (Don't modify)\n",
    "print(f\"\\nManual prediction: {manual_pred:.4f}\")\n",
    "print(f\"Model prediction:  {model_pred:.4f}\")\n",
    "print(f\"Actual value:      {actual:.4f}\")\n",
    "\n",
    "if abs(manual_pred - model_pred) < 0.01:\n",
    "    print(\"\\n‚úÖ Perfect! You understand the linear equation!\")\n",
    "    print(\"üéâ Task 2.2 Complete!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Manual and model predictions should match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà Exercise 3: Polynomial Regression\n",
    "\n",
    "Sometimes relationships aren't perfectly linear - let's add some curves!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Generate Non-Linear Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic non-linear data\n",
    "np.random.seed(42)\n",
    "X_simple = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_simple = 3 + 2*X_simple + 0.5*X_simple**2 + np.random.normal(0, 2, X_simple.shape)\n",
    "y_simple = y_simple.flatten()\n",
    "\n",
    "# TODO 3.1: Visualize the non-linear relationship\n",
    "# Create a scatter plot to see the curve\n",
    "\n",
    "# Your code here:\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Create scatter plot\n",
    "\n",
    "\n",
    "print(\"üí° Notice the curved pattern - linear regression won't fit perfectly!\")\n",
    "print(\"‚úÖ Task 3.1 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Compare Linear vs Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3.2: Fit both linear and polynomial regression\n",
    "# Requirements:\n",
    "#   1. Fit simple Linear Regression\n",
    "#   2. Create polynomial features (degree=2)\n",
    "#   3. Fit Linear Regression on polynomial features\n",
    "#   4. Compare R¬≤ scores\n",
    "#   5. Visualize both fits\n",
    "\n",
    "# Split data\n",
    "X_train_simple, X_test_simple, y_train_simple, y_test_simple = train_test_split(\n",
    "    X_simple, y_simple, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Your code here:\n",
    "# 1. Linear model\n",
    "linear_model = # Create and fit linear model\n",
    "\n",
    "y_pred_linear = # Predict\n",
    "r2_linear = # Calculate R¬≤\n",
    "\n",
    "# 2. Polynomial model\n",
    "poly = # Create PolynomialFeatures(degree=2)\n",
    "X_train_poly = # Transform training data\n",
    "X_test_poly = # Transform test data\n",
    "\n",
    "poly_model = # Create and fit on polynomial features\n",
    "\n",
    "y_pred_poly = # Predict\n",
    "r2_poly = # Calculate R¬≤\n",
    "\n",
    "# Validation and Visualization\n",
    "print(\"Model Comparison:\")\n",
    "print(f\"  Linear Regression R¬≤:     {r2_linear:.4f}\")\n",
    "print(f\"  Polynomial Regression R¬≤: {r2_poly:.4f}\")\n",
    "print(f\"  Improvement: {(r2_poly - r2_linear):.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_test_simple, y_test_simple, alpha=0.5, label='Actual')\n",
    "plt.scatter(X_test_simple, y_pred_linear, alpha=0.5, label='Linear Fit')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Linear Regression (R¬≤={r2_linear:.3f})')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test_simple, y_test_simple, alpha=0.5, label='Actual')\n",
    "plt.scatter(X_test_simple, y_pred_poly, alpha=0.5, label='Polynomial Fit')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Polynomial Regression (R¬≤={r2_poly:.3f})')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if r2_poly > r2_linear:\n",
    "    print(\"\\n‚úÖ Polynomial fits better for non-linear data!\")\n",
    "    print(\"üéâ Task 3.2 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3: Detecting Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3.3: Explore what happens with very high polynomial degrees\n",
    "# Requirements:\n",
    "#   1. Try polynomial degrees from 1 to 15\n",
    "#   2. Calculate train and test R¬≤ for each\n",
    "#   3. Plot the results\n",
    "#   4. Identify the overfitting point\n",
    "\n",
    "# Your code here:\n",
    "degrees = range(1, 16)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create polynomial features\n",
    "    \n",
    "    # Transform data\n",
    "    \n",
    "    # Train model\n",
    "    \n",
    "    # Calculate R¬≤ for both train and test\n",
    "    \n",
    "    # Append to lists\n",
    "    pass\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, train_scores, 'o-', label='Training R¬≤', linewidth=2)\n",
    "plt.plot(degrees, test_scores, 's-', label='Test R¬≤', linewidth=2)\n",
    "plt.xlabel('Polynomial Degree', fontsize=12)\n",
    "plt.ylabel('R¬≤ Score', fontsize=12)\n",
    "plt.title('Overfitting Detection', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Signs of Overfitting:\")\n",
    "print(\"  - Training R¬≤ keeps improving\")\n",
    "print(\"  - Test R¬≤ starts decreasing\")\n",
    "print(\"  - Large gap between train and test scores\")\n",
    "print(\"\\n‚úÖ Task 3.3 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ°Ô∏è Exercise 4: Regularization\n",
    "\n",
    "Prevent overfitting with Ridge and Lasso regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Compare Ridge and Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to California Housing with all features\n",
    "# TODO 4.1: Compare Linear, Ridge, and Lasso Regression\n",
    "# Requirements:\n",
    "#   1. Standardize features (important for regularization!)\n",
    "#   2. Train LinearRegression, Ridge(alpha=1.0), and Lasso(alpha=1.0)\n",
    "#   3. Calculate test R¬≤ for each\n",
    "#   4. Compare number of non-zero coefficients\n",
    "\n",
    "# Use the housing data from Exercise 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Your code here:\n",
    "# 1. Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = # Fit and transform training data\n",
    "X_test_scaled = # Transform test data\n",
    "\n",
    "# 2. Train three models\n",
    "models = {\n",
    "    'Linear': # LinearRegression()\n",
    "    'Ridge': # Ridge(alpha=1.0)\n",
    "    'Lasso': # Lasso(alpha=1.0)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    # Train model\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    \n",
    "    # Count non-zero coefficients\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Key Differences:\")\n",
    "print(\"  Ridge: Shrinks all coefficients but keeps all features\")\n",
    "print(\"  Lasso: Can reduce some coefficients to exactly zero (feature selection!)\")\n",
    "print(\"\\n‚úÖ Task 4.1 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Tuning Regularization Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.2: Find optimal alpha for Ridge regression\n",
    "# Requirements:\n",
    "#   1. Try different alpha values: [0.01, 0.1, 1, 10, 100]\n",
    "#   2. Use cross-validation to evaluate each\n",
    "#   3. Plot CV scores vs alpha\n",
    "#   4. Identify best alpha\n",
    "\n",
    "# Your code here:\n",
    "alphas = [0.01, 0.1, 1, 10, 100]\n",
    "cv_scores_list = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Create Ridge model with this alpha\n",
    "    \n",
    "    # Perform 5-fold cross-validation\n",
    "    \n",
    "    # Store mean CV score\n",
    "    pass\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Create plot of alpha vs CV scores\n",
    "\n",
    "\n",
    "# Find and print best alpha\n",
    "best_idx = # Index of best score\n",
    "best_alpha = alphas[best_idx]\n",
    "best_score = cv_scores_list[best_idx]\n",
    "\n",
    "print(f\"\\nBest alpha: {best_alpha}\")\n",
    "print(f\"Best CV R¬≤: {best_score:.4f}\")\n",
    "print(\"\\n‚úÖ Task 4.2 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Exercise 5: Model Selection Challenge\n",
    "\n",
    "Put everything together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: Choose the Best Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5.1: Complete workflow to find the best model\n",
    "# Given scenario: Predicting house prices with limited features\n",
    "# Requirements:\n",
    "#   1. Select only top 3 most important features from correlation\n",
    "#   2. Try: Linear, Ridge, Lasso, Polynomial(degree=2) + Ridge\n",
    "#   3. Use cross-validation for fair comparison\n",
    "#   4. Select best model and evaluate on test set\n",
    "\n",
    "print(\"üè† Housing Price Prediction Challenge\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Your complete solution here:\n",
    "# Step 1: Feature selection\n",
    "\n",
    "\n",
    "# Step 2: Train/test split\n",
    "\n",
    "\n",
    "# Step 3: Compare models with CV\n",
    "\n",
    "\n",
    "# Step 4: Train best model and evaluate\n",
    "\n",
    "\n",
    "# Validation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if r2_test > 0.5:  # Replace with your test R¬≤\n",
    "    print(\"\\n‚úÖ Excellent! You've built a solid regression model!\")\n",
    "    print(\"\\nüí° What you learned:\")\n",
    "    print(\"  - Linear regression basics\")\n",
    "    print(\"  - Polynomial features for non-linearity\")\n",
    "    print(\"  - Regularization to prevent overfitting\")\n",
    "    print(\"  - Model selection with cross-validation\")\n",
    "    print(\"\\nüéâ Task 5.1 Complete!\")\n",
    "    print(\"üéâ Lab 01 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ Lab Complete!\n",
    "\n",
    "### What You Practiced:\n",
    "\n",
    "‚úÖ **Exercise 1**: Simple Linear Regression basics  \n",
    "‚úÖ **Exercise 2**: Understanding and interpreting coefficients  \n",
    "‚úÖ **Exercise 3**: Polynomial Regression and overfitting  \n",
    "‚úÖ **Exercise 4**: Ridge and Lasso regularization  \n",
    "‚úÖ **Exercise 5**: Complete model selection workflow  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Coefficients show feature importance** - positive/negative and magnitude matter\n",
    "2. **Polynomial features capture non-linear relationships** but can overfit\n",
    "3. **Regularization prevents overfitting** by penalizing large coefficients\n",
    "4. **Ridge shrinks coefficients**, Lasso can **eliminate features**\n",
    "5. **Always standardize** before applying regularization\n",
    "6. **Cross-validation** helps find optimal hyperparameters\n",
    "\n",
    "### When to Use Each:\n",
    "\n",
    "- **Linear Regression**: Fast baseline, interpretable, works when relationships are linear\n",
    "- **Polynomial**: When you see curves in scatter plots\n",
    "- **Ridge**: When all features might be useful, prevents overfitting\n",
    "- **Lasso**: When you want automatic feature selection\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Move to **Lab 02** for Decision Trees and Random Forests\n",
    "- Try these techniques on your own datasets\n",
    "- Experiment with different polynomial degrees and alpha values\n",
    "\n",
    "**Outstanding work! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
