{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 8 - Notebook 01B: Regularized Linear Models\n",
    "**Instructor:** Amir Charkhi | **Goal:** Master regularization techniques\n",
    "\n",
    "> Ridge, Lasso, and ElasticNet for better models\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "**Problem:** Overfitting and noisy features\n",
    "\n",
    "**Solutions:**\n",
    "1. ‚úÖ Ridge Regression (L2) - Shrink all coefficients\n",
    "2. ‚úÖ Lasso Regression (L1) - Select features automatically\n",
    "3. ‚úÖ ElasticNet - Best of both worlds\n",
    "\n",
    "**Why Regularization?**\n",
    "- Prevents overfitting\n",
    "- Handles many features\n",
    "- Automatic feature selection (Lasso)\n",
    "- More stable predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "print(\"‚úÖ Regularization module loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Understanding the Problem\n",
    "\n",
    "**Scenario:** Many features, some are noisy or irrelevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1: Create Dataset with Many Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with useful and noisy features\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "n_features = 20\n",
    "\n",
    "# Create features\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Only first 5 features are useful\n",
    "true_coef = np.zeros(n_features)\n",
    "true_coef[:5] = [10, 8, 6, 4, 2]  # Real effects\n",
    "# Rest are 0 (noise features)\n",
    "\n",
    "# Create target\n",
    "y = X @ true_coef + np.random.randn(n_samples) * 2\n",
    "\n",
    "print(f\"Dataset: {n_samples} samples, {n_features} features\")\n",
    "print(f\"Only 5 features are truly useful!\")\n",
    "print(f\"True coefficients: {true_coef[:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2: Split and Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# IMPORTANT: Scale features for regularization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training: {len(X_train)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\")\n",
    "print(\"‚úÖ Data scaled (required for regularization!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3: Baseline - Regular Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regular linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_score = lr.score(X_train_scaled, y_train)\n",
    "test_score = lr.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"Regular Linear Regression:\")\n",
    "print(f\"  Training R¬≤: {train_score:.4f}\")\n",
    "print(f\"  Test R¬≤:     {test_score:.4f}\")\n",
    "print(f\"  Difference:  {train_score - test_score:.4f}\")\n",
    "\n",
    "if train_score - test_score > 0.1:\n",
    "    print(\"\\n‚ö†Ô∏è Overfitting detected! Need regularization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4: Visualize Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coefficients vs true coefficients\n",
    "plt.figure(figsize=(12, 5))\n",
    "x_pos = np.arange(len(lr.coef_))\n",
    "\n",
    "plt.bar(x_pos, true_coef, alpha=0.5, label='True Coefficients', color='green')\n",
    "plt.bar(x_pos, lr.coef_, alpha=0.5, label='Learned Coefficients', color='blue')\n",
    "\n",
    "plt.xlabel('Feature Index', fontsize=12)\n",
    "plt.ylabel('Coefficient Value', fontsize=12)\n",
    "plt.title('True vs Learned Coefficients (Linear Regression)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Notice: Noise features (5+) get non-zero coefficients!\")\n",
    "print(\"   This is overfitting to training noise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Ridge Regression (L2 Regularization)\n",
    "\n",
    "**How it works:** Penalize large coefficients\n",
    "\n",
    "**Formula:** Minimize: `MSE + Œ± √ó (sum of squared coefficients)`\n",
    "\n",
    "**Effect:** Shrinks all coefficients, especially large ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1: Train Ridge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ridge with default alpha\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "ridge_train = ridge.score(X_train_scaled, y_train)\n",
    "ridge_test = ridge.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"Ridge Regression (Œ±=1.0):\")\n",
    "print(f\"  Training R¬≤: {ridge_train:.4f}\")\n",
    "print(f\"  Test R¬≤:     {ridge_test:.4f}\")\n",
    "print(f\"  Difference:  {ridge_train - ridge_test:.4f}\")\n",
    "print(\"\\n‚úÖ Less overfitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2: Compare Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coefficient magnitudes\n",
    "plt.figure(figsize=(12, 5))\n",
    "x_pos = np.arange(n_features)\n",
    "\n",
    "plt.bar(x_pos - 0.2, lr.coef_, width=0.4, alpha=0.7, \n",
    "        label='Linear Regression', color='blue')\n",
    "plt.bar(x_pos + 0.2, ridge.coef_, width=0.4, alpha=0.7, \n",
    "        label='Ridge', color='red')\n",
    "\n",
    "plt.xlabel('Feature Index', fontsize=12)\n",
    "plt.ylabel('Coefficient Value', fontsize=12)\n",
    "plt.title('Linear Regression vs Ridge Coefficients', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Ridge shrinks coefficients, especially noise features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3: Effect of Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different alpha values\n",
    "alphas = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    train_scores.append(ridge.score(X_train_scaled, y_train))\n",
    "    test_scores.append(ridge.score(X_test_scaled, y_test))\n",
    "\n",
    "print(\"Ridge Performance vs Alpha:\")\n",
    "print(\"\\nAlpha   | Train R¬≤ | Test R¬≤  | Difference\")\n",
    "print(\"-\" * 50)\n",
    "for alpha, train, test in zip(alphas, train_scores, test_scores):\n",
    "    print(f\"{alpha:7.3f} | {train:8.4f} | {test:8.4f} | {train-test:8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4: Visualize Alpha Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scores vs alpha\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(alphas, train_scores, 'o-', label='Training R¬≤', linewidth=2)\n",
    "plt.semilogx(alphas, test_scores, 's-', label='Test R¬≤', linewidth=2)\n",
    "plt.xlabel('Alpha (Regularization Strength)', fontsize=12)\n",
    "plt.ylabel('R¬≤ Score', fontsize=12)\n",
    "plt.title('Ridge: Effect of Alpha on Performance', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Alpha controls regularization:\")\n",
    "print(\"  - Low alpha = less regularization (might overfit)\")\n",
    "print(\"  - High alpha = more regularization (might underfit)\")\n",
    "print(\"  - Sweet spot in the middle!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Lasso Regression (L1 Regularization)\n",
    "\n",
    "**How it works:** Penalize absolute value of coefficients\n",
    "\n",
    "**Formula:** Minimize: `MSE + Œ± √ó (sum of absolute coefficients)`\n",
    "\n",
    "**Magic:** Can set coefficients to EXACTLY zero (feature selection!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1: Train Lasso Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "lasso_train = lasso.score(X_train_scaled, y_train)\n",
    "lasso_test = lasso.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"Lasso Regression (Œ±=0.1):\")\n",
    "print(f\"  Training R¬≤: {lasso_train:.4f}\")\n",
    "print(f\"  Test R¬≤:     {lasso_test:.4f}\")\n",
    "print(f\"  Difference:  {lasso_train - lasso_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2: Automatic Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count non-zero coefficients\n",
    "n_nonzero = np.sum(lasso.coef_ != 0)\n",
    "n_zero = np.sum(lasso.coef_ == 0)\n",
    "\n",
    "print(f\"\\nüéØ Lasso Feature Selection:\")\n",
    "print(f\"  Selected features: {n_nonzero} (non-zero coefficients)\")\n",
    "print(f\"  Removed features:  {n_zero} (zero coefficients)\")\n",
    "print(f\"\\n  Lasso automatically removed {n_zero} features!\")\n",
    "\n",
    "# Show which features were selected\n",
    "selected = np.where(lasso.coef_ != 0)[0]\n",
    "print(f\"\\n  Selected feature indices: {selected}\")\n",
    "print(f\"  (Remember: first 5 are the real features!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3: Visualize Lasso Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coefficients\n",
    "plt.figure(figsize=(12, 5))\n",
    "x_pos = np.arange(n_features)\n",
    "\n",
    "plt.bar(x_pos - 0.3, true_coef, width=0.3, alpha=0.7, \n",
    "        label='True', color='green')\n",
    "plt.bar(x_pos, ridge.coef_, width=0.3, alpha=0.7, \n",
    "        label='Ridge', color='blue')\n",
    "plt.bar(x_pos + 0.3, lasso.coef_, width=0.3, alpha=0.7, \n",
    "        label='Lasso', color='red')\n",
    "\n",
    "plt.xlabel('Feature Index', fontsize=12)\n",
    "plt.ylabel('Coefficient Value', fontsize=12)\n",
    "plt.title('Ridge vs Lasso Coefficients', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Notice: Lasso sets many coefficients to EXACTLY zero!\")\n",
    "print(\"   Ridge only shrinks them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4: Lasso Path (varying alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how features are eliminated as alpha increases\n",
    "alphas_lasso = [0.001, 0.01, 0.1, 0.5, 1.0, 5.0]\n",
    "coef_matrix = []\n",
    "\n",
    "for alpha in alphas_lasso:\n",
    "    lasso_temp = Lasso(alpha=alpha)\n",
    "    lasso_temp.fit(X_train_scaled, y_train)\n",
    "    coef_matrix.append(lasso_temp.coef_)\n",
    "\n",
    "coef_matrix = np.array(coef_matrix)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(min(10, n_features)):  # Plot first 10 features\n",
    "    plt.plot(alphas_lasso, coef_matrix[:, i], marker='o', label=f'Feature {i}')\n",
    "\n",
    "plt.xlabel('Alpha', fontsize=12)\n",
    "plt.ylabel('Coefficient Value', fontsize=12)\n",
    "plt.title('Lasso Path: Coefficients vs Alpha', fontsize=14, fontweight='bold')\n",
    "plt.xscale('log')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° As alpha increases, more coefficients become zero!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: ElasticNet (L1 + L2)\n",
    "\n",
    "**Best of both worlds!**\n",
    "\n",
    "**Combines:**\n",
    "- Ridge penalty (shrink coefficients)\n",
    "- Lasso penalty (select features)\n",
    "\n",
    "**Use when:** Many correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1: Train ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ElasticNet\n",
    "elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)  # 50% L1, 50% L2\n",
    "elastic.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "elastic_train = elastic.score(X_train_scaled, y_train)\n",
    "elastic_test = elastic.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"ElasticNet (Œ±=0.1, l1_ratio=0.5):\")\n",
    "print(f\"  Training R¬≤: {elastic_train:.4f}\")\n",
    "print(f\"  Test R¬≤:     {elastic_test:.4f}\")\n",
    "print(f\"  Difference:  {elastic_train - elastic_test:.4f}\")\n",
    "\n",
    "# Feature selection\n",
    "n_selected = np.sum(elastic.coef_ != 0)\n",
    "print(f\"\\n  Selected {n_selected} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2: Compare All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Linear', 'Ridge', 'Lasso', 'ElasticNet'],\n",
    "    'Train R¬≤': [train_score, ridge_train, lasso_train, elastic_train],\n",
    "    'Test R¬≤': [test_score, ridge_test, lasso_test, elastic_test],\n",
    "    'Gap': [train_score - test_score, \n",
    "            ridge_train - ridge_test, \n",
    "            lasso_train - lasso_test, \n",
    "            elastic_train - elastic_test],\n",
    "    'Features': [n_features, \n",
    "                 n_features, \n",
    "                 np.sum(lasso.coef_ != 0), \n",
    "                 np.sum(elastic.coef_ != 0)]\n",
    "})\n",
    "\n",
    "print(\"\\nüìä MODEL COMPARISON:\\n\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Analysis:\")\n",
    "print(\"  - Linear: Overfits (large gap)\")\n",
    "print(\"  - Ridge: Reduces overfitting, keeps all features\")\n",
    "print(\"  - Lasso: Automatic feature selection\")\n",
    "print(\"  - ElasticNet: Balanced approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3: Visualize Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Performance comparison\n",
    "x = np.arange(len(results))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, results['Train R¬≤'], width, label='Train', alpha=0.8)\n",
    "axes[0].bar(x + width/2, results['Test R¬≤'], width, label='Test', alpha=0.8)\n",
    "axes[0].set_xlabel('Model', fontsize=12)\n",
    "axes[0].set_ylabel('R¬≤ Score', fontsize=12)\n",
    "axes[0].set_title('Model Performance Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticks(results['Model'])\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Feature selection\n",
    "axes[1].bar(results['Model'], results['Features'], alpha=0.8, color='coral')\n",
    "axes[1].set_xlabel('Model', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Features Used', fontsize=12)\n",
    "axes[1].set_title('Feature Selection Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1].axhline(y=5, color='red', linestyle='--', label='True # of useful features')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Choosing the Right Model\n",
    "\n",
    "**Decision Guide:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1: When to Use Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision framework\n",
    "decision_guide = \"\"\"\n",
    "üéØ MODEL SELECTION GUIDE:\n",
    "\n",
    "1. LINEAR REGRESSION\n",
    "   Use when:\n",
    "   - Few features (< 10)\n",
    "   - All features are relevant\n",
    "   - No multicollinearity\n",
    "   - Need interpretability\n",
    "   ‚ö†Ô∏è Avoid when: Many features or noise\n",
    "\n",
    "2. RIDGE REGRESSION\n",
    "   Use when:\n",
    "   - Many correlated features\n",
    "   - Want to keep all features\n",
    "   - Prevent overfitting\n",
    "   ‚úÖ Safe default choice\n",
    "\n",
    "3. LASSO REGRESSION\n",
    "   Use when:\n",
    "   - Many features, few are useful\n",
    "   - Want automatic feature selection\n",
    "   - Need sparse model\n",
    "   ‚ö†Ô∏è Can be unstable with correlated features\n",
    "\n",
    "4. ELASTICNET\n",
    "   Use when:\n",
    "   - Many correlated features\n",
    "   - Want some feature selection\n",
    "   - Lasso is unstable\n",
    "   ‚úÖ Best of Ridge + Lasso\n",
    "\n",
    "5. POLYNOMIAL REGRESSION\n",
    "   Use when:\n",
    "   - Non-linear relationships\n",
    "   - Curved patterns in data\n",
    "   ‚ö†Ô∏è Watch for overfitting!\n",
    "\"\"\"\n",
    "\n",
    "print(decision_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "### Linear Models Summary:\n",
    "\n",
    "1. **Simple Linear Regression**\n",
    "   - One feature ‚Üí fast and interpretable\n",
    "   - Great for understanding relationships\n",
    "\n",
    "2. **Multiple Linear Regression**\n",
    "   - Many features ‚Üí more realistic\n",
    "   - Check for multicollinearity\n",
    "\n",
    "3. **Polynomial Regression**\n",
    "   - Handle curves ‚Üí more flexible\n",
    "   - Careful with degree (don't overfit!)\n",
    "\n",
    "4. **Regularization** (Ridge/Lasso/ElasticNet)\n",
    "   - Prevents overfitting ‚Üí better generalization\n",
    "   - Essential with many features\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "‚úÖ Always scale features for regularization  \n",
    "‚úÖ Use train/test split  \n",
    "‚úÖ Try multiple alpha values  \n",
    "‚úÖ Compare train vs test performance  \n",
    "‚úÖ Check coefficient magnitudes  \n",
    "‚úÖ Visualize predictions vs actuals  \n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "‚Üí **Notebook 02:** Tree-Based Models  \n",
    "‚Üí **Lab:** Practice with real datasets  \n",
    "‚Üí **Project:** Compare linear vs tree models  \n",
    "\n",
    "---\n",
    "\n",
    "**Great job! You now understand all linear modeling techniques! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
