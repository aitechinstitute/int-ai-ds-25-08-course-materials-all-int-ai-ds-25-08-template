{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** Â· *Intermediate AI & Data Science*\n",
    "### Week 8 - Notebook 02: Tree-Based Models Complete Guide\n",
    "**Instructor:** Amir Charkhi | **Goal:** Master decision trees and ensemble methods\n",
    "\n",
    "> From single trees to powerful forests\n",
    "\n",
    "## ðŸ“š What You'll Learn Today\n",
    "\n",
    "**Tree-Based Models Covered:**\n",
    "1. âœ… Decision Trees (Classification & Regression)\n",
    "2. âœ… Random Forest (Bagging)\n",
    "3. âœ… Gradient Boosting (Boosting)\n",
    "4. âœ… XGBoost (Advanced Boosting)\n",
    "5. âœ… Feature Importance\n",
    "6. âœ… Hyperparameter Tuning\n",
    "\n",
    "**Why Tree Models?**\n",
    "- Handle non-linear relationships\n",
    "- No feature scaling needed\n",
    "- Automatic feature interactions\n",
    "- Easy to interpret (single tree)\n",
    "- Often win Kaggle competitions!\n",
    "\n",
    "**Time**: 90 minutes | **Prerequisites**: Week 7 & Linear Models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"ðŸŒ³ WEEK 8: TREE-BASED MODELS\")\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML imports\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.datasets import make_classification, load_diabetes\n",
    "\n",
    "print(\"âœ… Tree libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Decision Trees\n",
    "\n",
    "**How it works:** Series of yes/no questions\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Is age > 30?\n",
    "â”œâ”€ Yes: Is income > 50k?\n",
    "â”‚  â”œâ”€ Yes: Approve loan âœ…\n",
    "â”‚  â””â”€ No:  Deny loan âŒ\n",
    "â””â”€ No:  Deny loan âŒ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Create Simple Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification data\n",
    "np.random.seed(42)\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {len(X)} samples\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "print(f\"Class balance: {(y==1).mean():.1%} positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two classes\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red', 'blue']\n",
    "for class_value in [0, 1]:\n",
    "    mask = y == class_value\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], \n",
    "                c=colors[class_value], \n",
    "                label=f'Class {class_value}',\n",
    "                alpha=0.6, s=50)\n",
    "\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Classification Problem', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(X_train)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4: Train a Small Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shallow tree for visualization\n",
    "tree_small = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_small.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = tree_small.score(X_train, y_train)\n",
    "test_acc = tree_small.score(X_test, y_test)\n",
    "\n",
    "print(\"Decision Tree (max_depth=3):\")\n",
    "print(f\"  Training accuracy: {train_acc:.3f}\")\n",
    "print(f\"  Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5: Visualize the Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision tree\n",
    "plt.figure(figsize=(15, 8))\n",
    "plot_tree(tree_small, \n",
    "          feature_names=['Feature 1', 'Feature 2'],\n",
    "          class_names=['Class 0', 'Class 1'],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title('Decision Tree Structure', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ How to read this:\")\n",
    "print(\"  - Top box: root node (first question)\")\n",
    "print(\"  - Each box: a decision point\")\n",
    "print(\"  - Color: class prediction (orange=class 0, blue=class 1)\")\n",
    "print(\"  - Samples: number of training examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6: Visualize Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mesh for decision boundary\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "print(\"Creating decision boundary visualization...\")\n",
    "print(\"(This creates a grid and predicts each point)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on mesh\n",
    "Z = tree_small.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, \n",
    "            cmap='RdBu', edgecolor='black', s=50)\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Decision Tree: Decision Boundary', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(label='Predicted Class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Notice the rectangular decision boundaries!\")\n",
    "print(\"   Trees split space with axis-aligned cuts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Overfitting Problem\n",
    "\n",
    "**Trees can grow too deep** â†’ memorize training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Train a Deep Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tree with no depth limit\n",
    "tree_deep = DecisionTreeClassifier(random_state=42)\n",
    "tree_deep.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "deep_train = tree_deep.score(X_train, y_train)\n",
    "deep_test = tree_deep.score(X_test, y_test)\n",
    "\n",
    "print(\"Deep Decision Tree (no limit):\")\n",
    "print(f\"  Training accuracy: {deep_train:.3f}\")\n",
    "print(f\"  Test accuracy: {deep_test:.3f}\")\n",
    "print(f\"  Tree depth: {tree_deep.get_depth()}\")\n",
    "print(f\"  Number of leaves: {tree_deep.get_n_leaves()}\")\n",
    "\n",
    "if deep_train - deep_test > 0.1:\n",
    "    print(\"\\nâš ï¸ OVERFITTING: Perfect on training, worse on test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Compare Different Depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different max_depth values\n",
    "depths = range(1, 15)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    train_scores.append(tree.score(X_train, y_train))\n",
    "    test_scores.append(tree.score(X_test, y_test))\n",
    "\n",
    "print(\"Tested depths from 1 to 14\")\n",
    "print(f\"Best test score: {max(test_scores):.3f} at depth {test_scores.index(max(test_scores))+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Visualize Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training vs test scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(depths, train_scores, 'o-', label='Training', linewidth=2)\n",
    "plt.plot(depths, test_scores, 's-', label='Test', linewidth=2)\n",
    "plt.xlabel('Max Depth', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Decision Tree: Depth vs Performance', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Key insights:\")\n",
    "print(\"  - Shallow tree: underfits (both scores low)\")\n",
    "print(\"  - Medium depth: good balance\")\n",
    "print(\"  - Deep tree: overfits (train high, test drops)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Random Forest (Ensemble Power!)\n",
    "\n",
    "**Idea:** Train many trees, average their predictions\n",
    "\n",
    "**How:** Each tree sees different random subset of data + features\n",
    "\n",
    "**Result:** More accurate + less overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Train Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,  # number of trees\n",
    "    max_depth=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"âœ… Random Forest trained!\")\n",
    "print(f\"   Number of trees: {rf.n_estimators}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Evaluate Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare single tree vs forest\n",
    "rf_train = rf.score(X_train, y_train)\n",
    "rf_test = rf.score(X_test, y_test)\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(\"\\nSingle Deep Tree:\")\n",
    "print(f\"  Training: {deep_train:.3f}\")\n",
    "print(f\"  Test:     {deep_test:.3f}\")\n",
    "print(f\"  Gap:      {deep_train - deep_test:.3f}\")\n",
    "\n",
    "print(\"\\nRandom Forest:\")\n",
    "print(f\"  Training: {rf_train:.3f}\")\n",
    "print(f\"  Test:     {rf_test:.3f}\")\n",
    "print(f\"  Gap:      {rf_train - rf_test:.3f}\")\n",
    "\n",
    "if rf_test > deep_test:\n",
    "    print(\"\\nâœ… Random Forest performs better!\")\n",
    "    print(\"   Less overfitting, better generalization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Effect of Number of Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different numbers of trees\n",
    "n_trees = [1, 5, 10, 25, 50, 100, 200]\n",
    "train_scores_rf = []\n",
    "test_scores_rf = []\n",
    "\n",
    "for n in n_trees:\n",
    "    rf_temp = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    rf_temp.fit(X_train, y_train)\n",
    "    train_scores_rf.append(rf_temp.score(X_train, y_train))\n",
    "    test_scores_rf.append(rf_temp.score(X_test, y_test))\n",
    "\n",
    "print(\"Tested different forest sizes\")\n",
    "print(f\"Best with {n_trees[test_scores_rf.index(max(test_scores_rf))]} trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: Visualize Forest Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance vs number of trees\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_trees, train_scores_rf, 'o-', label='Training', linewidth=2)\n",
    "plt.plot(n_trees, test_scores_rf, 's-', label='Test', linewidth=2)\n",
    "plt.xlabel('Number of Trees', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Random Forest: Effect of Forest Size', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ More trees usually help, but diminishing returns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5: Random Forest Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on mesh\n",
    "Z_rf = rf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_rf = Z_rf.reshape(xx.shape)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z_rf, alpha=0.3, cmap='RdBu')\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, \n",
    "            cmap='RdBu', edgecolor='black', s=50)\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Random Forest: Decision Boundary', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(label='Predicted Class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Smoother boundary than single tree!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Feature Importance\n",
    "\n",
    "**Question:** Which features matter most?\n",
    "\n",
    "**Trees tell us!** Based on how much each feature improves splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Create Dataset with Real Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes dataset for regression\n",
    "diabetes = load_diabetes()\n",
    "X_diab = diabetes.data\n",
    "y_diab = diabetes.target\n",
    "feature_names = diabetes.feature_names\n",
    "\n",
    "print(f\"Diabetes dataset: {len(X_diab)} patients\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Target: Disease progression (continuous)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Train Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
    "    X_diab, y_diab, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Random Forest for regression\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train_d, y_train_d)\n",
    "\n",
    "# Evaluate\n",
    "r2 = rf_reg.score(X_test_d, y_test_d)\n",
    "print(f\"âœ… Random Forest Regressor trained!\")\n",
    "print(f\"   Test RÂ² Score: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3: Extract Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get importances\n",
    "importances = rf_reg.feature_importances_\n",
    "\n",
    "# Create DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(importance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4: Visualize Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['feature'], importance_df['importance'], color='skyblue')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title('Random Forest: Feature Importances', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ’¡ Most important feature: {importance_df.iloc[0]['feature']}\")\n",
    "print(f\"   Contributes {importance_df.iloc[0]['importance']:.1%} to predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*[Continuing with Gradient Boosting in next section...]*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
