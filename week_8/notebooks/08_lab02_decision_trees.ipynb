{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 8 - Lab 02: Decision Trees & Random Forests\n",
    "**Instructor:** Amir Charkhi | **Type:** Hands-On Practice\n",
    "\n",
    "> Master Tree-Based Models for Classification and Regression\n",
    "\n",
    "## üéØ Lab Objectives\n",
    "\n",
    "In this lab, you'll practice:\n",
    "- Building Decision Tree classifiers and regressors\n",
    "- Understanding tree parameters and preventing overfitting\n",
    "- Visualizing decision trees\n",
    "- Building Random Forest ensembles\n",
    "- Extracting feature importance\n",
    "- Comparing tree models with linear models\n",
    "\n",
    "**Time**: 40-50 minutes  \n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ (Intermediate-Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Quick Reference\n",
    "\n",
    "**Decision Trees:**\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# Classification Tree\n",
    "dt_clf = DecisionTreeClassifier(max_depth=5, min_samples_split=10, random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# Regression Tree\n",
    "dt_reg = DecisionTreeRegressor(max_depth=5, min_samples_leaf=5, random_state=42)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# Feature importance\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Visualize tree\n",
    "plot_tree(dt_clf, feature_names=features, class_names=classes, filled=True)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Run this cell first!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, load_wine, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "print(\"‚úÖ Setup complete! Let's grow some trees! üå≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üå≥ Exercise 1: Classification Trees\n",
    "\n",
    "Let's start with Decision Tree classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Build a Simple Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset for classification\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name='species')\n",
    "\n",
    "print(f\"Dataset: {len(X)} samples, {X.shape[1]} features, {len(np.unique(y))} classes\")\n",
    "print(f\"Features: {list(X.columns)}\")\n",
    "\n",
    "# TODO 1.1: Build an unconstrained Decision Tree\n",
    "# Steps:\n",
    "#   1. Split data (80/20, stratified, random_state=42)\n",
    "#   2. Create DecisionTreeClassifier with only random_state=42 (no constraints!)\n",
    "#   3. Fit the model\n",
    "#   4. Calculate train and test accuracy\n",
    "\n",
    "# Your code here:\n",
    "X_train, X_test, y_train, y_test = # Split\n",
    "\n",
    "dt = # Create decision tree (no max_depth or other constraints)\n",
    "# Fit the tree\n",
    "\n",
    "train_acc = # Training accuracy\n",
    "test_acc = # Test accuracy\n",
    "\n",
    "# Validation (Don't modify)\n",
    "print(f\"\\nTree Information:\")\n",
    "print(f\"  Max depth: {dt.get_depth()}\")\n",
    "print(f\"  Number of leaves: {dt.get_n_leaves()}\")\n",
    "print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"  Test Accuracy:     {test_acc:.4f}\")\n",
    "print(f\"  Accuracy Gap:      {train_acc - test_acc:.4f}\")\n",
    "\n",
    "if train_acc > 0.95 and test_acc > 0.85:\n",
    "    if train_acc - test_acc > 0.05:\n",
    "        print(\"\\n‚ö†Ô∏è  High accuracy but large gap suggests overfitting!\")\n",
    "    print(\"\\n‚úÖ Tree trained successfully!\")\n",
    "    print(\"üéâ Task 1.1 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Visualize the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1.2: Visualize the decision tree\n",
    "# Requirements:\n",
    "#   1. Create a smaller tree (max_depth=3) for better visualization\n",
    "#   2. Use plot_tree to visualize it\n",
    "#   3. Include feature names and class names\n",
    "\n",
    "# Your code here:\n",
    "# Create a smaller tree for visualization\n",
    "dt_small = # DecisionTreeClassifier with max_depth=3\n",
    "# Fit the tree\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(20, 10))\n",
    "# Use plot_tree with:\n",
    "#   - feature_names from iris\n",
    "#   - class_names from iris\n",
    "#   - filled=True for colors\n",
    "#   - fontsize=10\n",
    "\n",
    "\n",
    "plt.title('Decision Tree Visualization (max_depth=3)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° How to read the tree:\")\n",
    "print(\"  - Each box is a decision node or leaf\")\n",
    "print(\"  - Top line: splitting rule (e.g., 'petal width <= 0.8')\")\n",
    "print(\"  - 'gini': impurity measure (lower = more pure)\")\n",
    "print(\"  - 'samples': number of training samples reaching this node\")\n",
    "print(\"  - 'value': class distribution at this node\")\n",
    "print(\"  - 'class': predicted class for this node\")\n",
    "print(\"\\n‚úÖ Task 1.2 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Preventing Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1.3: Compare trees with different constraints\n",
    "# Requirements:\n",
    "#   1. Train 4 trees with different max_depth values: [2, 5, 10, None]\n",
    "#   2. Calculate train and test accuracy for each\n",
    "#   3. Plot the results\n",
    "#   4. Identify the best max_depth\n",
    "\n",
    "# Your code here:\n",
    "max_depths = [2, 5, 10, None]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    # Create tree with this max_depth\n",
    "    \n",
    "    # Train and evaluate\n",
    "    \n",
    "    # Store results\n",
    "    pass\n",
    "\n",
    "# Visualize\n",
    "depth_labels = [str(d) if d is not None else 'None' for d in max_depths]\n",
    "x = np.arange(len(max_depths))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x - width/2, train_scores, width, label='Training', alpha=0.8)\n",
    "plt.bar(x + width/2, test_scores, width, label='Test', alpha=0.8)\n",
    "plt.xlabel('Max Depth', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Effect of max_depth on Overfitting', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x, depth_labels)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best depth\n",
    "best_idx = np.argmax(test_scores)\n",
    "best_depth = max_depths[best_idx]\n",
    "print(f\"\\nBest max_depth: {best_depth}\")\n",
    "print(f\"Test Accuracy: {test_scores[best_idx]:.4f}\")\n",
    "print(\"\\nüí° Sweet spot: deep enough to learn patterns, shallow enough to generalize\")\n",
    "print(\"\\n‚úÖ Task 1.3 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Exercise 2: Regression Trees\n",
    "\n",
    "Decision Trees work for regression too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Build a Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing for regression\n",
    "housing = fetch_california_housing()\n",
    "X_reg = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y_reg = pd.Series(housing.target, name='MedHouseVal')\n",
    "\n",
    "# TODO 2.1: Compare Linear Regression vs Decision Tree Regression\n",
    "# Steps:\n",
    "#   1. Split data (80/20, random_state=42)\n",
    "#   2. Train LinearRegression\n",
    "#   3. Train DecisionTreeRegressor (max_depth=10)\n",
    "#   4. Calculate RMSE and R¬≤ for both\n",
    "#   5. Compare the results\n",
    "\n",
    "# Your code here:\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = # Split\n",
    "\n",
    "# Linear Regression\n",
    "lr = # Create and fit\n",
    "\n",
    "y_pred_lr = # Predict\n",
    "rmse_lr = # Calculate RMSE\n",
    "r2_lr = # Calculate R¬≤\n",
    "\n",
    "# Decision Tree Regression\n",
    "dt_reg = # Create with max_depth=10 and fit\n",
    "\n",
    "y_pred_dt = # Predict\n",
    "rmse_dt = # Calculate RMSE\n",
    "r2_dt = # Calculate R¬≤\n",
    "\n",
    "# Validation and Comparison\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'RMSE':>15} {'R¬≤':>15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Linear Regression':<20} {rmse_lr:>15.4f} {r2_lr:>15.4f}\")\n",
    "print(f\"{'Decision Tree':<20} {rmse_dt:>15.4f} {r2_dt:>15.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if r2_dt > 0.6:\n",
    "    print(\"\\n‚úÖ Decision Tree captures non-linear patterns!\")\n",
    "    print(\"üéâ Task 2.1 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2.2: Create prediction vs actual plots for both models\n",
    "# Compare how well each model predicts\n",
    "\n",
    "# Your code here:\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Linear Regression plot\n",
    "axes[0].scatter(y_test_reg, y_pred_lr, alpha=0.5, s=20)\n",
    "axes[0].plot([y_test_reg.min(), y_test_reg.max()], \n",
    "             [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Price', fontsize=11)\n",
    "axes[0].set_ylabel('Predicted Price', fontsize=11)\n",
    "axes[0].set_title(f'Linear Regression\\n(R¬≤={r2_lr:.3f})', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Decision Tree plot\n",
    "axes[1].scatter(y_test_reg, y_pred_dt, alpha=0.5, s=20)\n",
    "axes[1].plot([y_test_reg.min(), y_test_reg.max()], \n",
    "             [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Price', fontsize=11)\n",
    "axes[1].set_ylabel('Predicted Price', fontsize=11)\n",
    "axes[1].set_title(f'Decision Tree\\n(R¬≤={r2_dt:.3f})', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Perfect predictions fall on the red diagonal line\")\n",
    "print(\"‚úÖ Task 2.2 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üå≤üå≥üå≤ Exercise 3: Random Forests\n",
    "\n",
    "Many trees are better than one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Build a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to Wine classification for better comparison\n",
    "wine = load_wine()\n",
    "X_wine = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "y_wine = pd.Series(wine.target, name='wine_class')\n",
    "\n",
    "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(\n",
    "    X_wine, y_wine, test_size=0.2, stratify=y_wine, random_state=42\n",
    ")\n",
    "\n",
    "# TODO 3.1: Compare Decision Tree vs Random Forest\n",
    "# Requirements:\n",
    "#   1. Train Decision Tree (max_depth=10)\n",
    "#   2. Train Random Forest (n_estimators=100, max_depth=10)\n",
    "#   3. Compare accuracy and cross-validation scores\n",
    "#   4. Show that Random Forest is more stable\n",
    "\n",
    "# Your code here:\n",
    "# Single Decision Tree\n",
    "dt_single = # Create and fit\n",
    "\n",
    "dt_test_acc = # Test accuracy\n",
    "dt_cv_scores = # 5-fold CV scores\n",
    "\n",
    "# Random Forest\n",
    "rf = # Create and fit (n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "rf_test_acc = # Test accuracy\n",
    "rf_cv_scores = # 5-fold CV scores\n",
    "\n",
    "# Comparison\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<20} {'Test Acc':>15} {'CV Mean':>15} {'CV Std':>15}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Decision Tree':<20} {dt_test_acc:>15.4f} {dt_cv_scores.mean():>15.4f} {dt_cv_scores.std():>15.4f}\")\n",
    "print(f\"{'Random Forest':<20} {rf_test_acc:>15.4f} {rf_cv_scores.mean():>15.4f} {rf_cv_scores.std():>15.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüí° Random Forest Benefits:\")\n",
    "print(\"  - Usually higher accuracy than single tree\")\n",
    "print(\"  - Lower variance (more stable predictions)\")\n",
    "print(\"  - Less prone to overfitting\")\n",
    "print(\"\\n‚úÖ Task 3.1 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3.2: Extract and visualize feature importance from Random Forest\n",
    "# Requirements:\n",
    "#   1. Get feature_importances_ from the Random Forest\n",
    "#   2. Create a DataFrame with features and their importance\n",
    "#   3. Sort by importance\n",
    "#   4. Create a horizontal bar plot\n",
    "\n",
    "# Your code here:\n",
    "# Get feature importances\n",
    "importances = # rf.feature_importances_\n",
    "\n",
    "# Create DataFrame\n",
    "feature_imp = pd.DataFrame({\n",
    "    'Feature': # Feature names\n",
    "    'Importance': # Importances\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Create horizontal bar plot\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top features\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "print(feature_imp.head())\n",
    "\n",
    "print(\"\\nüí° Feature importance shows which features the trees used most for splitting\")\n",
    "print(\"‚úÖ Task 3.2 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3: Effect of Number of Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3.3: See how performance changes with number of trees\n",
    "# Requirements:\n",
    "#   1. Try different n_estimators: [10, 50, 100, 200, 500]\n",
    "#   2. Calculate test accuracy for each\n",
    "#   3. Plot the results\n",
    "#   4. Note where performance plateaus\n",
    "\n",
    "# Your code here:\n",
    "n_estimators_list = [10, 50, 100, 200, 500]\n",
    "test_accs = []\n",
    "\n",
    "for n_est in n_estimators_list:\n",
    "    # Create and train Random Forest with this n_estimators\n",
    "    \n",
    "    # Calculate test accuracy\n",
    "    \n",
    "    # Append to list\n",
    "    pass\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_estimators_list, test_accs, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Trees (n_estimators)', fontsize=12)\n",
    "plt.ylabel('Test Accuracy', fontsize=12)\n",
    "plt.title('Random Forest: Effect of Number of Trees', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest n_estimators: {n_estimators_list[np.argmax(test_accs)]}\")\n",
    "print(f\"Best accuracy: {max(test_accs):.4f}\")\n",
    "print(\"\\nüí° More trees = better performance but diminishing returns\")\n",
    "print(\"   Also increases training time!\")\n",
    "print(\"\\n‚úÖ Task 3.3 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Exercise 4: Hyperparameter Tuning\n",
    "\n",
    "Find the optimal tree configuration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Grid Search for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.1: Use GridSearchCV to find best Decision Tree parameters\n",
    "# Requirements:\n",
    "#   1. Define parameter grid for max_depth, min_samples_split, min_samples_leaf\n",
    "#   2. Use GridSearchCV with 5-fold cross-validation\n",
    "#   3. Fit on training data\n",
    "#   4. Print best parameters and best score\n",
    "\n",
    "# Your code here:\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = # GridSearchCV with DecisionTreeClassifier\n",
    "\n",
    "# Fit\n",
    "\n",
    "\n",
    "# Results\n",
    "print(\"Grid Search Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Test the best model\n",
    "best_dt = grid_search.best_estimator_\n",
    "test_acc = best_dt.score(X_test_w, y_test_w)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "if test_acc > 0.90:\n",
    "    print(\"\\n‚úÖ Excellent! Tuned tree performs great!\")\n",
    "    print(\"üéâ Task 4.1 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Grid Search for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.2: Tune Random Forest hyperparameters\n",
    "# Try different combinations of n_estimators, max_depth, and min_samples_split\n",
    "\n",
    "# Your code here:\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "print(\"Tuning Random Forest...\")\n",
    "print(f\"Total combinations to try: {3 * 3 * 2}\")\n",
    "\n",
    "grid_search_rf = # GridSearchCV with RandomForestClassifier\n",
    "\n",
    "# Fit (this may take a minute)\n",
    "\n",
    "\n",
    "print(\"\\nRandom Forest Grid Search Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best parameters: {grid_search_rf.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search_rf.best_score_:.4f}\")\n",
    "\n",
    "# Test\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "test_acc_rf = best_rf.score(X_test_w, y_test_w)\n",
    "print(f\"Test accuracy: {test_acc_rf:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Task 4.2 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ Exercise 5: Final Comparison Challenge\n",
    "\n",
    "Compare all models you've learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: Complete Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5.1: Compare Linear, Tree, and Forest models\n",
    "# Requirements:\n",
    "#   1. Train: Logistic Regression, Decision Tree (tuned), Random Forest (tuned)\n",
    "#   2. Evaluate all with cross-validation\n",
    "#   3. Test on hold-out set\n",
    "#   4. Create comprehensive comparison table\n",
    "#   5. Identify the best model\n",
    "\n",
    "print(\"üèÜ Complete Model Comparison Challenge\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Your complete solution here:\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': best_dt,  # From Task 4.1\n",
    "    'Random Forest': best_rf   # From Task 4.2\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_w, y_train_w, cv=cv)\n",
    "    \n",
    "    # Train on full training set\n",
    "    model.fit(X_train_w, y_train_w)\n",
    "    \n",
    "    # Test accuracy\n",
    "    test_acc = model.score(X_test_w, y_test_w)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'CV Mean': f\"{cv_scores.mean():.4f}\",\n",
    "        'CV Std': f\"{cv_scores.std():.4f}\",\n",
    "        'Test Acc': f\"{test_acc:.4f}\"\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualization\n",
    "test_accs = [float(r['Test Acc']) for r in results]\n",
    "model_names = [r['Model'] for r in results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "bars = plt.bar(model_names, test_accs, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.ylabel('Test Accuracy', fontsize=12)\n",
    "plt.title('Final Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylim([min(test_accs) - 0.05, 1.0])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, test_accs):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{acc:.3f}',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Winner\n",
    "best_idx = np.argmax(test_accs)\n",
    "print(f\"\\nüèÜ WINNER: {model_names[best_idx]}\")\n",
    "print(f\"   Test Accuracy: {test_accs[best_idx]:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"  - Trees can capture non-linear patterns\")\n",
    "print(\"  - Random Forests are usually more stable than single trees\")\n",
    "print(\"  - Hyperparameter tuning improves performance\")\n",
    "print(\"  - More complex ‚â† always better (depends on data)\")\n",
    "\n",
    "print(\"\\n‚úÖ Task 5.1 Complete!\")\n",
    "print(\"üéâ Lab 02 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ Lab Complete!\n",
    "\n",
    "### What You Practiced:\n",
    "\n",
    "‚úÖ **Exercise 1**: Decision Tree classification and visualization  \n",
    "‚úÖ **Exercise 2**: Decision Tree regression  \n",
    "‚úÖ **Exercise 3**: Random Forest ensembles and feature importance  \n",
    "‚úÖ **Exercise 4**: Hyperparameter tuning with GridSearchCV  \n",
    "‚úÖ **Exercise 5**: Comprehensive model comparison  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Decision Trees**:\n",
    "   - Capture non-linear relationships naturally\n",
    "   - Easy to visualize and interpret\n",
    "   - Prone to overfitting without constraints\n",
    "   - Key parameters: `max_depth`, `min_samples_split`, `min_samples_leaf`\n",
    "\n",
    "2. **Random Forests**:\n",
    "   - Ensemble of many decision trees\n",
    "   - More stable and robust than single trees\n",
    "   - Less prone to overfitting\n",
    "   - Provides feature importance rankings\n",
    "   - More trees = better (but diminishing returns)\n",
    "\n",
    "3. **When to Use Tree-Based Models**:\n",
    "   - ‚úÖ Non-linear relationships in data\n",
    "   - ‚úÖ Mixed feature types (categorical + numerical)\n",
    "   - ‚úÖ Need feature importance\n",
    "   - ‚úÖ Don't need feature scaling\n",
    "   - ‚ùå High-dimensional sparse data\n",
    "   - ‚ùå Need linear interpretability\n",
    "\n",
    "4. **Linear vs Tree Models**:\n",
    "   - **Linear**: Fast, interpretable coefficients, needs scaling\n",
    "   - **Trees**: Handle non-linearity, no scaling needed, feature importance\n",
    "\n",
    "### Decision Tree Cheat Sheet:\n",
    "\n",
    "```python\n",
    "# Prevent Overfitting:\n",
    "- max_depth: Limit tree depth (start with 5-10)\n",
    "- min_samples_split: Minimum samples to split a node (try 5-10)\n",
    "- min_samples_leaf: Minimum samples per leaf (try 2-5)\n",
    "- max_features: Number of features per split (for Random Forest)\n",
    "\n",
    "# Random Forest Tips:\n",
    "- Start with n_estimators=100\n",
    "- More trees = slower but often better\n",
    "- Can use all CPU cores: n_jobs=-1\n",
    "- Check feature_importances_ for insights\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try **Lab 03** for a mini-project combining everything!\n",
    "- Experiment with Gradient Boosting (coming soon!)\n",
    "- Apply these models to your own datasets\n",
    "- Try ensemble methods (stacking, voting)\n",
    "\n",
    "**Fantastic work! You've mastered tree-based models! üå≤üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
