{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Metrics: A Comprehensive Guide\n",
    "\n",
    "This notebook provides a thorough introduction to evaluation metrics for both regression and classification models. We'll cover the theory behind each metric and demonstrate their practical application using simple linear models.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Regression Metrics](#regression)\n",
    "3. [Classification Metrics](#classification)\n",
    "4. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id='introduction'></a>\n",
    "\n",
    "Model evaluation is crucial in machine learning. It helps us understand how well our model performs and whether it's suitable for our task. Different types of problems require different evaluation metrics.\n",
    "\n",
    "- **Regression**: Predicting continuous values (e.g., house prices, temperature)\n",
    "- **Classification**: Predicting discrete categories (e.g., spam/not spam, disease diagnosis)\n",
    "\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Regression Metrics <a id='regression'></a>\n",
    "\n",
    "Regression metrics evaluate how well a model predicts continuous numerical values. Let's explore the most common metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Regression Dataset\n",
    "\n",
    "First, let's create a simple dataset for demonstrating regression metrics. We'll generate data with a linear relationship plus some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic regression data\n",
    "# Simulating: y = 3x + 5 + noise\n",
    "n_samples = 200\n",
    "X_reg = np.random.randn(n_samples, 1) * 10\n",
    "y_reg = 3 * X_reg.flatten() + 5 + np.random.randn(n_samples) * 10\n",
    "\n",
    "# Split the data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train a simple linear regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_reg = lr_model.predict(X_test_reg)\n",
    "\n",
    "# Visualize the data and predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test_reg, y_test_reg, alpha=0.6, label='Actual values')\n",
    "plt.scatter(X_test_reg, y_pred_reg, alpha=0.6, label='Predicted values')\n",
    "plt.xlabel('Predicted (y)')\n",
    "plt.ylabel('Target (y)')\n",
    "plt.title('Regression: Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Model coefficients: {lr_model.coef_[0]:.2f}\")\n",
    "print(f\"Model intercept: {lr_model.intercept_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mean Absolute Error (MAE)\n",
    "\n",
    "### Theory\n",
    "MAE measures the average absolute difference between predicted and actual values.\n",
    "\n",
    "$$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$\n",
    "\n",
    "Where:\n",
    "- $n$ = number of samples\n",
    "- $y_i$ = actual value\n",
    "- $\\hat{y}_i$ = predicted value\n",
    "\n",
    "**Characteristics:**\n",
    "- Easy to interpret (same units as the target variable)\n",
    "- Less sensitive to outliers than MSE\n",
    "- Range: [0, âˆž), lower is better\n",
    "- All errors weighted equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(errors, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "\n",
    "# Manual calculation to understand the formula\n",
    "manual_mae = np.mean(np.abs(y_test_reg - y_pred_reg))\n",
    "print(f\"Manual MAE calculation: {manual_mae:.2f}\")\n",
    "\n",
    "# Visualize errors\n",
    "errors = y_test_reg - y_pred_reg\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(errors, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(0, color='red', linestyle='--', linewidth=2, label='Perfect predictions')\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test_reg, errors, alpha=0.6)\n",
    "plt.axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Prediction Error')\n",
    "plt.title('Residual Plot')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nInterpretation: On average, our predictions are off by {mae:.2f} units.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mean Squared Error (MSE)\n",
    "\n",
    "### Theory\n",
    "MSE measures the average squared difference between predicted and actual values.\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Characteristics:**\n",
    "- Penalizes larger errors more heavily (due to squaring)\n",
    "- More sensitive to outliers than MAE\n",
    "- Not in the same units as the target variable (squared units)\n",
    "- Range: [0, âˆž), lower is better\n",
    "- Differentiable everywhere (useful for optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "\n",
    "# Manual calculation\n",
    "manual_mse = np.mean((y_test_reg - y_pred_reg) ** 2)\n",
    "print(f\"Manual MSE calculation: {manual_mse:.2f}\")\n",
    "\n",
    "# Demonstrate the effect of outliers\n",
    "# Compare MAE and MSE with and without an outlier\n",
    "errors_example = np.array([1, 2, 3, 4, 5])  # Small errors\n",
    "errors_with_outlier = np.array([1, 2, 3, 4, 50])  # One large error\n",
    "\n",
    "mae_no_outlier = np.mean(np.abs(errors_example))\n",
    "mse_no_outlier = np.mean(errors_example ** 2)\n",
    "\n",
    "mae_with_outlier = np.mean(np.abs(errors_with_outlier))\n",
    "mse_with_outlier = np.mean(errors_with_outlier ** 2)\n",
    "\n",
    "print(\"\\n--- Effect of Outliers ---\")\n",
    "print(f\"Without outlier - MAE: {mae_no_outlier:.2f}, MSE: {mse_no_outlier:.2f}\")\n",
    "print(f\"With outlier    - MAE: {mae_with_outlier:.2f}, MSE: {mse_with_outlier:.2f}\")\n",
    "print(f\"\\nMAE increased by: {((mae_with_outlier/mae_no_outlier - 1) * 100):.1f}%\")\n",
    "print(f\"MSE increased by: {((mse_with_outlier/mse_no_outlier - 1) * 100):.1f}%\")\n",
    "print(\"\\nNotice how MSE is much more sensitive to the outlier!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Root Mean Squared Error (RMSE)\n",
    "\n",
    "### Theory\n",
    "RMSE is the square root of MSE, bringing the error metric back to the same units as the target variable.\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "**Characteristics:**\n",
    "- Same units as the target variable (like MAE)\n",
    "- Still penalizes large errors more than MAE\n",
    "- More interpretable than MSE\n",
    "- Range: [0, âˆž), lower is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "\n",
    "# Manual calculation\n",
    "manual_rmse = np.sqrt(np.mean((y_test_reg - y_pred_reg) ** 2))\n",
    "print(f\"Manual RMSE calculation: {manual_rmse:.2f}\")\n",
    "\n",
    "# Compare all three metrics\n",
    "print(\"\\n--- Comparison of Error Metrics ---\")\n",
    "print(f\"MAE:  {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MSE:  {mse:.2f}\")\n",
    "\n",
    "# Visualization\n",
    "metrics_comparison = pd.DataFrame({\n",
    "    'Metric': ['MAE', 'RMSE'],\n",
    "    'Value': [mae, rmse]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(metrics_comparison['Metric'], metrics_comparison['Value'], alpha=0.7)\n",
    "plt.ylabel('Error Value')\n",
    "plt.title('Comparison of MAE and RMSE')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(metrics_comparison['Value']):\n",
    "    plt.text(i, v + 0.5, f'{v:.2f}', ha='center', fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nNote: RMSE ({rmse:.2f}) > MAE ({mae:.2f})\")\n",
    "print(\"This is always true when there's variability in errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. R-squared (RÂ² or Coefficient of Determination)\n",
    "\n",
    "### Theory\n",
    "RÂ² represents the proportion of variance in the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}$$\n",
    "\n",
    "Where:\n",
    "- $SS_{res}$ = Sum of Squares of Residuals (unexplained variance)\n",
    "- $SS_{tot}$ = Total Sum of Squares (total variance)\n",
    "- $\\bar{y}$ = mean of actual values\n",
    "\n",
    "**Characteristics:**\n",
    "- Range: (0, 1], where 1 is perfect prediction\n",
    "- 0 means the model performs as well as predicting the mean\n",
    "- Negative values mean the model performs worse than predicting the mean\n",
    "- Scale-independent (unitless)\n",
    "- Can be misleading with multiple features (use Adjusted RÂ² instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RÂ²\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "print(f\"R-squared (RÂ²): {r2:.4f}\")\n",
    "\n",
    "# Manual calculation to understand the formula\n",
    "ss_res = np.sum((y_test_reg - y_pred_reg) ** 2)  # Residual sum of squares\n",
    "ss_tot = np.sum((y_test_reg - np.mean(y_test_reg)) ** 2)  # Total sum of squares\n",
    "manual_r2 = 1 - (ss_res / ss_tot)\n",
    "print(f\"Manual RÂ² calculation: {manual_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation: {r2*100:.2f}% of the variance in the target variable\")\n",
    "print(\"is explained by our model.\")\n",
    "\n",
    "# Visualize the concept of RÂ²\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Predictions vs Actual\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(y_test_reg, y_pred_reg, alpha=0.6)\n",
    "plt.plot([y_test_reg.min(), y_test_reg.max()], \n",
    "         [y_test_reg.min(), y_test_reg.max()], \n",
    "         'r--', linewidth=2, label='Perfect predictions')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'Predictions vs Actual (RÂ² = {r2:.3f})')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Baseline (mean) prediction\n",
    "plt.subplot(1, 3, 2)\n",
    "mean_baseline = np.full_like(y_test_reg, np.mean(y_test_reg))\n",
    "plt.scatter(y_test_reg, mean_baseline, alpha=0.6, label='Baseline (mean)')\n",
    "plt.plot([y_test_reg.min(), y_test_reg.max()], \n",
    "         [y_test_reg.min(), y_test_reg.max()], \n",
    "         'r--', linewidth=2, label='Perfect predictions')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Baseline Model (Always predicts mean)')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: RÂ² comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "baseline_r2 = r2_score(y_test_reg, mean_baseline)\n",
    "models = ['Baseline\\n(Mean)', 'Our Model']\n",
    "r2_scores = [baseline_r2, r2]\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "plt.bar(models, r2_scores, color=colors, alpha=0.7)\n",
    "plt.ylabel('RÂ² Score')\n",
    "plt.title('Model Comparison')\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.axhline(y=1, color='green', linestyle='--', linewidth=1, alpha=0.3)\n",
    "for i, v in enumerate(r2_scores):\n",
    "    plt.text(i, v + 0.05, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBaseline RÂ²: {baseline_r2:.4f} (always predicts the mean)\")\n",
    "print(f\"Our Model RÂ²: {r2:.4f}\")\n",
    "print(f\"Improvement: {(r2 - baseline_r2):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adjusted R-squared\n",
    "\n",
    "### Theory\n",
    "Adjusted RÂ² modifies RÂ² to account for the number of predictors in the model. It penalizes the addition of unnecessary features.\n",
    "\n",
    "$$R^2_{adj} = 1 - \\frac{(1 - R^2)(n - 1)}{n - p - 1}$$\n",
    "\n",
    "Where:\n",
    "- $n$ = number of samples\n",
    "- $p$ = number of predictors (features)\n",
    "- $R^2$ = R-squared value\n",
    "\n",
    "**Characteristics:**\n",
    "- Always lower than or equal to RÂ²\n",
    "- Better for comparing models with different numbers of features\n",
    "- Penalizes overfitting\n",
    "- More reliable for multiple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Adjusted RÂ²\n",
    "n = len(y_test_reg)\n",
    "p = X_test_reg.shape[1]  # number of features\n",
    "\n",
    "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "print(f\"Adjusted R-squared: {adjusted_r2:.4f}\")\n",
    "print(f\"\\nNumber of samples (n): {n}\")\n",
    "print(f\"Number of features (p): {p}\")\n",
    "\n",
    "# Demonstrate why Adjusted RÂ² matters\n",
    "print(\"\\n--- Why Adjusted RÂ² Matters ---\")\n",
    "print(\"When we add more features:\")\n",
    "print(\"- RÂ² can only increase (or stay the same)\")\n",
    "print(\"- Adjusted RÂ² can decrease if the new feature doesn't add value\")\n",
    "print(\"- This helps prevent overfitting\")\n",
    "\n",
    "# Simulate adding irrelevant features\n",
    "n_irrelevant_features = 10\n",
    "X_with_noise = np.concatenate([\n",
    "    X_train_reg, \n",
    "    np.random.randn(len(X_train_reg), n_irrelevant_features)\n",
    "], axis=1)\n",
    "X_test_with_noise = np.concatenate([\n",
    "    X_test_reg, \n",
    "    np.random.randn(len(X_test_reg), n_irrelevant_features)\n",
    "], axis=1)\n",
    "\n",
    "# Train model with irrelevant features\n",
    "lr_noise = LinearRegression()\n",
    "lr_noise.fit(X_with_noise, y_train_reg)\n",
    "y_pred_noise = lr_noise.predict(X_test_with_noise)\n",
    "\n",
    "r2_noise = r2_score(y_test_reg, y_pred_noise)\n",
    "p_noise = X_test_with_noise.shape[1]\n",
    "adjusted_r2_noise = 1 - (1 - r2_noise) * (n - 1) / (n - p_noise - 1)\n",
    "\n",
    "print(f\"\\nWith {n_irrelevant_features} irrelevant features added:\")\n",
    "print(f\"RÂ²: {r2:.4f} â†’ {r2_noise:.4f} (change: {r2_noise - r2:+.4f})\")\n",
    "print(f\"Adj RÂ²: {adjusted_r2:.4f} â†’ {adjusted_r2_noise:.4f} (change: {adjusted_r2_noise - adjusted_r2:+.4f})\")\n",
    "print(\"\\nNotice: RÂ² slightly increased, but Adjusted RÂ² decreased!\")\n",
    "print(\"This indicates the added features don't improve the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics Summary\n",
    "\n",
    "Let's summarize all regression metrics for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table\n",
    "regression_summary = pd.DataFrame({\n",
    "    'Metric': ['MAE', 'MSE', 'RMSE', 'RÂ²', 'Adjusted RÂ²'],\n",
    "    'Value': [mae, mse, rmse, r2, adjusted_r2],\n",
    "    'Best Value': ['0', '0', '0', '1', '1'],\n",
    "    'Units': ['same as target', 'squared target units', 'same as target', 'unitless', 'unitless'],\n",
    "    'Outlier Sensitivity': ['Low', 'High', 'High', 'Medium', 'Medium']\n",
    "})\n",
    "\n",
    "print(\"\\n=== REGRESSION METRICS SUMMARY ===\")\n",
    "print(regression_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n--- When to Use Each Metric ---\")\n",
    "print(\"\\nMAE:\")\n",
    "print(\"  âœ“ When you want intuitive interpretation\")\n",
    "print(\"  âœ“ When outliers should be treated equally\")\n",
    "print(\"  âœ“ When errors are expected to be normally distributed\")\n",
    "\n",
    "print(\"\\nMSE/RMSE:\")\n",
    "print(\"  âœ“ When large errors are particularly undesirable\")\n",
    "print(\"  âœ“ When using gradient-based optimization\")\n",
    "print(\"  âœ“ RMSE preferred over MSE for interpretation\")\n",
    "\n",
    "print(\"\\nRÂ²:\")\n",
    "print(\"  âœ“ When you want to understand explained variance\")\n",
    "print(\"  âœ“ When comparing models on the same dataset\")\n",
    "print(\"  âœ“ When interpretation is more important than absolute error\")\n",
    "\n",
    "print(\"\\nAdjusted RÂ²:\")\n",
    "print(\"  âœ“ When comparing models with different numbers of features\")\n",
    "print(\"  âœ“ When preventing overfitting is important\")\n",
    "print(\"  âœ“ In multiple regression scenarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Classification Metrics <a id='classification'></a>\n",
    "\n",
    "Classification metrics evaluate how well a model predicts discrete categories. Let's explore the most important metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Classification Dataset\n",
    "\n",
    "Let's create a binary classification dataset for demonstrating classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic classification data\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_clf, y_clf = make_classification(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train a logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_clf = log_reg.predict(X_test_clf)\n",
    "y_pred_proba = log_reg.predict_proba(X_test_clf)[:, 1]  # Probability of class 1\n",
    "\n",
    "# Visualize the data and decision boundary\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_test_clf[y_test_clf == 0, 0], X_test_clf[y_test_clf == 0, 1], \n",
    "            label='Class 0', alpha=0.6, s=50)\n",
    "plt.scatter(X_test_clf[y_test_clf == 1, 0], X_test_clf[y_test_clf == 1, 1], \n",
    "            label='Class 1', alpha=0.6, s=50)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('True Labels')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test_clf[y_pred_clf == 0, 0], X_test_clf[y_pred_clf == 0, 1], \n",
    "            label='Predicted Class 0', alpha=0.6, s=50, marker='s')\n",
    "plt.scatter(X_test_clf[y_pred_clf == 1, 0], X_test_clf[y_pred_clf == 1, 1], \n",
    "            label='Predicted Class 1', alpha=0.6, s=50, marker='s')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Predicted Labels')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training samples: {len(X_train_clf)}\")\n",
    "print(f\"Test samples: {len(X_test_clf)}\")\n",
    "print(f\"Class distribution in test set: {np.bincount(y_test_clf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Confusion Matrix Foundation\n",
    "\n",
    "Before diving into individual metrics, let's understand the confusion matrix - the foundation of all classification metrics.\n",
    "\n",
    "### Theory\n",
    "A confusion matrix is a table showing the counts of:\n",
    "- **True Positives (TP)**: Correctly predicted positive class\n",
    "- **True Negatives (TN)**: Correctly predicted negative class\n",
    "- **False Positives (FP)**: Incorrectly predicted positive (Type I error)\n",
    "- **False Negatives (FN)**: Incorrectly predicted negative (Type II error)\n",
    "\n",
    "```\n",
    "                 Predicted\n",
    "                 0      1\n",
    "Actual    0     TN     FP\n",
    "          1     FN     TP\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test_clf, y_pred_clf)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"True Negatives (TN):  {tn} - Correctly predicted Class 0\")\n",
    "print(f\"False Positives (FP): {fp} - Wrongly predicted Class 1 (Type I error)\")\n",
    "print(f\"False Negatives (FN): {fn} - Wrongly predicted Class 0 (Type II error)\")\n",
    "print(f\"True Positives (TP):  {tp} - Correctly predicted Class 1\")\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "            yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Add text annotations for clarity\n",
    "plt.text(0.5, 0.25, f'TN\\n{tn}', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "plt.text(1.5, 0.25, f'FP\\n{fp}', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "plt.text(0.5, 1.25, f'FN\\n{fn}', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "plt.text(1.5, 1.25, f'TP\\n{tp}', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal correct predictions: {tn + tp}\")\n",
    "print(f\"Total incorrect predictions: {fp + fn}\")\n",
    "print(f\"Total samples: {tn + fp + fn + tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Accuracy\n",
    "\n",
    "### Theory\n",
    "Accuracy measures the proportion of correct predictions out of all predictions.\n",
    "\n",
    "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "**Characteristics:**\n",
    "- Range: [0, 1], higher is better\n",
    "- Easy to understand and interpret\n",
    "- **Problem**: Misleading with imbalanced datasets\n",
    "- Treats all errors equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_clf, y_pred_clf)\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Manual calculation\n",
    "manual_accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(f\"Manual accuracy calculation: {manual_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation: The model correctly classified {accuracy*100:.2f}% of samples.\")\n",
    "\n",
    "# Demonstrate the imbalanced data problem\n",
    "print(\"\\n--- Problem: Imbalanced Data ---\")\n",
    "print(\"Imagine a dataset where 95% of samples are Class 0:\")\n",
    "print(\"A 'dumb' model that always predicts Class 0 would have 95% accuracy!\")\n",
    "print(\"This is why we need other metrics.\")\n",
    "\n",
    "# Simulate imbalanced scenario\n",
    "imbalanced_size = 1000\n",
    "y_true_imbalanced = np.concatenate([np.zeros(950), np.ones(50)])\n",
    "y_pred_all_zero = np.zeros(imbalanced_size)  # Always predict Class 0\n",
    "\n",
    "accuracy_dumb = accuracy_score(y_true_imbalanced, y_pred_all_zero)\n",
    "print(f\"\\nDumb model (always predicts 0) accuracy: {accuracy_dumb:.4f} ({accuracy_dumb*100:.1f}%)\")\n",
    "print(\"But it missed ALL positive cases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Precision\n",
    "\n",
    "### Theory\n",
    "Precision (also called Positive Predictive Value) measures the proportion of positive predictions that were actually correct.\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "**Answers the question**: \"Of all samples predicted as positive, how many were truly positive?\"\n",
    "\n",
    "**Characteristics:**\n",
    "- Range: [0, 1], higher is better\n",
    "- Focuses on minimizing false positives\n",
    "- Important when false positives are costly\n",
    "- **Use case**: Email spam detection (don't want to flag legitimate emails as spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision\n",
    "precision = precision_score(y_test_clf, y_pred_clf)\n",
    "print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "\n",
    "# Manual calculation\n",
    "manual_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "print(f\"Manual precision calculation: {manual_precision:.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation: Of all samples predicted as positive (Class 1),\")\n",
    "print(f\"{precision*100:.2f}% were actually positive.\")\n",
    "\n",
    "print(f\"\\nOut of {tp + fp} positive predictions:\")\n",
    "print(f\"  âœ“ {tp} were correct (True Positives)\")\n",
    "print(f\"  âœ— {fp} were wrong (False Positives)\")\n",
    "\n",
    "# Real-world example\n",
    "print(\"\\n--- Real-World Example: Spam Detection ---\")\n",
    "print(\"High precision means: When we flag an email as spam, we're usually right.\")\n",
    "print(\"Low precision means: We're flagging too many legitimate emails as spam.\")\n",
    "print(\"\\nIn spam detection, FALSE POSITIVES are costly:\")\n",
    "print(\"  - Missing an important email is worse than receiving spam\")\n",
    "print(\"  - So we want HIGH PRECISION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recall (Sensitivity)\n",
    "\n",
    "### Theory\n",
    "Recall (also called Sensitivity, True Positive Rate, or Hit Rate) measures the proportion of actual positive samples that were correctly identified.\n",
    "\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "**Answers the question**: \"Of all actual positive samples, how many did we correctly identify?\"\n",
    "\n",
    "**Characteristics:**\n",
    "- Range: [0, 1], higher is better\n",
    "- Focuses on minimizing false negatives\n",
    "- Important when false negatives are costly\n",
    "- **Use case**: Disease diagnosis (don't want to miss sick patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recall\n",
    "recall = recall_score(y_test_clf, y_pred_clf)\n",
    "print(f\"Recall: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "\n",
    "# Manual calculation\n",
    "manual_recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "print(f\"Manual recall calculation: {manual_recall:.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation: Of all actual positive samples (Class 1),\")\n",
    "print(f\"we correctly identified {recall*100:.2f}%.\")\n",
    "\n",
    "print(f\"\\nOut of {tp + fn} actual positive samples:\")\n",
    "print(f\"  âœ“ {tp} were correctly identified (True Positives)\")\n",
    "print(f\"  âœ— {fn} were missed (False Negatives)\")\n",
    "\n",
    "# Real-world example\n",
    "print(\"\\n--- Real-World Example: Cancer Detection ---\")\n",
    "print(\"High recall means: We catch most cancer cases.\")\n",
    "print(\"Low recall means: We're missing too many cancer cases.\")\n",
    "print(\"\\nIn cancer detection, FALSE NEGATIVES are costly:\")\n",
    "print(\"  - Missing a cancer diagnosis can be fatal\")\n",
    "print(\"  - So we want HIGH RECALL\")\n",
    "print(\"  - Even if it means more false alarms (lower precision)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision vs Recall Trade-off\n",
    "\n",
    "There's typically a trade-off between precision and recall. Let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision and recall at different thresholds\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test_clf, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Precision and Recall vs Threshold\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(thresholds, precisions[:-1], label='Precision', linewidth=2)\n",
    "plt.plot(thresholds, recalls[:-1], label='Recall', linewidth=2)\n",
    "plt.xlabel('Classification Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Precision-Recall Curve\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(recalls, precisions, linewidth=2)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Current model's position\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter([recall], [precision], s=200, c='red', zorder=5, \n",
    "            label=f'Current Model\\n(P={precision:.2f}, R={recall:.2f})', marker='*')\n",
    "plt.plot(recalls, precisions, linewidth=2, alpha=0.3)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Model Performance')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"--- Understanding the Trade-off ---\")\n",
    "print(\"\\nAs we move the classification threshold:\")\n",
    "print(\"  â€¢ Lower threshold â†’ More positive predictions\")\n",
    "print(\"    - Higher Recall (catch more positives)\")\n",
    "print(\"    - Lower Precision (more false positives)\")\n",
    "print(\"\\n  â€¢ Higher threshold â†’ Fewer positive predictions\")\n",
    "print(\"    - Higher Precision (more confident predictions)\")\n",
    "print(\"    - Lower Recall (miss more positives)\")\n",
    "print(\"\\nThe optimal threshold depends on your application!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. F1-Score\n",
    "\n",
    "### Theory\n",
    "The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both.\n",
    "\n",
    "$$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} = \\frac{2TP}{2TP + FP + FN}$$\n",
    "\n",
    "**Why harmonic mean?** It penalizes extreme values more than the arithmetic mean.\n",
    "\n",
    "**Characteristics:**\n",
    "- Range: [0, 1], higher is better\n",
    "- Balances precision and recall\n",
    "- Useful when class distribution is imbalanced\n",
    "- Best when you need both high precision AND high recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test_clf, y_pred_clf)\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Manual calculation\n",
    "manual_f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "print(f\"Manual F1 calculation: {manual_f1:.4f}\")\n",
    "\n",
    "# Alternative formula\n",
    "manual_f1_alt = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
    "print(f\"Manual F1 (alternative formula): {manual_f1_alt:.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation: F1-score balances precision ({precision:.4f}) \")\n",
    "print(f\"and recall ({recall:.4f}) into a single metric.\")\n",
    "\n",
    "# Demonstrate why harmonic mean is used\n",
    "print(\"\\n--- Why Harmonic Mean? ---\")\n",
    "example_cases = [\n",
    "    (0.9, 0.9, \"Balanced case\"),\n",
    "    (1.0, 0.5, \"High precision, low recall\"),\n",
    "    (0.5, 1.0, \"Low precision, high recall\"),\n",
    "    (0.1, 0.9, \"Very imbalanced\")\n",
    "]\n",
    "\n",
    "for prec, rec, description in example_cases:\n",
    "    harmonic = 2 * (prec * rec) / (prec + rec)\n",
    "    arithmetic = (prec + rec) / 2\n",
    "    print(f\"\\n{description}:\")\n",
    "    print(f\"  Precision: {prec:.2f}, Recall: {rec:.2f}\")\n",
    "    print(f\"  Arithmetic mean: {arithmetic:.2f}\")\n",
    "    print(f\"  Harmonic mean (F1): {harmonic:.2f}\")\n",
    "    print(f\"  â†’ Harmonic mean penalizes imbalance!\")\n",
    "\n",
    "# Visualize precision, recall, and F1\n",
    "plt.figure(figsize=(8, 6))\n",
    "metrics_names = ['Precision', 'Recall', 'F1-Score']\n",
    "metrics_values = [precision, recall, f1]\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "\n",
    "bars = plt.bar(metrics_names, metrics_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.ylim([0, 1.1])\n",
    "plt.ylabel('Score')\n",
    "plt.title('Classification Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=1.0, color='green', linestyle='--', alpha=0.3, linewidth=1)\n",
    "\n",
    "for i, (name, value) in enumerate(zip(metrics_names, metrics_values)):\n",
    "    plt.text(i, value + 0.03, f'{value:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ROC Curve and AUC\n",
    "\n",
    "### Theory\n",
    "The ROC (Receiver Operating Characteristic) curve plots the True Positive Rate (Recall) against the False Positive Rate at various threshold settings.\n",
    "\n",
    "$$TPR (Recall) = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "$$FPR = \\frac{FP}{FP + TN}$$\n",
    "\n",
    "**AUC (Area Under the Curve)** summarizes the ROC curve into a single number.\n",
    "\n",
    "**Characteristics:**\n",
    "- AUC Range: [0, 1]\n",
    "  - 0.5 = random classifier (diagonal line)\n",
    "  - 1.0 = perfect classifier\n",
    "  - < 0.5 = worse than random (inverted predictions)\n",
    "- Threshold-independent metric\n",
    "- Good for imbalanced datasets\n",
    "- Measures the model's ability to distinguish between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test_clf, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test_clf, y_pred_proba)\n",
    "\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: ROC Curve\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.5)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Threshold effect on TPR and FPR\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(roc_thresholds, tpr[:-1], label='True Positive Rate', linewidth=2)\n",
    "plt.plot(roc_thresholds, fpr[:-1], label='False Positive Rate', linewidth=2)\n",
    "plt.xlabel('Classification Threshold')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('TPR and FPR vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: AUC interpretation\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.fill_between(fpr, tpr, alpha=0.3, label=f'AUC = {roc_auc:.3f}')\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC: Area Under ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Interpreting ROC-AUC ---\")\n",
    "print(f\"AUC = {roc_auc:.4f}\")\n",
    "if roc_auc >= 0.9:\n",
    "    print(\"  â†’ Excellent classification\")\n",
    "elif roc_auc >= 0.8:\n",
    "    print(\"  â†’ Good classification\")\n",
    "elif roc_auc >= 0.7:\n",
    "    print(\"  â†’ Fair classification\")\n",
    "elif roc_auc >= 0.6:\n",
    "    print(\"  â†’ Poor classification\")\n",
    "else:\n",
    "    print(\"  â†’ No discrimination (barely better than random)\")\n",
    "\n",
    "print(\"\\nWhat AUC means:\")\n",
    "print(f\"  {roc_auc*100:.1f}% chance that the model will rank a random\")\n",
    "print(\"  positive example higher than a random negative example.\")\n",
    "\n",
    "print(\"\\n--- When to Use ROC-AUC ---\")\n",
    "print(\"âœ“ When you care about ranking predictions\")\n",
    "print(\"âœ“ When working with imbalanced datasets\")\n",
    "print(\"âœ“ When you want a threshold-independent metric\")\n",
    "print(\"âœ— When you need to choose a specific threshold\")\n",
    "print(\"âœ— When the cost of FP and FN are very different\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report\n",
    "\n",
    "Sklearn provides a convenient classification report that shows multiple metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "print(\"=== CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(y_test_clf, y_pred_clf, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "print(\"\\n--- Understanding the Report ---\")\n",
    "print(\"\\nPer-class metrics:\")\n",
    "print(\"  â€¢ Precision: Of predictions for this class, how many were correct?\")\n",
    "print(\"  â€¢ Recall: Of actual instances of this class, how many did we find?\")\n",
    "print(\"  â€¢ F1-score: Harmonic mean of precision and recall\")\n",
    "print(\"  â€¢ Support: Number of actual instances of this class\")\n",
    "\n",
    "print(\"\\nAveraged metrics:\")\n",
    "print(\"  â€¢ Macro avg: Unweighted mean (treats all classes equally)\")\n",
    "print(\"  â€¢ Weighted avg: Weighted by number of instances per class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics Summary\n",
    "\n",
    "Let's create a comprehensive summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "classification_summary = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
    "    'Value': [\n",
    "        f\"{accuracy:.4f}\",\n",
    "        f\"{precision:.4f}\",\n",
    "        f\"{recall:.4f}\",\n",
    "        f\"{f1:.4f}\",\n",
    "        f\"{roc_auc:.4f}\"\n",
    "    ],\n",
    "    'Best Value': ['1.0', '1.0', '1.0', '1.0', '1.0'],\n",
    "    'Focuses On': [\n",
    "        'Overall correctness',\n",
    "        'Minimizing false positives',\n",
    "        'Minimizing false negatives',\n",
    "        'Balance of precision & recall',\n",
    "        'Ranking ability'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n=== CLASSIFICATION METRICS SUMMARY ===\")\n",
    "print(classification_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n--- Choosing the Right Metric ---\")\n",
    "\n",
    "print(\"\\n1. ACCURACY\")\n",
    "print(\"   Use when: Classes are balanced\")\n",
    "print(\"   Example: Coin flip prediction\")\n",
    "print(\"   Avoid when: Classes are imbalanced (e.g., fraud detection)\")\n",
    "\n",
    "print(\"\\n2. PRECISION\")\n",
    "print(\"   Use when: False positives are costly\")\n",
    "print(\"   Example: Spam detection (don't flag legitimate emails)\")\n",
    "print(\"   Question: 'When I predict positive, am I usually right?'\")\n",
    "\n",
    "print(\"\\n3. RECALL\")\n",
    "print(\"   Use when: False negatives are costly\")\n",
    "print(\"   Example: Cancer detection (don't miss sick patients)\")\n",
    "print(\"   Question: 'Am I finding all the positive cases?'\")\n",
    "\n",
    "print(\"\\n4. F1-SCORE\")\n",
    "print(\"   Use when: Need balance between precision and recall\")\n",
    "print(\"   Example: Most imbalanced classification problems\")\n",
    "print(\"   Best for: Comparing models on imbalanced data\")\n",
    "\n",
    "print(\"\\n5. ROC-AUC\")\n",
    "print(\"   Use when: Want threshold-independent evaluation\")\n",
    "print(\"   Example: Comparing different models\")\n",
    "print(\"   Best for: Understanding model's discriminative ability\")\n",
    "\n",
    "print(\"\\n--- Real-World Scenarios ---\")\n",
    "\n",
    "scenarios = [\n",
    "    (\"Email Spam Filter\", \"High Precision\", \"Don't mark real emails as spam\"),\n",
    "    (\"Cancer Screening\", \"High Recall\", \"Don't miss any cancer cases\"),\n",
    "    (\"Fraud Detection\", \"High F1\", \"Balance catching fraud vs. false alarms\"),\n",
    "    (\"Customer Churn\", \"High ROC-AUC\", \"Rank customers by churn probability\"),\n",
    "]\n",
    "\n",
    "for scenario, metric, reason in scenarios:\n",
    "    print(f\"\\n{scenario}:\")\n",
    "    print(f\"  â†’ Optimize for: {metric}\")\n",
    "    print(f\"  â†’ Why: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary and Key Takeaways <a id='summary'></a>\n",
    "\n",
    "## Regression Metrics Recap\n",
    "\n",
    "| Metric | Formula | When to Use | Key Insight |\n",
    "|--------|---------|-------------|-------------|\n",
    "| **MAE** | Mean of absolute errors | Intuitive interpretation needed | Same units as target, less sensitive to outliers |\n",
    "| **MSE** | Mean of squared errors | Mathematical optimization | Heavily penalizes large errors |\n",
    "| **RMSE** | Square root of MSE | Interpretable error metric | Same units as target, sensitive to outliers |\n",
    "| **RÂ²** | 1 - (SS_res/SS_tot) | Understanding explained variance | Scale-independent, 0 to 1 range |\n",
    "| **Adj. RÂ²** | RÂ² adjusted for features | Comparing models with different features | Prevents overfitting |\n",
    "\n",
    "## Classification Metrics Recap\n",
    "\n",
    "| Metric | Formula | When to Use | Key Insight |\n",
    "|--------|---------|-------------|-------------|\n",
    "| **Accuracy** | (TP+TN)/Total | Balanced datasets | Simple but can be misleading |\n",
    "| **Precision** | TP/(TP+FP) | Minimize false positives | \"Of predictions, how many correct?\" |\n",
    "| **Recall** | TP/(TP+FN) | Minimize false negatives | \"Of actuals, how many found?\" |\n",
    "| **F1-Score** | 2Ã—(PÃ—R)/(P+R) | Balance P and R | Harmonic mean penalizes extremes |\n",
    "| **ROC-AUC** | Area under ROC curve | Threshold-independent | Ranking/discrimination ability |\n",
    "\n",
    "## Key Concepts to Remember\n",
    "\n",
    "### Regression\n",
    "1. **Error Magnitude**: MAE and RMSE tell you *how much* you're wrong\n",
    "2. **Explained Variance**: RÂ² tells you *how well* you explain the data\n",
    "3. **Outlier Sensitivity**: MSE/RMSE are more sensitive than MAE\n",
    "4. **Overfitting**: Use Adjusted RÂ² when adding features\n",
    "\n",
    "### Classification\n",
    "1. **Confusion Matrix**: Foundation of all classification metrics\n",
    "2. **Precision-Recall Trade-off**: You can't always maximize both\n",
    "3. **Context Matters**: Choose metrics based on cost of errors\n",
    "4. **Imbalanced Data**: Accuracy alone is misleading\n",
    "5. **Threshold Selection**: ROC-AUC evaluates all thresholds at once\n",
    "\n",
    "## Final Advice\n",
    "\n",
    "### For Regression:\n",
    "- Start with RMSE and RÂ² for most problems\n",
    "- Use MAE if outliers shouldn't dominate\n",
    "- Always use Adjusted RÂ² when comparing models with different numbers of features\n",
    "- Consider the domain: sometimes business metrics matter more than statistical ones\n",
    "\n",
    "### For Classification:\n",
    "- Never rely on accuracy alone, especially with imbalanced data\n",
    "- Think about the **cost** of false positives vs false negatives\n",
    "- Use F1-score as a general-purpose metric for imbalanced data\n",
    "- Use ROC-AUC when you need to compare models without picking a threshold\n",
    "- Always look at the confusion matrix to understand your errors\n",
    "\n",
    "## Practice Exercises\n",
    "\n",
    "Try these to reinforce your learning:\n",
    "\n",
    "1. **Regression**: Create a dataset with outliers and compare MAE vs RMSE\n",
    "2. **Classification**: Adjust the classification threshold and observe changes in precision/recall\n",
    "3. **Real-world**: Think of 3 applications and determine which metric is most important\n",
    "4. **Imbalanced Data**: Create a 99:1 imbalanced dataset and see why accuracy fails\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- Scikit-learn Documentation: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "- Understanding Precision-Recall: https://developers.google.com/machine-learning/crash-course/classification\n",
    "- Cross-validation techniques for better evaluation\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: Metrics are tools to guide your modeling decisions. Always understand what you're measuring and why it matters for your specific problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section (Try It Yourself!)\n",
    "\n",
    "Now it's your turn! Try modifying the code cells above or create new ones to:\n",
    "\n",
    "1. Change the threshold in the logistic regression model and observe how metrics change\n",
    "2. Create a more imbalanced dataset and see the effect on different metrics\n",
    "3. Add more features to the regression model and compare RÂ² vs Adjusted RÂ²\n",
    "4. Try different types of errors in regression and see which metric captures them best\n",
    "\n",
    "Happy learning! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
