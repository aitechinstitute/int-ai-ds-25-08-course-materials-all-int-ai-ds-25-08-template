{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 7 - Notebook 03: Cross-Validation & Model Selection\n",
    "**Instructor:** Amir Charkhi |  **Goal:** Proper Model Comparison & Selection\n",
    "\n",
    "> Format: theory ‚Üí implementation ‚Üí best practices ‚Üí real-world application.\n",
    "\n",
    "## How Do We Choose the Best Model? üèÜ\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand why single train/test split isn't enough\n",
    "- Master K-fold cross-validation\n",
    "- Learn stratified cross-validation for classification\n",
    "- Interpret validation curves and learning curves\n",
    "- Compare multiple models fairly\n",
    "- Avoid common pitfalls in model selection\n",
    "\n",
    "**Prerequisites:** Notebooks 01 (ML Fundamentals) and 02 (Model Evaluation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î The Problem with Single Train/Test Split\n",
    "\n",
    "**Scenario:** You're comparing two models...\n",
    "\n",
    "```\n",
    "Split 1 (random_state=42):\n",
    "Model A: 85% accuracy\n",
    "Model B: 83% accuracy\n",
    "‚Üí Choose Model A! ‚úÖ\n",
    "\n",
    "Split 2 (random_state=99):\n",
    "Model A: 81% accuracy\n",
    "Model B: 87% accuracy\n",
    "‚Üí Wait... Choose Model B? ü§î\n",
    "```\n",
    "\n",
    "**The Issue:** Performance depends on which data points ended up in test set!\n",
    "\n",
    "**The Solution:** Cross-Validation - test on multiple different splits and average results.\n",
    "\n",
    "This notebook will teach you:\n",
    "- How to evaluate models robustly\n",
    "- Techniques to avoid overfitting\n",
    "- How to select models with confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    learning_curve,\n",
    "    validation_curve\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"üìö Cross-Validation Toolkit:\")\n",
    "print(\"\")\n",
    "print(\"KEY CONCEPTS:\")\n",
    "print(\"  - K-Fold CV: Split data into K parts, train on K-1, test on 1\")\n",
    "print(\"  - Stratified K-Fold: Maintains class proportions in each fold\")\n",
    "print(\"  - Validation Curve: Performance vs hyperparameter values\")\n",
    "print(\"  - Learning Curve: Performance vs training set size\")\n",
    "print(\"\")\n",
    "print(\"‚úÖ All libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Part 1: Demonstrating the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 How Random Seed Affects Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé≤ EXPERIMENT: Impact of Random Seed on Model Performance\\n\")\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Test with different random seeds\n",
    "seeds = range(0, 50, 5)\n",
    "results = []\n",
    "\n",
    "for seed in seeds:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=seed\n",
    "    )\n",
    "    \n",
    "    model = LogisticRegression(max_iter=200)\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "    \n",
    "    results.append({'seed': seed, 'accuracy': accuracy})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"Results across {len(seeds)} different random splits:\")\n",
    "print(f\"  Min accuracy:  {results_df['accuracy'].min():.1%}\")\n",
    "print(f\"  Max accuracy:  {results_df['accuracy'].max():.1%}\")\n",
    "print(f\"  Mean accuracy: {results_df['accuracy'].mean():.1%}\")\n",
    "print(f\"  Std deviation: {results_df['accuracy'].std():.1%}\")\n",
    "print(f\"  Range:         {(results_df['accuracy'].max() - results_df['accuracy'].min())*100:.1f} percentage points!\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(results_df['seed'], results_df['accuracy'], 'o-', linewidth=2, markersize=8, alpha=0.6)\n",
    "plt.axhline(results_df['accuracy'].mean(), color='red', linestyle='--', \n",
    "            linewidth=2, label=f\"Mean: {results_df['accuracy'].mean():.1%}\")\n",
    "plt.fill_between(results_df['seed'], \n",
    "                 results_df['accuracy'].mean() - results_df['accuracy'].std(),\n",
    "                 results_df['accuracy'].mean() + results_df['accuracy'].std(),\n",
    "                 alpha=0.2, color='red', label='¬±1 Std Dev')\n",
    "plt.xlabel('Random Seed', fontsize=12)\n",
    "plt.ylabel('Test Accuracy', fontsize=12)\n",
    "plt.title('Model Performance Varies with Random Split!', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   Same model, same data, but performance varies by up to 10+ percentage points!\")\n",
    "print(\"   ‚Üí Single split gives unreliable estimate of true performance\")\n",
    "print(\"   ‚Üí Solution: Cross-validation! Test on multiple splits and average.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Part 2: K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Understanding K-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ K-FOLD CROSS-VALIDATION EXPLAINED\\n\")\n",
    "\n",
    "print(\"How it works:\")\n",
    "print(\"\")\n",
    "print(\"1. Split data into K equal parts (folds)\")\n",
    "print(\"2. For each fold:\")\n",
    "print(\"   - Use that fold as test set\")\n",
    "print(\"   - Use remaining K-1 folds as training set\")\n",
    "print(\"   - Train model and evaluate\")\n",
    "print(\"3. Average performance across all K folds\")\n",
    "print(\"\")\n",
    "print(\"Example with 5-Fold CV on 100 samples:\")\n",
    "print(\"\")\n",
    "print(\"Fold 1: Train[20-100], Test[0-20]   ‚Üí Accuracy: 85%\")\n",
    "print(\"Fold 2: Train[0-20,40-100], Test[20-40] ‚Üí Accuracy: 88%\")\n",
    "print(\"Fold 3: Train[0-40,60-100], Test[40-60] ‚Üí Accuracy: 82%\")\n",
    "print(\"Fold 4: Train[0-60,80-100], Test[60-80] ‚Üí Accuracy: 87%\")\n",
    "print(\"Fold 5: Train[0-80], Test[80-100]       ‚Üí Accuracy: 84%\")\n",
    "print(\"                                         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(\"                              Mean CV Score: 85.2% ¬± 2.4%\")\n",
    "print(\"\")\n",
    "print(\"Benefits:\")\n",
    "print(\"  ‚úÖ Every sample used for both training and testing\")\n",
    "print(\"  ‚úÖ More reliable performance estimate\")\n",
    "print(\"  ‚úÖ Get mean AND standard deviation\")\n",
    "print(\"  ‚úÖ Reduces impact of lucky/unlucky splits\")\n",
    "print(\"\")\n",
    "print(\"Common K values:\")\n",
    "print(\"  - K=5: Good balance (most common)\")\n",
    "print(\"  - K=10: More robust but slower\")\n",
    "print(\"  - K=n (Leave-One-Out): Maximum robustness but very slow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implementing K-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíª IMPLEMENTING K-FOLD CROSS-VALIDATION\\n\")\n",
    "\n",
    "# Load data\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "print(f\"Dataset: {len(X)} wine samples, {X.shape[1]} features, {len(np.unique(y))} classes\")\n",
    "print(\"\")\n",
    "\n",
    "# Create model\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Method 1: Simple cross_val_score\n",
    "print(\"Method 1: Simple cross_val_score\")\n",
    "print(\"=\"*50)\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"5-Fold CV Scores: {[f'{score:.3f}' for score in cv_scores]}\")\n",
    "print(f\"Mean CV Score: {cv_scores.mean():.3f} (¬±{cv_scores.std():.3f})\")\n",
    "print(\"\")\n",
    "\n",
    "# Method 2: Manual K-Fold with more control\n",
    "print(\"Method 2: Manual K-Fold (more control)\")\n",
    "print(\"=\"*50)\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    score = accuracy_score(y_test, model.predict(X_test))\n",
    "    fold_scores.append(score)\n",
    "    \n",
    "    print(f\"Fold {fold}: Train size={len(train_idx)}, Test size={len(test_idx)}, Accuracy={score:.3f}\")\n",
    "\n",
    "print(f\"\\nMean: {np.mean(fold_scores):.3f} (¬±{np.std(fold_scores):.3f})\")\n",
    "\n",
    "# Visualize fold scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "folds = range(1, len(fold_scores) + 1)\n",
    "plt.bar(folds, fold_scores, alpha=0.6, color='skyblue', edgecolor='black')\n",
    "plt.axhline(np.mean(fold_scores), color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {np.mean(fold_scores):.3f}')\n",
    "plt.xlabel('Fold', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('5-Fold Cross-Validation Results', fontsize=14, fontweight='bold')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "for i, score in enumerate(fold_scores, 1):\n",
    "    plt.text(i, score + 0.02, f'{score:.3f}', ha='center', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Notice: Scores are more consistent now, giving reliable estimate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stratified K-Fold for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ STRATIFIED K-FOLD CROSS-VALIDATION\\n\")\n",
    "\n",
    "print(\"Problem with regular K-Fold for classification:\")\n",
    "print(\"  If you have imbalanced classes, some folds might not have all classes!\")\n",
    "print(\"\")\n",
    "print(\"Solution: Stratified K-Fold\")\n",
    "print(\"  ‚Üí Ensures each fold has same class distribution as original dataset\")\n",
    "print(\"\")\n",
    "\n",
    "# Compare regular vs stratified\n",
    "print(\"Comparison: Regular K-Fold vs Stratified K-Fold\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Regular K-Fold\n",
    "kfold_regular = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores_regular = cross_val_score(model, X, y, cv=kfold_regular, scoring='accuracy')\n",
    "\n",
    "# Stratified K-Fold\n",
    "kfold_stratified = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores_stratified = cross_val_score(model, X, y, cv=kfold_stratified, scoring='accuracy')\n",
    "\n",
    "print(f\"Regular K-Fold:     {scores_regular.mean():.4f} (¬±{scores_regular.std():.4f})\")\n",
    "print(f\"Stratified K-Fold:  {scores_stratified.mean():.4f} (¬±{scores_stratified.std():.4f})\")\n",
    "print(\"\")\n",
    "\n",
    "# Show class distribution in folds\n",
    "print(\"Class distribution in each fold:\\n\")\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold_stratified.split(X, y), 1):\n",
    "    y_test_fold = y[test_idx]\n",
    "    class_counts = pd.Series(y_test_fold).value_counts().sort_index()\n",
    "    class_props = (class_counts / len(y_test_fold) * 100)\n",
    "    print(f\"Fold {fold}: \", end=\"\")\n",
    "    for cls in range(len(np.unique(y))):\n",
    "        print(f\"Class {cls}: {class_props[cls]:.1f}%  \", end=\"\")\n",
    "    print()\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Regular K-Fold\n",
    "axes[0].bar(range(1, 6), scores_regular, alpha=0.6, color='lightcoral', edgecolor='black')\n",
    "axes[0].axhline(scores_regular.mean(), color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Fold', fontsize=11)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0].set_title(f'Regular K-Fold\\n(Mean: {scores_regular.mean():.3f} ¬±{scores_regular.std():.3f})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylim([0.9, 1.0])\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Stratified K-Fold\n",
    "axes[1].bar(range(1, 6), scores_stratified, alpha=0.6, color='skyblue', edgecolor='black')\n",
    "axes[1].axhline(scores_stratified.mean(), color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Fold', fontsize=11)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[1].set_title(f'Stratified K-Fold\\n(Mean: {scores_stratified.mean():.3f} ¬±{scores_stratified.std():.3f})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylim([0.9, 1.0])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Best Practice: Always use StratifiedKFold for classification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ Part 3: Comparing Multiple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model Comparison with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèÜ COMPARING MULTIPLE MODELS WITH CROSS-VALIDATION\\n\")\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=200),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Perform 5-fold CV for each model\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "print(\"Evaluating models with 5-fold cross-validation...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Mean Score': scores.mean(),\n",
    "        'Std Dev': scores.std(),\n",
    "        'Min Score': scores.min(),\n",
    "        'Max Score': scores.max()\n",
    "    })\n",
    "    print(f\"{name:25} ‚Üí {scores.mean():.4f} (¬±{scores.std():.4f})\")\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('Mean Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüìä Summary Table:\\n\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Mean scores with error bars\n",
    "axes[0].barh(results_df['Model'], results_df['Mean Score'], \n",
    "             xerr=results_df['Std Dev'], alpha=0.6, \n",
    "             color='skyblue', edgecolor='black', capsize=5)\n",
    "axes[0].set_xlabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Model Comparison\\n(Mean ¬± Std Dev)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlim([0.85, 1.0])\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Box plot showing distribution\n",
    "all_scores = []\n",
    "labels = []\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    all_scores.append(scores)\n",
    "    labels.append(name)\n",
    "\n",
    "bp = axes[1].boxplot(all_scores, labels=labels, patch_artist=True, vert=False)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('lightcoral')\n",
    "    patch.set_alpha(0.6)\n",
    "axes[1].set_xlabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Score Distribution Across Folds', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlim([0.85, 1.0])\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüèÜ Winner:\", results_df.iloc[0]['Model'])\n",
    "print(f\"   Score: {results_df.iloc[0]['Mean Score']:.4f} (¬±{results_df.iloc[0]['Std Dev']:.4f})\")\n",
    "print(\"\\nüí° Key Insight: Use cross-validation to compare models fairly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà Part 4: Learning Curves & Validation Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Learning Curves - Diagnosing Bias vs Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà LEARNING CURVES - Understanding Model Behavior\\n\")\n",
    "\n",
    "print(\"What are learning curves?\")\n",
    "print(\"  ‚Üí Show model performance vs. training set size\")\n",
    "print(\"  ‚Üí Help diagnose underfitting (bias) vs overfitting (variance)\")\n",
    "print(\"\")\n",
    "print(\"How to interpret:\")\n",
    "print(\"\")\n",
    "print(\"HIGH BIAS (Underfitting):\")\n",
    "print(\"  - Train and validation scores both low\")\n",
    "print(\"  - Scores plateau early\")\n",
    "print(\"  - Small gap between them\")\n",
    "print(\"  ‚Üí Solution: More complex model, more features\")\n",
    "print(\"\")\n",
    "print(\"HIGH VARIANCE (Overfitting):\")\n",
    "print(\"  - Train score high, validation score low\")\n",
    "print(\"  - Large gap between them\")\n",
    "print(\"  - Gap doesn't close with more data\")\n",
    "print(\"  ‚Üí Solution: More data, regularization, simpler model\")\n",
    "print(\"\")\n",
    "print(\"GOOD FIT:\")\n",
    "print(\"  - Both scores high\")\n",
    "print(\"  - Small gap between them\")\n",
    "print(\"  - Converge with more data\")\n",
    "print(\"\")\n",
    "\n",
    "# Generate learning curves for different model complexities\n",
    "models_to_compare = [\n",
    "    ('Underfitting (max_depth=1)', DecisionTreeClassifier(max_depth=1, random_state=42)),\n",
    "    ('Good Fit (max_depth=5)', DecisionTreeClassifier(max_depth=5, random_state=42)),\n",
    "    ('Overfitting (max_depth=20)', DecisionTreeClassifier(max_depth=20, random_state=42))\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for idx, (name, model) in enumerate(models_to_compare):\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y, \n",
    "        cv=5,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='accuracy',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    train_std = train_scores.std(axis=1)\n",
    "    val_mean = val_scores.mean(axis=1)\n",
    "    val_std = val_scores.std(axis=1)\n",
    "    \n",
    "    axes[idx].plot(train_sizes, train_mean, 'o-', color='blue', label='Training score')\n",
    "    axes[idx].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, \n",
    "                           alpha=0.1, color='blue')\n",
    "    axes[idx].plot(train_sizes, val_mean, 'o-', color='red', label='Validation score')\n",
    "    axes[idx].fill_between(train_sizes, val_mean - val_std, val_mean + val_std, \n",
    "                           alpha=0.1, color='red')\n",
    "    \n",
    "    axes[idx].set_xlabel('Training Set Size', fontsize=10)\n",
    "    axes[idx].set_ylabel('Accuracy', fontsize=10)\n",
    "    axes[idx].set_title(name, fontsize=11, fontweight='bold')\n",
    "    axes[idx].legend(loc='lower right', fontsize=9)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_ylim([0.3, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observations:\")\n",
    "print(\"   LEFT: Both scores low & close ‚Üí Underfitting (high bias)\")\n",
    "print(\"   MIDDLE: Both scores high & close ‚Üí Good fit!\")\n",
    "print(\"   RIGHT: Large gap between scores ‚Üí Overfitting (high variance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Validation Curves - Finding Optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä VALIDATION CURVES - Hyperparameter Tuning\\n\")\n",
    "\n",
    "print(\"What are validation curves?\")\n",
    "print(\"  ‚Üí Show model performance vs. hyperparameter values\")\n",
    "print(\"  ‚Üí Help find optimal hyperparameter settings\")\n",
    "print(\"\")\n",
    "print(\"Example: Finding optimal max_depth for Decision Tree\")\n",
    "print(\"\")\n",
    "\n",
    "# Generate validation curve\n",
    "param_range = range(1, 21)\n",
    "train_scores, val_scores = validation_curve(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    X, y,\n",
    "    param_name='max_depth',\n",
    "    param_range=param_range,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "train_std = train_scores.std(axis=1)\n",
    "val_mean = val_scores.mean(axis=1)\n",
    "val_std = val_scores.std(axis=1)\n",
    "\n",
    "# Find optimal value\n",
    "optimal_idx = np.argmax(val_mean)\n",
    "optimal_depth = param_range[optimal_idx]\n",
    "optimal_score = val_mean[optimal_idx]\n",
    "\n",
    "print(f\"Optimal max_depth: {optimal_depth}\")\n",
    "print(f\"Validation score at optimal: {optimal_score:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(param_range, train_mean, 'o-', color='blue', label='Training score', linewidth=2)\n",
    "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, \n",
    "                 alpha=0.1, color='blue')\n",
    "plt.plot(param_range, val_mean, 'o-', color='red', label='Validation score', linewidth=2)\n",
    "plt.fill_between(param_range, val_mean - val_std, val_mean + val_std, \n",
    "                 alpha=0.1, color='red')\n",
    "plt.axvline(optimal_depth, color='green', linestyle='--', linewidth=2, \n",
    "            label=f'Optimal (depth={optimal_depth})')\n",
    "plt.xlabel('max_depth', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Validation Curve: Finding Optimal Tree Depth', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(f\"   - Before depth {optimal_depth}: Underfitting (both scores improving)\")\n",
    "print(f\"   - At depth {optimal_depth}: Sweet spot! (best validation score)\")\n",
    "print(f\"   - After depth {optimal_depth}: Overfitting (gap widens, validation drops)\")\n",
    "print(\"\")\n",
    "print(\"üéØ Use validation curves to:\")\n",
    "print(\"   - Find optimal hyperparameter values\")\n",
    "print(\"   - Detect overfitting/underfitting\")\n",
    "print(\"   - Understand model sensitivity to parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Part 5: Common Pitfalls & Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö†Ô∏è CROSS-VALIDATION PITFALLS & BEST PRACTICES\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚ùå COMMON MISTAKES:\\n\")\n",
    "\n",
    "pitfalls = pd.DataFrame([\n",
    "    ['Not shuffling data before CV', \n",
    "     'Data might be ordered by class',\n",
    "     'Use shuffle=True in KFold'],\n",
    "    \n",
    "    ['Using regular KFold for classification', \n",
    "     'Imbalanced folds',\n",
    "     'Use StratifiedKFold'],\n",
    "    \n",
    "    ['Touching test set during CV', \n",
    "     'Data leakage!',\n",
    "     'CV only on training set'],\n",
    "    \n",
    "    ['Preprocessing before CV split', \n",
    "     'Information leakage from test to train',\n",
    "     'Use Pipeline or ColumnTransformer'],\n",
    "    \n",
    "    ['Using CV to select AND evaluate', \n",
    "     'Overly optimistic results',\n",
    "     'Nested CV or separate test set'],\n",
    "    \n",
    "    ['Too few folds (K=2)', \n",
    "     'High variance in estimates',\n",
    "     'Use K=5 or K=10'],\n",
    "    \n",
    "    ['Too many folds (K=n)', \n",
    "     'Computationally expensive',\n",
    "     'K=5 or K=10 usually sufficient'],\n",
    "    \n",
    "    ['Not setting random_state', \n",
    "     'Results not reproducible',\n",
    "     'Always set random_state']\n",
    "], columns=['Mistake', 'Why Bad', 'Solution'])\n",
    "\n",
    "print(pitfalls.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n‚úÖ BEST PRACTICES:\\n\")\n",
    "\n",
    "print(\"1. üìä Data Splitting Strategy:\")\n",
    "print(\"   Train (60-70%) ‚Üí Validation (10-20%) ‚Üí Test (10-20%)\")\n",
    "print(\"   OR\")\n",
    "print(\"   Train+Validation (80% with CV) ‚Üí Test (20% held out)\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"2. üîÑ Cross-Validation Guidelines:\")\n",
    "print(\"   - Use StratifiedKFold for classification\")\n",
    "print(\"   - Use K=5 or K=10 (good balance)\")\n",
    "print(\"   - Always shuffle=True\")\n",
    "print(\"   - Set random_state for reproducibility\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"3. üéØ Model Selection Workflow:\")\n",
    "print(\"   Step 1: Split ‚Üí Train (80%) + Test (20%)\")\n",
    "print(\"   Step 2: Use CV on training set to:\")\n",
    "print(\"           - Compare models\")\n",
    "print(\"           - Tune hyperparameters\")\n",
    "print(\"   Step 3: Select best model\")\n",
    "print(\"   Step 4: Train on full training set\")\n",
    "print(\"   Step 5: Evaluate ONCE on test set (final score)\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"4. üìà Reporting Results:\")\n",
    "print(\"   ‚úÖ Report mean ¬± std dev from CV\")\n",
    "print(\"   ‚úÖ Show multiple metrics (accuracy, F1, etc.)\")\n",
    "print(\"   ‚úÖ Include confidence intervals\")\n",
    "print(\"   ‚úÖ Report final test set score separately\")\n",
    "print(\"   ‚ùå Don't just report best single score\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"5. üîç When to Use What:\")\n",
    "print(\"   - Small dataset (<1000 samples): K=10 or Leave-One-Out\")\n",
    "print(\"   - Medium dataset (1000-10000): K=5 or K=10\")\n",
    "print(\"   - Large dataset (>10000): K=5 or simple train/val/test split\")\n",
    "print(\"   - Time series: TimeSeriesSplit (not covered today)\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüí° Golden Rule:\")\n",
    "print(\"   Use CV to SELECT models, use test set to EVALUATE final model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Congratulations! You've mastered model evaluation and selection!\")\n",
    "print(\"\")\n",
    "print(\"üìö Week 7 Complete! You learned:\")\n",
    "print(\"\")\n",
    "print(\"   Notebook 01: ‚úÖ ML Fundamentals & Lifecycle\")\n",
    "print(\"   Notebook 02: ‚úÖ Classification & Regression Metrics\")\n",
    "print(\"   Notebook 03: ‚úÖ Cross-Validation & Model Selection\")\n",
    "print(\"\")\n",
    "print(\"üéØ You now have a complete ML evaluation framework!\")\n",
    "print(\"\")\n",
    "print(\"üöÄ Next Week: Linear Models & Tree-Based Methods\")\n",
    "print(\"   Apply everything you learned to real algorithms!\")\n",
    "print(\"\")\n",
    "print(\"üí™ You're ready to build production-quality ML models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
