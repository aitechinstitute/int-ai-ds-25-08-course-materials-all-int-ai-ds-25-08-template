{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 7 - Notebook 02: Model Evaluation - The Complete Guide\n",
    "**Instructor:** Amir Charkhi |  **Goal:** Mastering ML Evaluation Metrics\n",
    "\n",
    "> Format: theory ‚Üí implementation ‚Üí best practices ‚Üí real-world application.\n",
    "\n",
    "## How Do We Know If Our Model Is Good? üéØ\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master classification metrics: accuracy, precision, recall, F1-score, ROC-AUC\n",
    "- Master regression metrics: MSE, RMSE, MAE, R¬≤\n",
    "- Understand when to use which metric (CRITICAL!)\n",
    "- Interpret confusion matrices like a pro\n",
    "- Avoid common evaluation pitfalls\n",
    "- Connect metrics to real business decisions\n",
    "\n",
    "**Prerequisites:** Notebook 01 (ML Fundamentals & Lifecycle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Why Can't We Just Use Accuracy?\n",
    "\n",
    "**Scenario:** You're building a fraud detection model for a bank.\n",
    "\n",
    "Dataset:\n",
    "- 990 legitimate transactions\n",
    "- 10 fraudulent transactions\n",
    "\n",
    "**Model A:**\n",
    "```python\n",
    "def predict(transaction):\n",
    "    return \"legitimate\"  # Always predict legitimate!\n",
    "```\n",
    "\n",
    "**Accuracy of Model A:** 990/1000 = **99%** ü§Ø\n",
    "\n",
    "But this model is USELESS! It never catches fraud!\n",
    "\n",
    "**The lesson:** Different problems need different metrics. Accuracy alone can be dangerously misleading.\n",
    "\n",
    "This notebook will teach you:\n",
    "- Which metrics to use for which problems\n",
    "- How to interpret each metric\n",
    "- How metrics connect to business outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Evaluation metrics - Classification\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Evaluation metrics - Regression\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"üìö Evaluation Metrics Guide:\")\n",
    "print(\"\")\n",
    "print(\"CLASSIFICATION METRICS:\")\n",
    "print(\"  - Accuracy: Overall correctness\")\n",
    "print(\"  - Precision: Of those we predicted positive, how many were right?\")\n",
    "print(\"  - Recall: Of all actual positives, how many did we find?\")\n",
    "print(\"  - F1-Score: Balance between precision and recall\")\n",
    "print(\"  - ROC-AUC: Overall ability to discriminate between classes\")\n",
    "print(\"\")\n",
    "print(\"REGRESSION METRICS:\")\n",
    "print(\"  - MSE: Mean Squared Error\")\n",
    "print(\"  - RMSE: Root Mean Squared Error (same units as target)\")\n",
    "print(\"  - MAE: Mean Absolute Error\")\n",
    "print(\"  - R¬≤: Proportion of variance explained\")\n",
    "print(\"\")\n",
    "print(\"‚úÖ All libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 1: Classification Metrics Deep Dive\n",
    "\n",
    "Let's use a real medical dataset: Breast Cancer Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Setup: Build a Model to Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üè• BREAST CANCER DETECTION - Setup\\n\")\n",
    "\n",
    "# Load data\n",
    "cancer = load_breast_cancer()\n",
    "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "y = pd.Series(cancer.target, name='diagnosis')\n",
    "\n",
    "# 0 = malignant (cancer), 1 = benign (not cancer)\n",
    "# Let's flip it to make it more intuitive: 1 = cancer, 0 = no cancer\n",
    "y = 1 - y\n",
    "\n",
    "print(f\"Dataset: {len(X)} patients\")\n",
    "print(f\"Features: {X.shape[1]} measurements\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Cancer: {(y==1).sum()} patients ({(y==1).sum()/len(y)*100:.1f}%)\")\n",
    "print(f\"  Healthy: {(y==0).sum()} patients ({(y==0).sum()/len(y)*100:.1f}%)\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of cancer\n",
    "\n",
    "print(\"\\n‚úÖ Model trained and predictions made!\")\n",
    "print(\"Now let's evaluate it with multiple metrics...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The Confusion Matrix - Foundation of All Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé≠ THE CONFUSION MATRIX\\n\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract components\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"Understanding the Confusion Matrix:\")\n",
    "print(\"\")\n",
    "print(\"                    PREDICTED\")\n",
    "print(\"               Negative  Positive\")\n",
    "print(\"ACTUAL Negative    TN        FP     (Type I Error)\")\n",
    "print(\"       Positive    FN        TP     (Type II Error)\")\n",
    "print(\"\")\n",
    "print(\"Where:\")\n",
    "print(\"  TN (True Negative):  Correctly predicted no cancer\")\n",
    "print(\"  TP (True Positive):  Correctly predicted cancer\")\n",
    "print(\"  FP (False Positive): Predicted cancer but was healthy (False Alarm)\")\n",
    "print(\"  FN (False Negative): Predicted healthy but had cancer (Missed Case!)\")\n",
    "print(\"\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nOur Model's Confusion Matrix:\")\n",
    "print(f\"  True Negatives (TN):  {tn:3d} - Correctly identified healthy\")\n",
    "print(f\"  True Positives (TP):  {tp:3d} - Correctly identified cancer\")\n",
    "print(f\"  False Positives (FP): {fp:3d} - Healthy but predicted cancer\")\n",
    "print(f\"  False Negatives (FN): {fn:3d} - Cancer but predicted healthy ‚ö†Ô∏è\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Healthy', 'Cancer'])\n",
    "disp.plot(ax=axes[0], cmap='Blues', values_format='d')\n",
    "axes[0].set_title('Confusion Matrix\\n(Numbers)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Normalized confusion matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "disp_norm = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=['Healthy', 'Cancer'])\n",
    "disp_norm.plot(ax=axes[1], cmap='Oranges', values_format='.2%')\n",
    "axes[1].set_title('Confusion Matrix\\n(Percentages)', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(f\"   The model is {cm_normalized[1,1]:.1%} accurate at detecting cancer (Recall)\")\n",
    "print(f\"   When it predicts cancer, it's right {cm_normalized[1,1]/(cm_normalized[0,1]+cm_normalized[1,1]):.1%} of the time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Accuracy - The Most Basic Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä ACCURACY\\n\")\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Definition: (TP + TN) / Total\")\n",
    "print(\"Meaning: What percentage of predictions were correct?\")\n",
    "print(\"\")\n",
    "print(f\"Calculation: ({tp} + {tn}) / {len(y_test)} = {accuracy:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.1%}\")\n",
    "print(\"\")\n",
    "print(\"‚úÖ Advantages:\")\n",
    "print(\"   - Easy to understand and explain\")\n",
    "print(\"   - Good when classes are balanced\")\n",
    "print(\"\")\n",
    "print(\"‚ùå Limitations:\")\n",
    "print(\"   - Misleading with imbalanced classes\")\n",
    "print(\"   - Doesn't distinguish between types of errors\")\n",
    "print(\"   - In medicine: Missing a cancer case is worse than a false alarm!\")\n",
    "print(\"\")\n",
    "print(\"üéØ Use when: Classes are balanced and all errors are equally bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Precision - \"When I predict positive, am I usually right?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ PRECISION\\n\")\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print(\"Definition: TP / (TP + FP)\")\n",
    "print(\"Meaning: Of all patients we predicted have cancer, what % actually have it?\")\n",
    "print(\"\")\n",
    "print(f\"Calculation: {tp} / ({tp} + {fp}) = {precision:.4f}\")\n",
    "print(f\"Precision: {precision:.1%}\")\n",
    "print(\"\")\n",
    "print(\"üí° Interpretation:\")\n",
    "print(f\"   When our model says 'cancer', it's correct {precision:.1%} of the time\")\n",
    "print(f\"   False alarm rate: {(1-precision)*100:.1f}%\")\n",
    "print(\"\")\n",
    "print(\"üéØ Use when: False Positives are costly\")\n",
    "print(\"   Examples:\")\n",
    "print(\"   - Spam detection: Don't want important emails in spam\")\n",
    "print(\"   - Legal cases: Don't want to wrongly convict innocent people\")\n",
    "print(\"   - Marketing: Don't want to waste money on unlikely customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Recall (Sensitivity) - \"Am I finding all the positives?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç RECALL (also called Sensitivity or True Positive Rate)\\n\")\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"Definition: TP / (TP + FN)\")\n",
    "print(\"Meaning: Of all patients who actually have cancer, what % did we catch?\")\n",
    "print(\"\")\n",
    "print(f\"Calculation: {tp} / ({tp} + {fn}) = {recall:.4f}\")\n",
    "print(f\"Recall: {recall:.1%}\")\n",
    "print(\"\")\n",
    "print(\"üí° Interpretation:\")\n",
    "print(f\"   We catch {recall:.1%} of all cancer cases\")\n",
    "print(f\"   Miss rate: {(1-recall)*100:.1f}% of cancer cases go undetected ‚ö†Ô∏è\")\n",
    "print(\"\")\n",
    "print(\"üéØ Use when: False Negatives are costly (missing positives is bad!)\")\n",
    "print(\"   Examples:\")\n",
    "print(\"   - Cancer detection: Can't miss cancer cases!\")\n",
    "print(\"   - Fraud detection: Must catch fraud even if some false alarms\")\n",
    "print(\"   - Security: Better safe than sorry\")\n",
    "print(\"   - COVID testing: Don't want to miss infected people\")\n",
    "print(\"\")\n",
    "print(\"‚öñÔ∏è The Precision-Recall Tradeoff:\")\n",
    "print(\"   - High precision ‚Üí fewer false alarms but might miss cases\")\n",
    "print(\"   - High recall ‚Üí catch more cases but more false alarms\")\n",
    "print(\"   - Can't optimize both perfectly - must choose based on problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 F1-Score - Balancing Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öñÔ∏è F1-SCORE\\n\")\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Definition: 2 √ó (Precision √ó Recall) / (Precision + Recall)\")\n",
    "print(\"Meaning: Harmonic mean of precision and recall\")\n",
    "print(\"\")\n",
    "print(f\"Calculation: 2 √ó ({precision:.3f} √ó {recall:.3f}) / ({precision:.3f} + {recall:.3f})\")\n",
    "print(f\"F1-Score: {f1:.4f} ({f1:.1%})\")\n",
    "print(\"\")\n",
    "print(\"üí° Why harmonic mean?\")\n",
    "print(\"   - Penalizes extreme imbalance\")\n",
    "print(\"   - If precision is 90% but recall is 10%, F1 is only 18%\")\n",
    "print(\"   - Forces you to care about both metrics\")\n",
    "print(\"\")\n",
    "print(\"üéØ Use when:\")\n",
    "print(\"   - You need a single metric but care about precision AND recall\")\n",
    "print(\"   - Classes are imbalanced\")\n",
    "print(\"   - You can't decide which error type is worse\")\n",
    "print(\"\")\n",
    "\n",
    "# Visualize all metrics together\n",
    "metrics = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(metrics.keys(), metrics.values(), \n",
    "               color=['skyblue', 'lightcoral', 'lightgreen', 'gold'],\n",
    "               alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('Classification Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, (name, value) in zip(bars, metrics.items()):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "            f'{value:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìã Full Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Healthy', 'Cancer']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 ROC Curve & AUC - The Big Picture Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà ROC CURVE & AUC SCORE\\n\")\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"ROC = Receiver Operating Characteristic\")\n",
    "print(\"AUC = Area Under the Curve\")\n",
    "print(\"\")\n",
    "print(\"What it shows:\")\n",
    "print(\"  - Trade-off between True Positive Rate (Recall) and False Positive Rate\")\n",
    "print(\"  - Model's ability to discriminate between classes across all thresholds\")\n",
    "print(\"\")\n",
    "print(f\"Our model's ROC-AUC: {roc_auc:.4f}\")\n",
    "print(\"\")\n",
    "print(\"Interpretation of AUC:\")\n",
    "print(\"  1.0 = Perfect classifier\")\n",
    "print(\"  0.9-1.0 = Excellent\")\n",
    "print(\"  0.8-0.9 = Very Good\")\n",
    "print(\"  0.7-0.8 = Good\")\n",
    "print(\"  0.6-0.7 = Fair\")\n",
    "print(\"  0.5 = Random guessing (coin flip)\")\n",
    "print(\"  < 0.5 = Worse than random (something's wrong!)\")\n",
    "print(\"\")\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "         label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "         label='Random Classifier (AUC = 0.50)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "plt.title('ROC Curve - Cancer Detection Model', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key Insights:\")\n",
    "print(\"   - Closer the curve to top-left = better model\")\n",
    "print(\"   - AUC = probability model ranks random positive higher than random negative\")\n",
    "print(f\"   - Our AUC of {roc_auc:.2f} means: {roc_auc:.0%} chance model correctly ranks cases\")\n",
    "print(\"\")\n",
    "print(\"üéØ Use when:\")\n",
    "print(\"   - Comparing models at a glance\")\n",
    "print(\"   - You don't care about a specific threshold\")\n",
    "print(\"   - Binary classification problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìâ Part 2: Regression Metrics Deep Dive\n",
    "\n",
    "Now let's look at metrics for predicting continuous values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setup: Build a Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üè• DIABETES PROGRESSION PREDICTION - Setup\\n\")\n",
    "\n",
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X_reg = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y_reg = pd.Series(diabetes.target, name='progression')\n",
    "\n",
    "print(f\"Dataset: {len(X_reg)} patients\")\n",
    "print(f\"Features: {X_reg.shape[1]} measurements\")\n",
    "print(f\"Target: Disease progression (quantitative measure)\")\n",
    "print(f\"  Range: {y_reg.min():.1f} to {y_reg.max():.1f}\")\n",
    "print(f\"  Mean: {y_reg.mean():.1f}\")\n",
    "print(f\"  Std: {y_reg.std():.1f}\")\n",
    "\n",
    "# Split data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "reg_model = LinearRegression()\n",
    "reg_model.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_reg = reg_model.predict(X_train_reg)\n",
    "y_test_pred_reg = reg_model.predict(X_test_reg)\n",
    "\n",
    "print(\"\\n‚úÖ Regression model trained!\")\n",
    "\n",
    "# Visualize predictions vs actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train_reg, y_train_pred_reg, alpha=0.5, s=30)\n",
    "axes[0].plot([y_train_reg.min(), y_train_reg.max()], \n",
    "             [y_train_reg.min(), y_train_reg.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Progression', fontsize=11)\n",
    "axes[0].set_ylabel('Predicted Progression', fontsize=11)\n",
    "axes[0].set_title('Training Set: Predictions vs Actual', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test set\n",
    "axes[1].scatter(y_test_reg, y_test_pred_reg, alpha=0.5, s=30, color='coral')\n",
    "axes[1].plot([y_test_reg.min(), y_test_reg.max()], \n",
    "             [y_test_reg.min(), y_test_reg.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Progression', fontsize=11)\n",
    "axes[1].set_ylabel('Predicted Progression', fontsize=11)\n",
    "axes[1].set_title('Test Set: Predictions vs Actual', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Closer to the red line = better predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìè MEAN ABSOLUTE ERROR (MAE)\\n\")\n",
    "\n",
    "mae_train = mean_absolute_error(y_train_reg, y_train_pred_reg)\n",
    "mae_test = mean_absolute_error(y_test_reg, y_test_pred_reg)\n",
    "\n",
    "print(\"Definition: Average of absolute differences between predictions and actual\")\n",
    "print(\"Formula: (1/n) √ó Œ£|actual - predicted|\")\n",
    "print(\"\")\n",
    "print(f\"Training MAE: {mae_train:.2f}\")\n",
    "print(f\"Test MAE: {mae_test:.2f}\")\n",
    "print(\"\")\n",
    "print(\"üí° Interpretation:\")\n",
    "print(f\"   On average, predictions are off by {mae_test:.1f} units\")\n",
    "print(f\"   As % of target range: {mae_test/(y_test_reg.max()-y_test_reg.min())*100:.1f}%\")\n",
    "print(\"\")\n",
    "print(\"‚úÖ Advantages:\")\n",
    "print(\"   - Easy to interpret (same units as target)\")\n",
    "print(\"   - Not sensitive to outliers\")\n",
    "print(\"   - All errors weighted equally\")\n",
    "print(\"\")\n",
    "print(\"‚ùå Limitations:\")\n",
    "print(\"   - Doesn't penalize large errors more than small ones\")\n",
    "print(\"\")\n",
    "print(\"üéØ Use when: You want intuitive error metric and all errors are equally bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Mean Squared Error (MSE) & Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìê MEAN SQUARED ERROR (MSE) & ROOT MEAN SQUARED ERROR (RMSE)\\n\")\n",
    "\n",
    "mse_train = mean_squared_error(y_train_reg, y_train_pred_reg)\n",
    "mse_test = mean_squared_error(y_test_reg, y_test_pred_reg)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "print(\"MSE Definition: Average of squared differences\")\n",
    "print(\"Formula: (1/n) √ó Œ£(actual - predicted)¬≤\")\n",
    "print(\"\")\n",
    "print(\"RMSE Definition: Square root of MSE\")\n",
    "print(\"Formula: ‚àöMSE\")\n",
    "print(\"\")\n",
    "print(\"Scores:\")\n",
    "print(f\"  Training MSE:  {mse_train:.2f}\")\n",
    "print(f\"  Test MSE:      {mse_test:.2f}\")\n",
    "print(f\"  Training RMSE: {rmse_train:.2f}\")\n",
    "print(f\"  Test RMSE:     {rmse_test:.2f}\")\n",
    "print(\"\")\n",
    "print(\"üí° Key Differences from MAE:\")\n",
    "print(f\"   MAE:  {mae_test:.2f} (average absolute error)\")\n",
    "print(f\"   RMSE: {rmse_test:.2f} (penalizes large errors more)\")\n",
    "print(f\"   RMSE > MAE because large errors are penalized more\")\n",
    "print(\"\")\n",
    "print(\"‚úÖ Advantages:\")\n",
    "print(\"   - Penalizes large errors heavily (good for critical applications)\")\n",
    "print(\"   - RMSE in same units as target (interpretable)\")\n",
    "print(\"   - Mathematically convenient (differentiable)\")\n",
    "print(\"\")\n",
    "print(\"‚ùå Limitations:\")\n",
    "print(\"   - Sensitive to outliers\")\n",
    "print(\"   - MSE not in same units (squared)\")\n",
    "print(\"\")\n",
    "print(\"üéØ Use when:\")\n",
    "print(\"   - Large errors are much worse than small errors\")\n",
    "print(\"   - Example: House price prediction (being off by $100k worse than off by $10k)\")\n",
    "\n",
    "# Visualize comparison\n",
    "errors = np.abs(y_test_reg - y_test_pred_reg)\n",
    "squared_errors = (y_test_reg - y_test_pred_reg) ** 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Error distribution\n",
    "axes[0].hist(errors, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0].axvline(mae_test, color='red', linestyle='--', linewidth=2, label=f'MAE = {mae_test:.1f}')\n",
    "axes[0].axvline(rmse_test, color='orange', linestyle='--', linewidth=2, label=f'RMSE = {rmse_test:.1f}')\n",
    "axes[0].set_xlabel('Absolute Error', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0].set_title('Distribution of Prediction Errors', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Comparison of errors\n",
    "sample_errors = np.array([5, 10, 20, 50])\n",
    "sample_mae = sample_errors\n",
    "sample_mse = sample_errors ** 2\n",
    "x = np.arange(len(sample_errors))\n",
    "width = 0.35\n",
    "axes[1].bar(x - width/2, sample_mae, width, label='Absolute Error', alpha=0.7, color='skyblue')\n",
    "axes[1].bar(x + width/2, sample_mse, width, label='Squared Error', alpha=0.7, color='coral')\n",
    "axes[1].set_xlabel('Error Magnitude', fontsize=11)\n",
    "axes[1].set_ylabel('Penalized Value', fontsize=11)\n",
    "axes[1].set_title('How MAE vs MSE Penalizes Errors', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(['5', '10', '20', '50'])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Notice: Squared error grows much faster for large errors!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 R¬≤ Score (Coefficient of Determination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä R¬≤ SCORE (R-SQUARED)\\n\")\n",
    "\n",
    "r2_train = r2_score(y_train_reg, y_train_pred_reg)\n",
    "r2_test = r2_score(y_test_reg, y_test_pred_reg)\n",
    "\n",
    "print(\"Definition: Proportion of variance in target explained by features\")\n",
    "print(\"Formula: 1 - (SS_residual / SS_total)\")\n",
    "print(\"\")\n",
    "print(f\"Training R¬≤: {r2_train:.4f}\")\n",
    "print(f\"Test R¬≤: {r2_test:.4f}\")\n",
    "print(\"\")\n",
    "print(\"üí° Interpretation:\")\n",
    "print(f\"   Model explains {r2_test*100:.1f}% of variance in disease progression\")\n",
    "print(f\"   {(1-r2_test)*100:.1f}% of variance is unexplained (other factors)\")\n",
    "print(\"\")\n",
    "print(\"Understanding R¬≤ values:\")\n",
    "print(\"  1.0 = Perfect predictions (explains 100% of variance)\")\n",
    "print(\"  0.9-1.0 = Excellent\")\n",
    "print(\"  0.7-0.9 = Good\")\n",
    "print(\"  0.5-0.7 = Moderate\")\n",
    "print(\"  0.3-0.5 = Weak\")\n",
    "print(\"  < 0.3 = Very weak\")\n",
    "print(\"  0.0 = No better than predicting the mean\")\n",
    "print(\"  < 0.0 = Worse than predicting the mean! (something's wrong)\")\n",
    "print(\"\")\n",
    "print(\"‚úÖ Advantages:\")\n",
    "print(\"   - Scale-independent (always between -‚àû and 1)\")\n",
    "print(\"   - Easy to interpret as percentage\")\n",
    "print(\"   - Shows model's explanatory power\")\n",
    "print(\"\")\n",
    "print(\"‚ùå Limitations:\")\n",
    "print(\"   - Can be misleading with non-linear relationships\")\n",
    "print(\"   - Always increases with more features (use adjusted R¬≤ for this)\")\n",
    "print(\"\")\n",
    "print(\"üéØ Use when:\")\n",
    "print(\"   - You want to understand model's explanatory power\")\n",
    "print(\"   - Comparing models with same target variable\")\n",
    "print(\"   - Communicating to non-technical stakeholders\")\n",
    "\n",
    "# Visualize R¬≤ concept\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Model with different R¬≤ values\n",
    "scenarios = [\n",
    "    ('Poor Model (R¬≤‚âà0.1)', y_test_reg, y_test_reg.mean() + np.random.normal(0, 50, len(y_test_reg))),\n",
    "    ('Our Model (R¬≤={:.2f})'.format(r2_test), y_test_reg, y_test_pred_reg),\n",
    "    ('Perfect Model (R¬≤=1.0)', y_test_reg, y_test_reg)\n",
    "]\n",
    "\n",
    "for i, (title, actual, predicted) in enumerate(scenarios):\n",
    "    axes[i].scatter(actual, predicted, alpha=0.5, s=30)\n",
    "    axes[i].plot([actual.min(), actual.max()], \n",
    "                 [actual.min(), actual.max()], \n",
    "                 'r--', lw=2)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    axes[i].set_xlabel('Actual', fontsize=10)\n",
    "    axes[i].set_ylabel('Predicted', fontsize=10)\n",
    "    axes[i].set_title(f'{title}', fontsize=11, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].text(0.05, 0.95, f'R¬≤ = {r2:.2f}', \n",
    "                transform=axes[i].transAxes, \n",
    "                fontsize=12, fontweight='bold',\n",
    "                verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Regression Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä REGRESSION METRICS - COMPLETE SUMMARY\\n\")\n",
    "\n",
    "# Calculate all metrics\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'Metric': ['MAE', 'MSE', 'RMSE', 'R¬≤'],\n",
    "    'Training': [\n",
    "        f\"{mae_train:.2f}\",\n",
    "        f\"{mse_train:.2f}\",\n",
    "        f\"{rmse_train:.2f}\",\n",
    "        f\"{r2_train:.4f}\"\n",
    "    ],\n",
    "    'Test': [\n",
    "        f\"{mae_test:.2f}\",\n",
    "        f\"{mse_test:.2f}\",\n",
    "        f\"{rmse_test:.2f}\",\n",
    "        f\"{r2_test:.4f}\"\n",
    "    ],\n",
    "    'Interpretation': [\n",
    "        'Average absolute error',\n",
    "        'Average squared error',\n",
    "        'Average error (same units)',\n",
    "        'Variance explained (0-1)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(metrics_summary.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Error metrics\n",
    "error_metrics = ['MAE', 'RMSE']\n",
    "train_errors = [mae_train, rmse_train]\n",
    "test_errors = [mae_test, rmse_test]\n",
    "x = np.arange(len(error_metrics))\n",
    "width = 0.35\n",
    "axes[0].bar(x - width/2, train_errors, width, label='Training', alpha=0.7, color='skyblue')\n",
    "axes[0].bar(x + width/2, test_errors, width, label='Test', alpha=0.7, color='coral')\n",
    "axes[0].set_ylabel('Error', fontsize=11)\n",
    "axes[0].set_title('Error Metrics Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(error_metrics)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# R¬≤ score\n",
    "r2_data = [r2_train, r2_test]\n",
    "colors = ['skyblue', 'coral']\n",
    "bars = axes[1].bar(['Training', 'Test'], r2_data, color=colors, alpha=0.7, width=0.5)\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].set_ylabel('R¬≤ Score', fontsize=11)\n",
    "axes[1].set_title('R¬≤ Score Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for bar, val in zip(bars, r2_data):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{val:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "if abs(r2_train - r2_test) < 0.1:\n",
    "    print(\"   ‚úÖ Training and test R¬≤ are similar - good generalization!\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Gap between training and test R¬≤ - possible overfitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Part 3: Which Metric Should You Use?\n",
    "\n",
    "The ultimate decision framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß≠ METRIC SELECTION GUIDE\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìã CLASSIFICATION PROBLEMS:\\n\")\n",
    "\n",
    "classification_guide = pd.DataFrame([\n",
    "    ['Balanced classes, all errors equal', 'Accuracy'],\n",
    "    ['Must minimize false alarms', 'Precision'],\n",
    "    ['Cannot miss positive cases (life/death)', 'Recall'],\n",
    "    ['Need balance, imbalanced classes', 'F1-Score'],\n",
    "    ['Comparing models, threshold-independent', 'ROC-AUC'],\n",
    "    ['Medical diagnosis', 'Recall + F1'],\n",
    "    ['Spam detection', 'Precision'],\n",
    "    ['Fraud detection', 'Recall + ROC-AUC'],\n",
    "    ['Marketing (predict buyers)', 'Precision + F1']\n",
    "], columns=['Situation', 'Best Metric(s)'])\n",
    "\n",
    "print(classification_guide.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüìà REGRESSION PROBLEMS:\\n\")\n",
    "\n",
    "regression_guide = pd.DataFrame([\n",
    "    ['Simple interpretation needed', 'MAE or RMSE'],\n",
    "    ['All errors equally bad', 'MAE'],\n",
    "    ['Large errors much worse', 'MSE or RMSE'],\n",
    "    ['Sensitive to outliers', 'MAE (robust)'],\n",
    "    ['Model comparison (same target)', 'R¬≤'],\n",
    "    ['Explaining model to stakeholders', 'R¬≤ + RMSE'],\n",
    "    ['House price prediction', 'RMSE (in $)'],\n",
    "    ['Stock price prediction', 'MAE + RMSE'],\n",
    "    ['Weather forecasting', 'MAE']\n",
    "], columns=['Situation', 'Best Metric(s)'])\n",
    "\n",
    "print(regression_guide.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n‚ö†Ô∏è COMMON MISTAKES TO AVOID:\\n\")\n",
    "print(\"1. ‚ùå Using only accuracy for imbalanced classes\")\n",
    "print(\"2. ‚ùå Not considering the business cost of errors\")\n",
    "print(\"3. ‚ùå Optimizing for a metric that doesn't match your goal\")\n",
    "print(\"4. ‚ùå Looking at training metrics instead of test metrics\")\n",
    "print(\"5. ‚ùå Using R¬≤ when prediction accuracy matters more\")\n",
    "print(\"\")\n",
    "print(\"üéØ GOLDEN RULE: Choose metrics based on business impact, not convenience!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "You now understand model evaluation inside and out! Here's your framework:\n",
    "\n",
    "### For Classification:\n",
    "1. **Confusion Matrix** - Foundation of all metrics\n",
    "2. **Accuracy** - Overall correctness (use with balanced classes)\n",
    "3. **Precision** - How reliable are positive predictions?\n",
    "4. **Recall** - Are we catching all positives?\n",
    "5. **F1-Score** - Balanced metric for imbalanced classes\n",
    "6. **ROC-AUC** - Overall discrimination ability\n",
    "\n",
    "### For Regression:\n",
    "1. **MAE** - Average absolute error (intuitive, robust)\n",
    "2. **MSE** - Average squared error (penalizes large errors)\n",
    "3. **RMSE** - Square root of MSE (same units as target)\n",
    "4. **R¬≤** - Variance explained (0-1 scale)\n",
    "\n",
    "### Critical Principles:\n",
    "- ‚úÖ **Always use multiple metrics** - no single metric tells the whole story\n",
    "- ‚úÖ **Match metrics to business goals** - what error is most costly?\n",
    "- ‚úÖ **Test set metrics** - training metrics can be misleading\n",
    "- ‚úÖ **Consider class imbalance** - accuracy can be deceptive\n",
    "- ‚úÖ **Understand tradeoffs** - precision vs recall, MAE vs RMSE\n",
    "\n",
    "### Decision Framework:\n",
    "```\n",
    "1. What's my problem type? (Classification / Regression)\n",
    "2. Are classes balanced? (If no ‚Üí don't use accuracy alone)\n",
    "3. What's more costly? (False positive / False negative)\n",
    "4. What do stakeholders care about? (Interpretability / Performance)\n",
    "5. Choose 2-3 metrics that capture these priorities\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "Now you know HOW to evaluate models. Next:\n",
    "\n",
    "**Notebook 03**: Cross-validation and proper model selection\n",
    "- Why single train/test split isn't enough\n",
    "- K-fold cross-validation\n",
    "- Comparing models fairly\n",
    "- Avoiding overfitting\n",
    "\n",
    "Then Weeks 8-12: Apply these evaluation techniques to different algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Congratulations! You've mastered model evaluation!\")\n",
    "print(\"\")\n",
    "print(\"üìö You learned:\")\n",
    "print(\"   ‚úÖ All major classification metrics\")\n",
    "print(\"   ‚úÖ All major regression metrics\")\n",
    "print(\"   ‚úÖ When to use which metric\")\n",
    "print(\"   ‚úÖ How to interpret confusion matrices\")\n",
    "print(\"   ‚úÖ Real-world metric selection\")\n",
    "print(\"\")\n",
    "print(\"üéØ Next: Notebook 03 - Cross-Validation & Model Selection\")\n",
    "print(\"   Learn how to properly compare and select models!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
