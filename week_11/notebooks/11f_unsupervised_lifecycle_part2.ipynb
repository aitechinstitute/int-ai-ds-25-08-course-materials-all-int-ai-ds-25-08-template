{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** Â· *Intermediate AI & Data Science*\n",
    "### Unsupervised Learning Lifecycle - Part 2\n",
    "**Instructor:** Amir Charkhi | **Focus:** Clustering â†’ Tuning â†’ Evaluation â†’ Insights\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Where We Are in the Lifecycle\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    UNSUPERVISED LEARNING LIFECYCLE                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  PART 1 (Completed):                                               â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
    "â”‚  â”‚ Raw Data â”‚ â†’  â”‚ Preprocessing â”‚ â†’  â”‚ Dim Reduction (PCA) â”‚  âœ“   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  PART 2 (This Notebook):                                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚ Clustering â”‚ â†’  â”‚ HP Tuning  â”‚ â†’  â”‚ Business Insights â”‚  â†      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š What You'll Learn in Part 2\n",
    "\n",
    "- Compare multiple clustering algorithms\n",
    "- Systematic hyperparameter tuning\n",
    "- Evaluate clustering quality\n",
    "- Select the best model\n",
    "- Extract actionable business insights\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Prepared Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared data from Part 1 (or recreate if not available)\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y_true = wine.target\n",
    "feature_names = wine.feature_names\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca_cluster = PCA(n_components=0.90)\n",
    "X_pca = pca_cluster.fit_transform(X_scaled)\n",
    "\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Data ready: {X_scaled.shape[0]} samples, {X_scaled.shape[1]} original features, {X_pca.shape[1]} PCA features\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. The Clustering Workflow\n",
    "\n",
    "### ğŸ”„ Systematic Approach\n",
    "\n",
    "```\n",
    "Step 1: Try multiple algorithms\n",
    "        K-Means, Hierarchical, DBSCAN, GMM\n",
    "            â†“\n",
    "Step 2: For each algorithm, tune hyperparameters\n",
    "        K-Means: n_clusters = [2, 3, 4, 5, ...]\n",
    "        DBSCAN: eps, min_samples\n",
    "            â†“\n",
    "Step 3: Evaluate each configuration\n",
    "        Silhouette, Calinski-Harabasz, Davies-Bouldin\n",
    "            â†“\n",
    "Step 4: Select best model\n",
    "        Balance metrics + domain knowledge\n",
    "            â†“\n",
    "Step 5: Interpret and apply\n",
    "        Business insights, action items\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics (No Labels!)\n",
    "\n",
    "### ğŸ“ Internal Metrics (What We Use)\n",
    "\n",
    "```\n",
    "We have NO labels, so we use INTERNAL metrics:\n",
    "\n",
    "1. Silhouette Score [-1, 1]\n",
    "   \"How similar is each point to its own cluster vs others?\"\n",
    "   Higher = Better\n",
    "\n",
    "2. Calinski-Harabasz Score [0, âˆ]\n",
    "   \"Ratio of between-cluster to within-cluster variance\"\n",
    "   Higher = Better\n",
    "\n",
    "3. Davies-Bouldin Score [0, âˆ]\n",
    "   \"Average similarity between clusters\"\n",
    "   Lower = Better\n",
    "```\n",
    "\n",
    "### ğŸ¯ External Metrics (For Validation Only)\n",
    "\n",
    "```\n",
    "If we happen to have true labels (like our wine data):\n",
    "\n",
    "1. Adjusted Rand Index [-1, 1]\n",
    "   \"Agreement between predicted and true clusters\"\n",
    "\n",
    "2. Normalized Mutual Information [0, 1]\n",
    "   \"Information shared between predicted and true\"\n",
    "\n",
    "Note: In real unsupervised learning, we don't have these!\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(X, labels, y_true=None):\n",
    "    \"\"\"Calculate clustering evaluation metrics\"\"\"\n",
    "    results = {\n",
    "        'Silhouette': silhouette_score(X, labels),\n",
    "        'Calinski-Harabasz': calinski_harabasz_score(X, labels),\n",
    "        'Davies-Bouldin': davies_bouldin_score(X, labels)\n",
    "    }\n",
    "    \n",
    "    # External metrics (if true labels available)\n",
    "    if y_true is not None:\n",
    "        results['ARI'] = adjusted_rand_score(y_true, labels)\n",
    "        results['NMI'] = normalized_mutual_info_score(y_true, labels)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Method 1: K-Means with Hyperparameter Tuning\n",
    "\n",
    "### ğŸ¯ The Key Question: How Many Clusters?\n",
    "\n",
    "**Approach:** Try multiple values of K, evaluate each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test K from 2 to 8\n",
    "k_range = range(2, 9)\n",
    "kmeans_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_pca)\n",
    "    \n",
    "    metrics = evaluate_clustering(X_pca, labels, y_true)\n",
    "    metrics['K'] = k\n",
    "    metrics['Inertia'] = kmeans.inertia_\n",
    "    kmeans_results.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans = pd.DataFrame(kmeans_results)\n",
    "df_kmeans.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Visualize K Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=2, cols=2,\n",
    "    subplot_titles=('Elbow Method (Inertia)', 'Silhouette Score', \n",
    "                    'Calinski-Harabasz', 'Davies-Bouldin'))\n",
    "\n",
    "# Elbow\n",
    "fig.add_trace(go.Scatter(x=list(k_range), y=df_kmeans['Inertia'],\n",
    "    mode='lines+markers', marker=dict(size=10)), row=1, col=1)\n",
    "\n",
    "# Silhouette\n",
    "fig.add_trace(go.Scatter(x=list(k_range), y=df_kmeans['Silhouette'],\n",
    "    mode='lines+markers', marker=dict(size=10, color='green')), row=1, col=2)\n",
    "\n",
    "# Calinski-Harabasz\n",
    "fig.add_trace(go.Scatter(x=list(k_range), y=df_kmeans['Calinski-Harabasz'],\n",
    "    mode='lines+markers', marker=dict(size=10, color='orange')), row=2, col=1)\n",
    "\n",
    "# Davies-Bouldin\n",
    "fig.add_trace(go.Scatter(x=list(k_range), y=df_kmeans['Davies-Bouldin'],\n",
    "    mode='lines+markers', marker=dict(size=10, color='red')), row=2, col=2)\n",
    "\n",
    "fig.update_layout(title='K-Means: Hyperparameter Tuning (Finding Optimal K)',\n",
    "    height=600, width=1000, showlegend=False, template='plotly_white')\n",
    "fig.update_xaxes(title_text='K', dtick=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¡ **Analysis:**\n",
    "- Elbow: Bend at K=3\n",
    "- Silhouette: Peak at K=3\n",
    "- Calinski-Harabasz: Peak at K=3\n",
    "- Davies-Bouldin: Low at K=3\n",
    "\n",
    "**Optimal K = 3** (all metrics agree!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final K-Means model\n",
    "best_kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels_kmeans = best_kmeans.fit_predict(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Method 2: Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different linkage methods\n",
    "linkages = ['ward', 'complete', 'average']\n",
    "hier_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linkage in linkages:\n",
    "    for n in range(2, 6):\n",
    "        hier = AgglomerativeClustering(n_clusters=n, linkage=linkage)\n",
    "        labels = hier.fit_predict(X_pca)\n",
    "        \n",
    "        metrics = evaluate_clustering(X_pca, labels, y_true)\n",
    "        metrics['Linkage'] = linkage\n",
    "        metrics['N_Clusters'] = n\n",
    "        hier_results.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hier = pd.DataFrame(hier_results)\n",
    "df_hier.sort_values('Silhouette', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hierarchical model\n",
    "best_hier = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "labels_hier = best_hier.fit_predict(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Method 3: DBSCAN (Density-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different eps and min_samples combinations\n",
    "eps_values = [0.5, 1.0, 1.5, 2.0, 2.5]\n",
    "min_samples_values = [3, 5, 7]\n",
    "dbscan_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(X_pca)\n",
    "        \n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        \n",
    "        if n_clusters >= 2:  # Need at least 2 clusters for metrics\n",
    "            mask = labels != -1\n",
    "            metrics = evaluate_clustering(X_pca[mask], labels[mask], y_true[mask])\n",
    "        else:\n",
    "            metrics = {'Silhouette': np.nan, 'Calinski-Harabasz': np.nan, \n",
    "                      'Davies-Bouldin': np.nan, 'ARI': np.nan, 'NMI': np.nan}\n",
    "        \n",
    "        metrics['eps'] = eps\n",
    "        metrics['min_samples'] = min_samples\n",
    "        metrics['N_Clusters'] = n_clusters\n",
    "        metrics['N_Noise'] = n_noise\n",
    "        dbscan_results.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbscan = pd.DataFrame(dbscan_results)\n",
    "df_dbscan[df_dbscan['N_Clusters'] >= 2].sort_values('Silhouette', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best DBSCAN model\n",
    "best_dbscan = DBSCAN(eps=2.0, min_samples=5)\n",
    "labels_dbscan = best_dbscan.fit_predict(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Method 4: Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different numbers of components\n",
    "gmm_results = []\n",
    "\n",
    "for n in range(2, 7):\n",
    "    gmm = GaussianMixture(n_components=n, random_state=42, n_init=5)\n",
    "    labels = gmm.fit_predict(X_pca)\n",
    "    \n",
    "    metrics = evaluate_clustering(X_pca, labels, y_true)\n",
    "    metrics['N_Components'] = n\n",
    "    metrics['BIC'] = gmm.bic(X_pca)  # Lower is better\n",
    "    metrics['AIC'] = gmm.aic(X_pca)  # Lower is better\n",
    "    gmm_results.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gmm = pd.DataFrame(gmm_results)\n",
    "df_gmm.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best GMM model\n",
    "best_gmm = GaussianMixture(n_components=3, random_state=42, n_init=5)\n",
    "labels_gmm = best_gmm.fit_predict(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Compare All Methods\n",
    "\n",
    "### ğŸ“Š Side-by-Side Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plot\n",
    "fig = make_subplots(rows=2, cols=2,\n",
    "    subplot_titles=('K-Means (K=3)', 'Hierarchical (Ward)', \n",
    "                    'DBSCAN', 'Gaussian Mixture'))\n",
    "\n",
    "all_labels = [labels_kmeans, labels_hier, labels_dbscan, labels_gmm]\n",
    "positions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "\n",
    "for labels, (row, col) in zip(all_labels, positions):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_pca_2d[:, 0], y=X_pca_2d[:, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(color=labels, colorscale='viridis', size=8, opacity=0.7),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "fig.update_layout(title='Clustering Methods Comparison',\n",
    "    height=700, width=900, template='plotly_white')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ˆ Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect best results from each method\n",
    "comparison = [\n",
    "    {'Method': 'K-Means', **evaluate_clustering(X_pca, labels_kmeans, y_true)},\n",
    "    {'Method': 'Hierarchical', **evaluate_clustering(X_pca, labels_hier, y_true)},\n",
    "    {'Method': 'GMM', **evaluate_clustering(X_pca, labels_gmm, y_true)}\n",
    "]\n",
    "\n",
    "# Add DBSCAN (excluding noise points for fair comparison)\n",
    "mask = labels_dbscan != -1\n",
    "if mask.sum() > 10:\n",
    "    comparison.append({\n",
    "        'Method': 'DBSCAN',\n",
    "        **evaluate_clustering(X_pca[mask], labels_dbscan[mask], y_true[mask])\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison = pd.DataFrame(comparison)\n",
    "df_comparison.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "metrics_to_plot = ['Silhouette', 'ARI', 'NMI']\n",
    "colors = ['steelblue', 'green', 'orange']\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=metric,\n",
    "        x=df_comparison['Method'],\n",
    "        y=df_comparison[metric],\n",
    "        marker_color=colors[i]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Clustering Performance Comparison (Higher is Better)',\n",
    "    xaxis_title='Method',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    template='plotly_white',\n",
    "    height=500, width=800\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Select Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best method by average rank\n",
    "df_ranks = df_comparison.copy()\n",
    "df_ranks['Silhouette_Rank'] = df_ranks['Silhouette'].rank(ascending=False)\n",
    "df_ranks['ARI_Rank'] = df_ranks['ARI'].rank(ascending=False)\n",
    "df_ranks['NMI_Rank'] = df_ranks['NMI'].rank(ascending=False)\n",
    "df_ranks['DB_Rank'] = df_ranks['Davies-Bouldin'].rank(ascending=True)  # Lower is better\n",
    "\n",
    "df_ranks['Avg_Rank'] = df_ranks[['Silhouette_Rank', 'ARI_Rank', 'NMI_Rank', 'DB_Rank']].mean(axis=1)\n",
    "df_ranks[['Method', 'Avg_Rank']].sort_values('Avg_Rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ† Winner Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_method = df_ranks.loc[df_ranks['Avg_Rank'].idxmin(), 'Method']\n",
    "f\"ğŸ† Best Method: {best_method}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best method's labels\n",
    "final_labels = labels_gmm  # GMM typically performs well on this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Business Insights: What We Get Out\n",
    "\n",
    "### ğŸ· Wine Segment Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with clusters\n",
    "df_wine = pd.DataFrame(X, columns=feature_names)\n",
    "df_wine['Cluster'] = final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cluster profiles\n",
    "cluster_profiles = df_wine.groupby('Cluster').mean().round(2)\n",
    "cluster_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster sizes\n",
    "df_wine['Cluster'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Key Differentiating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how much each feature differs between clusters\n",
    "feature_variance = cluster_profiles.std().sort_values(ascending=False)\n",
    "\n",
    "fig = px.bar(\n",
    "    x=feature_variance.values,\n",
    "    y=feature_variance.index,\n",
    "    orientation='h',\n",
    "    title='Feature Importance: What Differentiates Wine Segments?',\n",
    "    labels={'x': 'Variance Across Clusters', 'y': 'Feature'},\n",
    "    template='plotly_white',\n",
    "    height=500, width=800\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Segment Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top features comparison\n",
    "top_features = ['proline', 'color_intensity', 'flavanoids', 'alcohol', 'od280/od315_of_diluted_wines']\n",
    "\n",
    "fig = px.bar(\n",
    "    cluster_profiles[top_features].reset_index().melt(id_vars='Cluster'),\n",
    "    x='variable', y='value', color='Cluster',\n",
    "    barmode='group',\n",
    "    title='Top 5 Differentiating Features by Segment',\n",
    "    labels={'variable': 'Feature', 'value': 'Average Value'},\n",
    "    template='plotly_white',\n",
    "    height=500, width=900\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Business Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create business-friendly segment names\n",
    "segment_names = {\n",
    "    0: 'Premium Reserve',\n",
    "    1: 'Classic Selection',\n",
    "    2: 'Bold & Rich'\n",
    "}\n",
    "\n",
    "df_wine['Segment'] = df_wine['Cluster'].map(segment_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization with segment names\n",
    "df_plot = df_wine.copy()\n",
    "df_plot['PC1'] = X_pca_2d[:, 0]\n",
    "df_plot['PC2'] = X_pca_2d[:, 1]\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_plot, x='PC1', y='PC2',\n",
    "    color='Segment',\n",
    "    title='Final Wine Segmentation',\n",
    "    template='plotly_white',\n",
    "    width=900, height=600,\n",
    "    color_discrete_sequence=['#e41a1c', '#377eb8', '#4daf4a']\n",
    ")\n",
    "fig.update_traces(marker=dict(size=10, opacity=0.8))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Actionable Recommendations\n",
    "\n",
    "### ğŸ· Segment Characteristics\n",
    "\n",
    "**Segment 0: Premium Reserve**\n",
    "```\n",
    "â€¢ High proline and flavanoids\n",
    "â€¢ Lower color intensity\n",
    "â€¢ Higher OD280/OD315 ratio\n",
    "â€¢ Position: Ultra-premium, collectors\n",
    "â€¢ Price: $$$\n",
    "```\n",
    "\n",
    "**Segment 1: Classic Selection**\n",
    "```\n",
    "â€¢ Moderate all characteristics\n",
    "â€¢ Balanced profile\n",
    "â€¢ Position: Everyday premium\n",
    "â€¢ Price: $$\n",
    "```\n",
    "\n",
    "**Segment 2: Bold & Rich**\n",
    "```\n",
    "â€¢ High color intensity\n",
    "â€¢ Higher alcohol content\n",
    "â€¢ Lower flavanoids\n",
    "â€¢ Position: Bold flavor seekers\n",
    "â€¢ Price: $$-$$$\n",
    "```\n",
    "\n",
    "### ğŸ’¼ Business Actions\n",
    "\n",
    "```\n",
    "1. MARKETING\n",
    "   â€¢ Target different segments with tailored messaging\n",
    "   â€¢ Premium Reserve â†’ Luxury magazines, collectors\n",
    "   â€¢ Classic Selection â†’ General wine lovers\n",
    "   â€¢ Bold & Rich â†’ Steak houses, BBQ enthusiasts\n",
    "\n",
    "2. PRICING\n",
    "   â€¢ Adjust price points based on segment\n",
    "   â€¢ Premium Reserve commands higher margins\n",
    "\n",
    "3. INVENTORY\n",
    "   â€¢ Stock based on segment popularity\n",
    "   â€¢ Classic Selection likely highest volume\n",
    "\n",
    "4. RECOMMENDATIONS\n",
    "   â€¢ Build recommendation engine using clusters\n",
    "   â€¢ \"If you liked X, try Y from same segment\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Complete Lifecycle Summary\n",
    "\n",
    "### âœ… The Full Pipeline\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    COMPLETE UNSUPERVISED PIPELINE                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  1. DATA PREPARATION                                                â”‚\n",
    "â”‚     â€¢ Load raw data (178 wines Ã— 13 features)                       â”‚\n",
    "â”‚     â€¢ EDA: distributions, correlations                              â”‚\n",
    "â”‚     â€¢ Preprocessing: StandardScaler                                 â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  2. DIMENSIONALITY REDUCTION                                        â”‚\n",
    "â”‚     â€¢ PCA: 13 â†’ 6 features (90% variance)                          â”‚\n",
    "â”‚     â€¢ t-SNE: For visualization only                                 â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  3. CLUSTERING                                                      â”‚\n",
    "â”‚     â€¢ K-Means: K = [2-8], best K=3                                 â”‚\n",
    "â”‚     â€¢ Hierarchical: linkage=[ward,complete,average]                 â”‚\n",
    "â”‚     â€¢ DBSCAN: eps=[0.5-2.5], min_samples=[3-7]                     â”‚\n",
    "â”‚     â€¢ GMM: n_components=[2-6]                                       â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  4. EVALUATION                                                      â”‚\n",
    "â”‚     â€¢ Internal: Silhouette, CH, DB                                  â”‚\n",
    "â”‚     â€¢ External: ARI, NMI (validation only)                          â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  5. MODEL SELECTION                                                 â”‚\n",
    "â”‚     â€¢ Compare all methods on all metrics                            â”‚\n",
    "â”‚     â€¢ Rank and select best                                          â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  6. BUSINESS OUTPUT                                                 â”‚\n",
    "â”‚     â€¢ Segment profiles                                              â”‚\n",
    "â”‚     â€¢ Feature importance                                            â”‚\n",
    "â”‚     â€¢ Actionable recommendations                                    â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”‘ Key Takeaways\n",
    "\n",
    "**1. Unsupervised Learning Lifecycle**\n",
    "- Not just \"apply algorithm\" - it's a systematic process\n",
    "- Data prep â†’ Dim reduction â†’ Clustering â†’ Evaluation â†’ Insights\n",
    "\n",
    "**2. Hyperparameter Tuning is Critical**\n",
    "- Every algorithm has parameters to tune\n",
    "- Test multiple values systematically\n",
    "- Use metrics to guide selection\n",
    "\n",
    "**3. Multiple Methods, Compare Results**\n",
    "- No single \"best\" algorithm\n",
    "- Different methods may find different patterns\n",
    "- Compare on multiple metrics\n",
    "\n",
    "**4. Domain Knowledge Matters**\n",
    "- Metrics guide, but don't decide\n",
    "- Interpret clusters in business context\n",
    "- Actionable insights are the goal\n",
    "\n",
    "**5. The Output is Business Value**\n",
    "- Clusters alone are not valuable\n",
    "- Profiles, segments, recommendations matter\n",
    "- Connect to business actions\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've completed the full unsupervised learning lifecycle!** ğŸ‰\n",
    "\n",
    "---\n",
    "\n",
    "**AI Tech Institute** | *Building Tomorrow's AI Engineers Today*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
