{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** Â· *Intermediate AI & Data Science*\n",
    "### Unsupervised Learning Lifecycle - Part 1\n",
    "**Instructor:** Amir Charkhi | **Focus:** Data â†’ Preprocessing â†’ Dimensionality Reduction\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ The Complete Unsupervised Learning Workflow\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    UNSUPERVISED LEARNING LIFECYCLE                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  PART 1 (This Notebook):                                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
    "â”‚  â”‚ Raw Data â”‚ â†’  â”‚ Preprocessing â”‚ â†’  â”‚ Dim Reduction (PCA) â”‚       â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  PART 2 (Next Notebook):                                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚ Clustering â”‚ â†’  â”‚ HP Tuning  â”‚ â†’  â”‚ Business Insights â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š What You'll Learn in Part 1\n",
    "\n",
    "- Load and explore real-world data\n",
    "- Build preprocessing pipeline\n",
    "- Understand WHY dimensionality reduction matters\n",
    "- Apply PCA for feature reduction\n",
    "- Use t-SNE for visualization\n",
    "- Prepare data for clustering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. The Dataset: Wine Recognition\n",
    "\n",
    "### ğŸ· Business Context\n",
    "\n",
    "**Scenario:** A wine distributor has chemical analysis data for wines from three different cultivars (grape varieties). They want to:\n",
    "- Group wines by chemical profile (without using cultivar labels)\n",
    "- Understand what makes wines similar/different\n",
    "- Create marketing segments\n",
    "\n",
    "**The Challenge:**\n",
    "```\n",
    "13 chemical features â†’ Hard to visualize!\n",
    "No labels provided â†’ Unsupervised learning needed!\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "y_true = wine.target  # We'll use this ONLY for evaluation, not training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¡ **178 wines, 13 chemical features** - too many dimensions to visualize directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Feature Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¡ **Notice the different scales!**\n",
    "- `proline`: 278 to 1680\n",
    "- `magnesium`: 70 to 162\n",
    "- `nonflavanoid_phenols`: 0.13 to 0.66\n",
    "\n",
    "**This is why we need scaling!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "### ğŸ” Check for Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values?\n",
    "X.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "X.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Clean data:** No missing values, all numeric features\n",
    "\n",
    "### ğŸ“ˆ Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 4 key features for visualization\n",
    "key_features = ['alcohol', 'malic_acid', 'flavanoids', 'proline']\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=key_features)\n",
    "\n",
    "for i, feat in enumerate(key_features):\n",
    "    row, col = (i // 2) + 1, (i % 2) + 1\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=X[feat], nbinsx=20, name=feat, showlegend=False),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "fig.update_layout(title='Feature Distributions', height=500, template='plotly_white')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”— Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = X.corr()\n",
    "\n",
    "fig = px.imshow(\n",
    "    corr_matrix,\n",
    "    text_auto='.2f',\n",
    "    color_continuous_scale='RdBu_r',\n",
    "    title='Feature Correlation Matrix',\n",
    "    width=900,\n",
    "    height=800\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¡ **Key Insight:** Many features are correlated!\n",
    "- `flavanoids` â†” `total_phenols`: 0.86\n",
    "- `od280/od315_of_diluted_wines` â†” `flavanoids`: 0.79\n",
    "\n",
    "**Correlated features = redundant information â†’ PCA can help!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing Pipeline\n",
    "\n",
    "### ğŸ”§ Why Preprocessing Matters\n",
    "\n",
    "```\n",
    "Problem: Different scales\n",
    "\n",
    "Feature A: [0.1, 0.2, 0.3]      (small values)\n",
    "Feature B: [1000, 2000, 3000]   (large values)\n",
    "\n",
    "Distance calculation:\n",
    "  Feature B dominates! Feature A is ignored.\n",
    "\n",
    "Solution: StandardScaler\n",
    "  All features â†’ mean=0, std=1\n",
    "  Equal contribution to distance\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify scaling worked\n",
    "pd.DataFrame({\n",
    "    'Feature': X.columns[:4],\n",
    "    'Original Mean': X.iloc[:, :4].mean().round(2).values,\n",
    "    'Scaled Mean': X_scaled[:, :4].mean(axis=0).round(6),\n",
    "    'Original Std': X.iloc[:, :4].std().round(2).values,\n",
    "    'Scaled Std': X_scaled[:, :4].std(axis=0).round(2)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **All features now have meanâ‰ˆ0 and stdâ‰ˆ1**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Why Dimensionality Reduction?\n",
    "\n",
    "### â“ The Problem: Curse of Dimensionality\n",
    "\n",
    "```\n",
    "With 13 features:\n",
    "\n",
    "1. Can't visualize (we see in 2D/3D only)\n",
    "2. Distance metrics become unreliable\n",
    "3. Clustering algorithms struggle\n",
    "4. Many features may be redundant (correlated)\n",
    "```\n",
    "\n",
    "### ğŸ’¡ The Solution: Reduce Dimensions\n",
    "\n",
    "```\n",
    "Two main purposes:\n",
    "\n",
    "1. VISUALIZATION (t-SNE)\n",
    "   13D â†’ 2D for plotting\n",
    "   See if natural groups exist\n",
    "\n",
    "2. FEATURE REDUCTION (PCA)\n",
    "   13D â†’ fewer D for modeling\n",
    "   Remove redundancy\n",
    "   Improve clustering\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PCA: Principal Component Analysis\n",
    "\n",
    "### ğŸ“– What PCA Does\n",
    "\n",
    "```\n",
    "Finds new axes (principal components) that:\n",
    "1. Capture maximum variance\n",
    "2. Are uncorrelated with each other\n",
    "3. Ordered by importance\n",
    "\n",
    "Result:\n",
    "  PC1: Most variance\n",
    "  PC2: Second most (perpendicular to PC1)\n",
    "  PC3: Third most (perpendicular to PC1 & PC2)\n",
    "  ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Step 1: How Many Components Do We Need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA with all components first\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative variance\n",
    "cumulative_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Find components for 95% variance\n",
    "n_components_95 = np.argmax(cumulative_var >= 0.95) + 1\n",
    "n_components_95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¡ **Only need ~10 components to capture 95% of information!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize variance explained\n",
    "fig = go.Figure()\n",
    "\n",
    "# Individual variance bars\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(range(1, 14)),\n",
    "    y=pca_full.explained_variance_ratio_,\n",
    "    name='Individual',\n",
    "    marker_color='steelblue'\n",
    "))\n",
    "\n",
    "# Cumulative line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(1, 14)),\n",
    "    y=cumulative_var,\n",
    "    name='Cumulative',\n",
    "    mode='lines+markers',\n",
    "    line=dict(color='red', width=3),\n",
    "    yaxis='y2'\n",
    "))\n",
    "\n",
    "# 95% threshold line\n",
    "fig.add_hline(y=0.95, line_dash='dash', line_color='green', \n",
    "              annotation_text='95% threshold', yref='y2')\n",
    "\n",
    "fig.update_layout(\n",
    "    title='PCA: Variance Explained by Component',\n",
    "    xaxis_title='Principal Component',\n",
    "    yaxis_title='Variance Ratio',\n",
    "    yaxis2=dict(title='Cumulative Variance', overlaying='y', side='right', range=[0, 1.05]),\n",
    "    template='plotly_white',\n",
    "    height=500,\n",
    "    width=900\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Step 2: Apply PCA for Visualization (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to 2 dimensions for visualization\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance captured by 2 components\n",
    "var_2d = pca_2d.explained_variance_ratio_.sum()\n",
    "f\"{var_2d:.1%} variance captured in 2D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for plotting\n",
    "df_pca = pd.DataFrame({\n",
    "    'PC1': X_pca_2d[:, 0],\n",
    "    'PC2': X_pca_2d[:, 1],\n",
    "    'True_Label': y_true  # Only for visualization validation!\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize - WITHOUT labels (unsupervised view)\n",
    "fig = px.scatter(\n",
    "    df_pca, x='PC1', y='PC2',\n",
    "    title=f'PCA: 13D â†’ 2D ({var_2d:.1%} variance) - Can You See Groups?',\n",
    "    template='plotly_white',\n",
    "    width=800, height=600\n",
    ")\n",
    "fig.update_traces(marker=dict(size=10, color='steelblue', opacity=0.7))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¡ **Can you see natural groupings?** Let's verify with true labels (just for validation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize WITH true labels (validation only!)\n",
    "fig = px.scatter(\n",
    "    df_pca, x='PC1', y='PC2',\n",
    "    color=df_pca['True_Label'].astype(str),\n",
    "    title='PCA with True Labels (Validation Only - We Pretend Not to Know These!)',\n",
    "    template='plotly_white',\n",
    "    width=800, height=600,\n",
    "    color_discrete_sequence=['#e41a1c', '#377eb8', '#4daf4a']\n",
    ")\n",
    "fig.update_traces(marker=dict(size=10, opacity=0.7))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **PCA reveals the underlying structure!** Three groups are visible.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” Step 3: Understand What PCA Learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature loadings - what contributes to each PC?\n",
    "loadings = pd.DataFrame(\n",
    "    pca_2d.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=X.columns\n",
    ").round(3)\n",
    "\n",
    "loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loadings\n",
    "fig = px.bar(\n",
    "    loadings.reset_index().melt(id_vars='index'),\n",
    "    x='index', y='value', color='variable',\n",
    "    barmode='group',\n",
    "    title='PCA Feature Loadings: What Drives Each Component?',\n",
    "    labels={'index': 'Feature', 'value': 'Loading', 'variable': 'Component'},\n",
    "    template='plotly_white',\n",
    "    height=500, width=1000\n",
    ")\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¡ **Interpretation:**\n",
    "- **PC1:** Driven by flavanoids, phenols, proline (quality indicators)\n",
    "- **PC2:** Driven by color_intensity, alcalinity, malic_acid (acidity/color)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. t-SNE: Better Visualization\n",
    "\n",
    "### ğŸ“– PCA vs t-SNE\n",
    "\n",
    "```\n",
    "PCA:                          t-SNE:\n",
    "- Linear transformation       - Non-linear\n",
    "- Preserves global structure  - Preserves LOCAL structure\n",
    "- Fast                        - Slower\n",
    "- Good for preprocessing      - Good for VISUALIZATION only\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df_tsne = pd.DataFrame({\n",
    "    'Dim1': X_tsne[:, 0],\n",
    "    'Dim2': X_tsne[:, 1],\n",
    "    'True_Label': y_true\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PCA vs t-SNE\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('PCA (Linear)', 't-SNE (Non-linear)'))\n",
    "\n",
    "# PCA\n",
    "for label in [0, 1, 2]:\n",
    "    mask = df_pca['True_Label'] == label\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_pca[mask]['PC1'], y=df_pca[mask]['PC2'],\n",
    "                   mode='markers', name=f'Class {label}',\n",
    "                   marker=dict(size=8, opacity=0.7),\n",
    "                   showlegend=True),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# t-SNE\n",
    "for label in [0, 1, 2]:\n",
    "    mask = df_tsne['True_Label'] == label\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_tsne[mask]['Dim1'], y=df_tsne[mask]['Dim2'],\n",
    "                   mode='markers', name=f'Class {label}',\n",
    "                   marker=dict(size=8, opacity=0.7),\n",
    "                   showlegend=False),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Dimensionality Reduction Comparison',\n",
    "    template='plotly_white',\n",
    "    height=500, width=1100\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¡ **t-SNE gives clearer cluster separation!** \n",
    "\n",
    "But remember: t-SNE is for **visualization only**, not for feature engineering.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prepare Data for Clustering\n",
    "\n",
    "### ğŸ¯ Which Representation to Use?\n",
    "\n",
    "```\n",
    "Option 1: Original scaled features (13D)\n",
    "  - All information preserved\n",
    "  - May have noise and redundancy\n",
    "\n",
    "Option 2: PCA-reduced features (e.g., 5D)\n",
    "  - Removes redundancy\n",
    "  - Keeps most variance\n",
    "  - Often improves clustering\n",
    "\n",
    "Option 3: t-SNE (2D)\n",
    "  - NEVER use for clustering!\n",
    "  - Only for visualization\n",
    "```\n",
    "\n",
    "**Best Practice:** Try both original and PCA-reduced, compare results!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA with optimal components for clustering\n",
    "pca_cluster = PCA(n_components=0.90)  # Keep 90% variance\n",
    "X_pca_cluster = pca_cluster.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_cluster.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¡ **Reduced from 13 to 6 dimensions while keeping 90% of variance!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save prepared data for Part 2\n",
    "prepared_data = {\n",
    "    'X_scaled': X_scaled,           # Original scaled (13D)\n",
    "    'X_pca': X_pca_cluster,         # PCA reduced (6D)\n",
    "    'X_pca_2d': X_pca_2d,           # For visualization (2D)\n",
    "    'X_tsne': X_tsne,               # For visualization (2D)\n",
    "    'y_true': y_true,               # True labels (for evaluation only)\n",
    "    'feature_names': X.columns.tolist()\n",
    "}\n",
    "\n",
    "# Save to file for Part 2\n",
    "np.savez('prepared_wine_data.npz', **prepared_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary: Part 1 Complete!\n",
    "\n",
    "### âœ… What We Did\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Raw Data (178 wines Ã— 13 features)                        â”‚\n",
    "â”‚       â†“                                                     â”‚\n",
    "â”‚  EDA: Checked distributions, correlations                   â”‚\n",
    "â”‚       â†“                                                     â”‚\n",
    "â”‚  Preprocessing: StandardScaler (mean=0, std=1)              â”‚\n",
    "â”‚       â†“                                                     â”‚\n",
    "â”‚  Dimensionality Reduction:                                  â”‚\n",
    "â”‚    â€¢ PCA (90% variance) â†’ 6 dimensions for clustering       â”‚\n",
    "â”‚    â€¢ PCA (2D) â†’ visualization                               â”‚\n",
    "â”‚    â€¢ t-SNE (2D) â†’ visualization                             â”‚\n",
    "â”‚       â†“                                                     â”‚\n",
    "â”‚  Ready for Clustering! (Part 2)                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ”‘ Key Takeaways\n",
    "\n",
    "**1. Preprocessing is Critical**\n",
    "- Scale features before distance-based methods\n",
    "- Different scales = biased results\n",
    "\n",
    "**2. Dimensionality Reduction Serves Two Purposes**\n",
    "- Visualization (PCA or t-SNE to 2D)\n",
    "- Feature reduction (PCA to fewer dimensions)\n",
    "\n",
    "**3. PCA vs t-SNE**\n",
    "- PCA: Fast, linear, good for preprocessing\n",
    "- t-SNE: Non-linear, better separation, visualization ONLY\n",
    "\n",
    "**4. Natural Groups Exist**\n",
    "- Visualization showed 3 distinct clusters\n",
    "- Now we need to find them automatically!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Next: Part 2\n",
    "\n",
    "In Part 2, we will:\n",
    "- Apply multiple clustering algorithms\n",
    "- Tune hyperparameters systematically\n",
    "- Evaluate and compare methods\n",
    "- Select the best model\n",
    "- Extract business insights\n",
    "\n",
    "---\n",
    "\n",
    "**AI Tech Institute** | *Building Tomorrow's AI Engineers Today*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
