{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** Â· *Intermediate AI & Data Science*\n",
    "### Week 9 Session 1: Linear Models for Regression\n",
    "**Instructor:** Amir Charkhi | **Goal:** Master Linear Regression and Its Variants\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand linear regression and its mathematical foundation\n",
    "- Learn regularization techniques: Ridge, Lasso, and ElasticNet\n",
    "- Master the complete ML workflow: data loading â†’ EDA â†’ training â†’ evaluation\n",
    "- Apply cross-validation and hyperparameter tuning\n",
    "- Interpret model coefficients and feature importance\n",
    "- Compare model performance using appropriate metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "**What you need to do:**  \n",
    "Import all necessary libraries for data manipulation, visualization, and machine learning.\n",
    "\n",
    "**Required imports:**\n",
    "- NumPy and Pandas for data handling\n",
    "- Matplotlib and Seaborn for visualization\n",
    "- Scikit-learn for models, preprocessing, and evaluation\n",
    "\n",
    "**ğŸ’¡ Hint:** We'll need `LinearRegression`, `Ridge`, `Lasso`, `ElasticNet`, `cross_val_score`, `GridSearchCV`, and regression metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load the Dataset\n",
    "\n",
    "**What you need to do:**  \n",
    "Load the Online Retail dataset - a real-world e-commerce transaction dataset from UCI.\n",
    "\n",
    "**Theory:**  \n",
    "This dataset contains actual transactions from a UK-based online retailer between 2009-2011. It includes:\n",
    "- **InvoiceNo**: Unique transaction identifier\n",
    "- **StockCode**: Product code\n",
    "- **Description**: Product name\n",
    "- **Quantity**: Number of items purchased\n",
    "- **InvoiceDate**: Transaction date and time\n",
    "- **UnitPrice**: Price per item\n",
    "- **CustomerID**: Unique customer identifier\n",
    "- **Country**: Customer's country\n",
    "\n",
    "**Our Goal:** Predict **TotalSales** (Quantity Ã— UnitPrice) based on various features we'll engineer.\n",
    "\n",
    "**ğŸ’¡ Hint:** We'll load the data and create meaningful features for regression modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Online Retail dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx'\n",
    "\n",
    "print(\"ğŸ“¥ Loading Online Retail dataset from UCI...\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "# Load the data\n",
    "df_raw = pd.read_excel(url)\n",
    "\n",
    "print(f\"âœ… Dataset loaded successfully!\")\n",
    "print(f\"ğŸ“Š Shape: {df_raw.shape[0]:,} rows Ã— {df_raw.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Initial Data Inspection & Feature Engineering\n",
    "\n",
    "**What you need to do:**  \n",
    "Inspect the raw data and create meaningful features for regression modeling.\n",
    "\n",
    "**Tasks:**\n",
    "1. Display first few rows and check data types\n",
    "2. Check for missing values\n",
    "3. Create target variable: **TotalSales** = Quantity Ã— UnitPrice\n",
    "4. Engineer features from the transaction data\n",
    "5. Clean the data (remove cancellations, negative values, missing customers)\n",
    "\n",
    "**ğŸ’¡ Hint:** We'll aggregate data at the transaction (Invoice) level to create our regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect raw data\n",
    "print(\"ğŸ“‹ First few rows of raw data:\")\n",
    "print(df_raw.head())\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"ğŸ” Data Info:\")\n",
    "print(df_raw.info())\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"â“ Missing Values:\")\n",
    "print(df_raw.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and Feature Engineering\n",
    "print(\"ğŸ§¹ Cleaning and engineering features...\\n\")\n",
    "\n",
    "# Create a copy for processing\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Remove rows with missing CustomerID (we need customers for aggregation)\n",
    "df = df.dropna(subset=['CustomerID'])\n",
    "\n",
    "# Remove cancellations (InvoiceNo starting with 'C')\n",
    "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
    "\n",
    "# Remove negative quantities and prices\n",
    "df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
    "\n",
    "# Create TotalSales column\n",
    "df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
    "\n",
    "# Extract time-based features\n",
    "df['Year'] = df['InvoiceDate'].dt.year\n",
    "df['Month'] = df['InvoiceDate'].dt.month\n",
    "df['DayOfWeek'] = df['InvoiceDate'].dt.dayofweek\n",
    "df['Hour'] = df['InvoiceDate'].dt.hour\n",
    "\n",
    "print(f\"âœ… Cleaned dataset shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"\\nğŸ“Š After cleaning: {df.shape[0]:,} transactions remaining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering at Invoice Level\n",
    "print(\"ğŸ”¨ Engineering features...\\\\n\")\n",
    "\n",
    "invoice_features = df.groupby('InvoiceNo').agg({\n",
    "    'TotalSales': 'sum',\n",
    "    'Quantity': 'sum',\n",
    "    'UnitPrice': 'mean',\n",
    "    'StockCode': 'nunique',\n",
    "    'CustomerID': 'first',\n",
    "    'Country': 'first',\n",
    "    'Year': 'first',\n",
    "    'Month': 'first',\n",
    "    'DayOfWeek': 'first',\n",
    "    'Hour': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "invoice_features.rename(columns={\n",
    "    'Quantity': 'TotalItems',\n",
    "    'UnitPrice': 'AvgItemPrice',\n",
    "    'StockCode': 'NumUniqueProducts'\n",
    "}, inplace=True)\n",
    "\n",
    "invoice_features['AvgPricePerItem'] = invoice_features['TotalSales'] / invoice_features['TotalItems']\n",
    "\n",
    "# Country encoding\n",
    "top_countries = invoice_features['Country'].value_counts().head(5).index.tolist()\n",
    "invoice_features['Country_Group'] = invoice_features['Country'].apply(\n",
    "    lambda x: x if x in top_countries else 'Other'\n",
    ")\n",
    "country_dummies = pd.get_dummies(invoice_features['Country_Group'], prefix='Country', drop_first=True)\n",
    "invoice_features = pd.concat([invoice_features, country_dummies], axis=1)\n",
    "\n",
    "invoice_features['IsWeekend'] = (invoice_features['DayOfWeek'] >= 5).astype(int)\n",
    "\n",
    "# âš ï¸ CRITICAL: Drop string columns NOW before train-test split\n",
    "invoice_features = invoice_features.drop(columns=['InvoiceNo', 'Country', 'Country_Group', 'CustomerID'])\n",
    "\n",
    "print(f\"âœ… Created {invoice_features.shape[0]:,} invoice-level samples\")\n",
    "print(f\"ğŸ“Š Total features: {invoice_features.shape[1]} columns\")\n",
    "print(f\"\\nğŸ“‹ Columns remaining: {list(invoice_features.columns)}\")\n",
    "\n",
    "# Verify all columns except target are numeric\n",
    "non_target_cols = [col for col in invoice_features.columns if col != 'TotalSales']\n",
    "numeric_check = invoice_features[non_target_cols].select_dtypes(include=[np.number]).shape[1] == len(non_target_cols)\n",
    "print(f\"\\nâœ… All feature columns are numeric: {numeric_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of engineered features\n",
    "print(\"ğŸ¯ Sample of engineered features:\")\n",
    "print(invoice_features.head(10))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"ğŸ“ˆ Descriptive statistics:\")\n",
    "print(invoice_features.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Train-Validation-Test Split\n",
    "\n",
    "**âš ï¸ CRITICAL: Split BEFORE detailed EDA to prevent data leakage!**\n",
    "\n",
    "**What you need to do:**  \n",
    "Split the data into three sets:\n",
    "- **Training set (60%)**: For model training\n",
    "- **Validation set (20%)**: For model selection and hyperparameter tuning\n",
    "- **Test set (20%)**: For final evaluation (DO NOT TOUCH until the end!)\n",
    "\n",
    "**Theory:**  \n",
    "The test set represents unseen data in production. It must remain isolated from all training decisions.\n",
    "\n",
    "**ğŸ’¡ Hint:** Select relevant features and create X (features) and y (target) matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling - EXCLUDE original string columns\n",
    "feature_cols = [\n",
    "    'TotalItems', 'AvgItemPrice', 'NumUniqueProducts', \n",
    "    'AvgPricePerItem', 'Year', 'Month', 'DayOfWeek', \n",
    "    'Hour', 'IsWeekend'\n",
    "]\n",
    "\n",
    "# Add country dummy variables\n",
    "country_cols = [col for col in invoice_features.columns if col.startswith('Country_')]\n",
    "feature_cols.extend(country_cols)\n",
    "\n",
    "# Create feature matrix X and target vector y\n",
    "X = invoice_features[feature_cols].copy()\n",
    "y = invoice_features['TotalSales'].copy()\n",
    "\n",
    "print(f\"ğŸ¯ Features for modeling: {len(feature_cols)} features\")\n",
    "print(f\"Feature list: {feature_cols}\")\n",
    "print(f\"\\nğŸ“Š X shape: {X.shape}\")\n",
    "print(f\"ğŸ“Š y shape: {y.shape}\")\n",
    "\n",
    "# Verify all columns are numeric\n",
    "print(f\"\\nâœ… All features are numeric: {X.select_dtypes(include=[np.number]).shape[1] == X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 60% train, 20% validation, 20% test\n",
    "print(\"âœ‚ï¸ Splitting data into train/validation/test sets...\\n\")\n",
    "\n",
    "# First split: 80% train+val, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 75% of temp = 60% train, 25% of temp = 20% val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"âœ… Data split complete!\")\n",
    "print(f\"\\nğŸ“Š Training set:   {X_train.shape[0]:>6,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"ğŸ“Š Validation set: {X_val.shape[0]:>6,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"ğŸ“Š Test set:       {X_test.shape[0]:>6,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nğŸ”’ Test set is now LOCKED until final evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "**âš ï¸ IMPORTANT: Perform EDA ONLY on the training set!**\n",
    "\n",
    "**What you need to do:**  \n",
    "Understand the training data through visualization and statistics.\n",
    "\n",
    "**Tasks:**\n",
    "1. Summary statistics\n",
    "2. Target variable distribution\n",
    "3. Correlation heatmap\n",
    "4. Top correlated features\n",
    "5. Scatter plots\n",
    "\n",
    "**ğŸ’¡ Hint:** This helps us understand relationships before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of training features\n",
    "print(\"ğŸ“Š Training Set Summary Statistics:\")\n",
    "print(\"=\"*80)\n",
    "print(X_train.describe())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"\\nğŸ¯ Target Variable (TotalSales) Statistics:\")\n",
    "print(y_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Original scale\n",
    "axes[0].hist(y_train, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Total Sales ($)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Target Variable Distribution (Training Set)')\n",
    "axes[0].axvline(y_train.mean(), color='red', linestyle='--', label=f'Mean: ${y_train.mean():.2f}')\n",
    "axes[0].axvline(y_train.median(), color='green', linestyle='--', label=f'Median: ${y_train.median():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Log scale (better for skewed distributions)\n",
    "axes[1].hist(np.log1p(y_train), bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_xlabel('Log(Total Sales + 1)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Target Variable Distribution - Log Scale')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ Insight: The target variable is right-skewed (common in sales data).\")\n",
    "print(f\"This suggests we might benefit from log transformation or robust models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and target for correlation analysis\n",
    "train_data = X_train.copy()\n",
    "train_data['TotalSales'] = y_train.values\n",
    "\n",
    "# Calculate correlations with target (numeric columns only)\n",
    "correlations = train_data.select_dtypes(include=[np.number]).corr()['TotalSales'].drop('TotalSales').sort_values(ascending=False)\n",
    "\n",
    "print(\"ğŸ“Š Top 10 Features Correlated with Total Sales:\")\n",
    "print(\"=\"*60)\n",
    "for feature, corr in correlations.head(10).items():\n",
    "    print(f\"{feature:.<45} {corr:>8.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“Š Bottom 5 Features (Negative/Weak Correlation):\")\n",
    "print(\"=\"*60)\n",
    "for feature, corr in correlations.tail(5).items():\n",
    "    print(f\"{feature:.<45} {corr:>8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap (top features only for readability)\n",
    "top_features = correlations.head(8).index.tolist() + ['TotalSales']\n",
    "correlation_matrix = train_data[top_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Correlation Heatmap: Top Features vs Total Sales', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for top correlated features\n",
    "top_3_features = correlations.head(3).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, feature in enumerate(top_3_features):\n",
    "    axes[idx].scatter(X_train[feature], y_train, alpha=0.3, s=10)\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Total Sales ($)')\n",
    "    axes[idx].set_title(f'{feature} vs Total Sales\\n(Correlation: {correlations[feature]:.3f})')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(X_train[feature], y_train, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[idx].plot(X_train[feature], p(X_train[feature]), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Feature Scaling\n",
    "\n",
    "**Theory:**  \n",
    "Linear models with regularization (Ridge, Lasso, ElasticNet) are sensitive to feature scales. Features with larger magnitudes can dominate the model. **Standardization** (z-score normalization) ensures all features have mean=0 and std=1.\n",
    "\n",
    "**Formula:** $X_{scaled} = \\frac{X - \\mu}{\\sigma}$\n",
    "\n",
    "**What you need to do:**  \n",
    "Fit StandardScaler on training data and transform all splits.\n",
    "\n",
    "**âš ï¸ CRITICAL:** Fit the scaler ONLY on training data to prevent data leakage!\n",
    "\n",
    "**ğŸ’¡ Hint:** Plain LinearRegression doesn't require scaling, but we'll scale for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit scaler on training data ONLY\n",
    "print(\"âš–ï¸ Scaling features for regularized models...\\n\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)  # For later use\n",
    "\n",
    "print(\"âœ… Features scaled successfully!\")\n",
    "print(f\"\\nğŸ“Š Scaled training features - Mean: {X_train_scaled.mean():.6f}, Std: {X_train_scaled.std():.6f}\")\n",
    "print(f\"\\nğŸ’¡ Note: We fit the scaler only on training data to prevent data leakage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Model 1: Linear Regression (Ordinary Least Squares)\n",
    "\n",
    "**ğŸ“š Theory:**  \n",
    "Linear Regression finds the best-fitting hyperplane through the data by minimizing the **sum of squared errors** (SSE).\n",
    "\n",
    "**Mathematical Form:**\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n$$\n",
    "\n",
    "**Objective Function (Cost Function):**\n",
    "$$\\text{Minimize: } SSE = \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Pros:**\n",
    "- Fast to train\n",
    "- Highly interpretable (coefficients show feature importance)\n",
    "- No hyperparameters to tune\n",
    "- Works well when features have true linear relationships\n",
    "\n",
    "**Cons:**\n",
    "- Assumes linear relationships\n",
    "- Sensitive to outliers\n",
    "- Prone to overfitting with many features (high variance)\n",
    "- No built-in feature selection\n",
    "\n",
    "**When to Use:**\n",
    "- As a baseline model\n",
    "- When interpretability is crucial\n",
    "- When you have fewer features than samples\n",
    "- When features are not highly correlated (no multicollinearity)\n",
    "\n",
    "**ğŸ“– References:**\n",
    "- [Scikit-learn: Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "- [ISL Book - Chapter 3: Linear Regression](https://www.statlearning.com/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear Regression model\n",
    "print(\"ğŸš€ Training Linear Regression (OLS) model...\\n\")\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)  # No need for scaling for OLS\n",
    "\n",
    "print(\"âœ… Model trained successfully!\")\n",
    "print(f\"\\nğŸ“Š Model Coefficients (Top 10 by magnitude):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': lr_model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(coef_df.head(10).to_string(index=False))\n",
    "print(f\"\\nğŸ“ Intercept: {lr_model.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation set\n",
    "y_pred_lr = lr_model.predict(X_val)\n",
    "\n",
    "# Calculate metrics\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_val, y_pred_lr))\n",
    "lr_mae = mean_absolute_error(y_val, y_pred_lr)\n",
    "lr_r2 = r2_score(y_val, y_pred_lr)\n",
    "\n",
    "print(\"ğŸ“Š Linear Regression - Validation Set Performance:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Root Mean Squared Error (RMSE): ${lr_rmse:>12,.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE):      ${lr_mae:>12,.2f}\")\n",
    "print(f\"RÂ² Score:                       {lr_r2:>12.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nğŸ’¡ Interpretation:\")\n",
    "print(f\"   - RÂ² = {lr_r2:.2%}: The model explains {lr_r2:.1%} of variance in Total Sales\")\n",
    "print(f\"   - MAE = ${lr_mae:.2f}: On average, predictions are off by ${lr_mae:.2f}\")\n",
    "print(f\"   - RMSE = ${lr_rmse:.2f}: The model has higher penalty for large errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance (absolute coefficient values)\n",
    "plt.figure(figsize=(10, 8))\n",
    "coef_df_top15 = coef_df.head(15).sort_values('Coefficient')\n",
    "\n",
    "plt.barh(coef_df_top15['Feature'], coef_df_top15['Coefficient'])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Linear Regression: Top 15 Feature Coefficients', fontsize=14, pad=20)\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ Positive coefficients increase sales, negative coefficients decrease sales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on training data\n",
    "print(\"ğŸ”„ Performing 5-Fold Cross-Validation on Linear Regression...\\n\")\n",
    "\n",
    "cv_scores_lr = cross_val_score(\n",
    "    lr_model, X_train, y_train, \n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "cv_scores_lr = -cv_scores_lr  # Convert to positive RMSE\n",
    "\n",
    "print(\"ğŸ“Š Cross-Validation Results (RMSE):\")\n",
    "print(\"=\"*60)\n",
    "for fold, score in enumerate(cv_scores_lr, 1):\n",
    "    print(f\"Fold {fold}: ${score:,.2f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean CV RMSE:   ${cv_scores_lr.mean():,.2f} (Â± ${cv_scores_lr.std():.2f})\")\n",
    "print(f\"Validation RMSE: ${lr_rmse:,.2f}\")\n",
    "print(\"\\nğŸ’¡ CV scores help us understand model stability across different data splits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Model 2: Ridge Regression (L2 Regularization)\n",
    "\n",
    "**ğŸ“š Theory:**  \n",
    "Ridge Regression adds a **penalty term** to the cost function to prevent overfitting. It shrinks coefficients but keeps all features.\n",
    "\n",
    "**Cost Function:**\n",
    "$$\\text{Minimize: } SSE + \\alpha \\sum_{j=1}^{n} \\beta_j^2$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = Regularization strength (hyperparameter)\n",
    "- $\\sum \\beta_j^2$ = L2 penalty (sum of squared coefficients)\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **L2 Penalty:** Penalizes large coefficients\n",
    "- **Feature shrinkage:** Reduces coefficient magnitudes (but never to exactly zero)\n",
    "- **Handles multicollinearity:** Works well when features are correlated\n",
    "- **Hyperparameter $\\alpha$:** Higher $\\alpha$ = stronger regularization = simpler model\n",
    "\n",
    "**Pros:**\n",
    "- Reduces overfitting\n",
    "- Handles multicollinearity better than OLS\n",
    "- More stable than OLS with correlated features\n",
    "- Keeps all features (doesn't eliminate any)\n",
    "\n",
    "**Cons:**\n",
    "- Less interpretable than OLS (shrunk coefficients)\n",
    "- Doesn't perform feature selection (all features retained)\n",
    "- Requires hyperparameter tuning\n",
    "- Requires feature scaling\n",
    "\n",
    "**When to Use:**\n",
    "- When you have many features\n",
    "- When features are correlated (multicollinearity)\n",
    "- When you want to keep all features but reduce overfitting\n",
    "- When OLS shows signs of overfitting\n",
    "\n",
    "**Typical $\\alpha$ Values:**\n",
    "- $\\alpha = 0$: Equivalent to OLS\n",
    "- $\\alpha \\in [0.01, 100]$: Common range for experimentation\n",
    "- Larger $\\alpha$: More regularization, simpler model\n",
    "\n",
    "**ğŸ“– References:**\n",
    "- [Scikit-learn: Ridge Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "- [ISL Book - Chapter 6: Ridge Regression](https://www.statlearning.com/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ridge Regression with default alpha\n",
    "print(\"ğŸš€ Training Ridge Regression (L2 Regularization)...\\n\")\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"âœ… Ridge model trained with Î± = 1.0\")\n",
    "print(f\"\\nğŸ“Š Ridge Coefficients (Top 10 by magnitude):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ridge_coef_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Ridge_Coef': ridge_model.coef_,\n",
    "    'OLS_Coef': lr_model.coef_\n",
    "}).sort_values('Ridge_Coef', key=abs, ascending=False)\n",
    "\n",
    "print(ridge_coef_df.head(10).to_string(index=False))\n",
    "print(f\"\\nğŸ’¡ Notice: Ridge coefficients are smaller (shrunk) compared to OLS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Ridge on validation set\n",
    "y_pred_ridge = ridge_model.predict(X_val_scaled)\n",
    "\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_val, y_pred_ridge))\n",
    "ridge_mae = mean_absolute_error(y_val, y_pred_ridge)\n",
    "ridge_r2 = r2_score(y_val, y_pred_ridge)\n",
    "\n",
    "print(\"ğŸ“Š Ridge Regression - Validation Set Performance:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Root Mean Squared Error (RMSE): ${ridge_rmse:>12,.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE):      ${ridge_mae:>12,.2f}\")\n",
    "print(f\"RÂ² Score:                       {ridge_r2:>12.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearchCV\n",
    "print(\"ğŸ¯ Tuning Ridge hyperparameter (alpha) with GridSearchCV...\\n\")\n",
    "\n",
    "param_grid_ridge = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1.0, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "ridge_grid = GridSearchCV(\n",
    "    Ridge(random_state=42),\n",
    "    param_grid_ridge,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "ridge_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nâœ… Best parameters: {ridge_grid.best_params_}\")\n",
    "print(f\"ğŸ“Š Best CV RMSE: ${-ridge_grid.best_score_:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best Ridge model\n",
    "best_ridge = ridge_grid.best_estimator_\n",
    "y_pred_ridge_tuned = best_ridge.predict(X_val_scaled)\n",
    "\n",
    "ridge_tuned_rmse = np.sqrt(mean_squared_error(y_val, y_pred_ridge_tuned))\n",
    "ridge_tuned_mae = mean_absolute_error(y_val, y_pred_ridge_tuned)\n",
    "ridge_tuned_r2 = r2_score(y_val, y_pred_ridge_tuned)\n",
    "\n",
    "print(\"ğŸ“Š Ridge (Tuned) - Validation Set Performance:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Root Mean Squared Error (RMSE): ${ridge_tuned_rmse:>12,.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE):      ${ridge_tuned_mae:>12,.2f}\")\n",
    "print(f\"RÂ² Score:                       {ridge_tuned_r2:>12.4f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ’¡ Best alpha = {ridge_grid.best_params_['alpha']}: This is the optimal regularization strength.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize alpha vs performance\n",
    "results_df = pd.DataFrame(ridge_grid.cv_results_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['param_alpha'], -results_df['mean_test_score'], marker='o', linewidth=2, markersize=8)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Alpha (Regularization Strength)', fontsize=12)\n",
    "plt.ylabel('Cross-Validated RMSE ($)', fontsize=12)\n",
    "plt.title('Ridge Regression: Alpha vs Performance', fontsize=14, pad=20)\n",
    "plt.axvline(ridge_grid.best_params_['alpha'], color='red', linestyle='--', \n",
    "            label=f\"Best Alpha = {ridge_grid.best_params_['alpha']}\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ The curve shows how different alpha values affect model performance.\")\n",
    "print(\"   Lower alpha = less regularization (closer to OLS)\")\n",
    "print(\"   Higher alpha = more regularization (simpler model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model 3: Lasso Regression (L1 Regularization)\n",
    "\n",
    "**ğŸ“š Theory:**  \n",
    "Lasso (**L**east **A**bsolute **S**hrinkage and **S**election **O**perator) adds an L1 penalty that can **shrink coefficients to exactly zero**, effectively performing **automatic feature selection**.\n",
    "\n",
    "**Cost Function:**\n",
    "$$\\text{Minimize: } SSE + \\alpha \\sum_{j=1}^{n} |\\beta_j|$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = Regularization strength\n",
    "- $\\sum |\\beta_j|$ = L1 penalty (sum of absolute coefficients)\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **L1 Penalty:** Penalizes absolute values of coefficients\n",
    "- **Sparse solutions:** Sets some coefficients to exactly zero\n",
    "- **Automatic feature selection:** Eliminates irrelevant features\n",
    "- **Creates simpler, more interpretable models**\n",
    "\n",
    "**Pros:**\n",
    "- Performs automatic feature selection\n",
    "- Creates sparse models (many coefficients = 0)\n",
    "- More interpretable than Ridge (fewer features)\n",
    "- Works well when only some features are important\n",
    "\n",
    "**Cons:**\n",
    "- Can be unstable with highly correlated features\n",
    "- Arbitrarily picks one feature from correlated group\n",
    "- Requires hyperparameter tuning\n",
    "- Requires feature scaling\n",
    "\n",
    "**When to Use:**\n",
    "- When you suspect many features are irrelevant\n",
    "- When you want automatic feature selection\n",
    "- When interpretability is important (fewer features)\n",
    "- When you need a sparse model\n",
    "\n",
    "**Ridge vs Lasso:**\n",
    "- **Ridge:** Keeps all features, shrinks coefficients toward zero\n",
    "- **Lasso:** Eliminates features, sets coefficients to exactly zero\n",
    "- **Ridge:** Better when most features are useful\n",
    "- **Lasso:** Better when many features are irrelevant\n",
    "\n",
    "**ğŸ“– References:**\n",
    "- [Scikit-learn: Lasso Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "- [ISL Book - Chapter 6: Lasso Regression](https://www.statlearning.com/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Lasso Regression\n",
    "print(\"ğŸš€ Training Lasso Regression (L1 Regularization)...\\n\")\n",
    "\n",
    "lasso_model = Lasso(alpha=1.0, random_state=42, max_iter=10000)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Count non-zero coefficients\n",
    "n_nonzero = np.sum(lasso_model.coef_ != 0)\n",
    "\n",
    "print(f\"âœ… Lasso model trained with Î± = 1.0\")\n",
    "print(f\"\\nğŸ¯ Feature Selection Results:\")\n",
    "print(f\"   Total features: {len(lasso_model.coef_)}\")\n",
    "print(f\"   Non-zero coefficients: {n_nonzero}\")\n",
    "print(f\"   Features eliminated: {len(lasso_model.coef_) - n_nonzero}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Lasso Coefficients (Non-Zero Features):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lasso_coef_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Lasso_Coef': lasso_model.coef_\n",
    "}).query('Lasso_Coef != 0').sort_values('Lasso_Coef', key=abs, ascending=False)\n",
    "\n",
    "print(lasso_coef_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Lasso on validation set\n",
    "y_pred_lasso = lasso_model.predict(X_val_scaled)\n",
    "\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_val, y_pred_lasso))\n",
    "lasso_mae = mean_absolute_error(y_val, y_pred_lasso)\n",
    "lasso_r2 = r2_score(y_val, y_pred_lasso)\n",
    "\n",
    "print(\"ğŸ“Š Lasso Regression - Validation Set Performance:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Root Mean Squared Error (RMSE): ${lasso_rmse:>12,.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE):      ${lasso_mae:>12,.2f}\")\n",
    "print(f\"RÂ² Score:                       {lasso_r2:>12.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearchCV\n",
    "print(\"ğŸ¯ Tuning Lasso hyperparameter (alpha) with GridSearchCV...\\n\")\n",
    "\n",
    "param_grid_lasso = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1.0, 10, 100]\n",
    "}\n",
    "\n",
    "lasso_grid = GridSearchCV(\n",
    "    Lasso(random_state=42, max_iter=10000),\n",
    "    param_grid_lasso,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lasso_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nâœ… Best parameters: {lasso_grid.best_params_}\")\n",
    "print(f\"ğŸ“Š Best CV RMSE: ${-lasso_grid.best_score_:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best Lasso model\n",
    "best_lasso = lasso_grid.best_estimator_\n",
    "y_pred_lasso_tuned = best_lasso.predict(X_val_scaled)\n",
    "\n",
    "lasso_tuned_rmse = np.sqrt(mean_squared_error(y_val, y_pred_lasso_tuned))\n",
    "lasso_tuned_mae = mean_absolute_error(y_val, y_pred_lasso_tuned)\n",
    "lasso_tuned_r2 = r2_score(y_val, y_pred_lasso_tuned)\n",
    "\n",
    "n_nonzero_tuned = np.sum(best_lasso.coef_ != 0)\n",
    "\n",
    "print(\"ğŸ“Š Lasso (Tuned) - Validation Set Performance:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Root Mean Squared Error (RMSE): ${lasso_tuned_rmse:>12,.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE):      ${lasso_tuned_mae:>12,.2f}\")\n",
    "print(f\"RÂ² Score:                       {lasso_tuned_r2:>12.4f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ¯ Feature Selection with Best Alpha ({lasso_grid.best_params_['alpha']}):\")\n",
    "print(f\"   Selected features: {n_nonzero_tuned}/{len(best_lasso.coef_)}\")\n",
    "\n",
    "# Show selected features\n",
    "selected_features = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': best_lasso.coef_\n",
    "}).query('Coefficient != 0').sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(f\"\\nğŸ“‹ Selected Features:\")\n",
    "print(selected_features.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Model 4: ElasticNet (L1 + L2 Regularization)\n",
    "\n",
    "**ğŸ“š Theory:**  \n",
    "ElasticNet combines the **best of both Ridge and Lasso** by using both L1 and L2 penalties.\n",
    "\n",
    "**Cost Function:**\n",
    "$$\\text{Minimize: } SSE + \\alpha \\rho \\sum_{j=1}^{n} |\\beta_j| + \\frac{\\alpha(1-\\rho)}{2} \\sum_{j=1}^{n} \\beta_j^2$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = Overall regularization strength\n",
    "- $\\rho$ (l1_ratio) = Balance between L1 and L2 (0 to 1)\n",
    "  - $\\rho = 0$: Pure Ridge\n",
    "  - $\\rho = 1$: Pure Lasso\n",
    "  - $0 < \\rho < 1$: Combination\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Hybrid approach:** Combines L1 sparsity with L2 stability\n",
    "- **Two hyperparameters:** $\\alpha$ and $\\rho$\n",
    "- **Feature selection:** Like Lasso, but more stable\n",
    "- **Handles correlated features:** Better than Lasso\n",
    "\n",
    "**Pros:**\n",
    "- Gets benefits of both Ridge and Lasso\n",
    "- More stable than Lasso with correlated features\n",
    "- Can select groups of correlated features (unlike Lasso)\n",
    "- Performs feature selection (unlike Ridge)\n",
    "\n",
    "**Cons:**\n",
    "- Two hyperparameters to tune (more complex)\n",
    "- Computationally more expensive\n",
    "- Less interpretable than pure Lasso or Ridge\n",
    "\n",
    "**When to Use:**\n",
    "- When you have many correlated features\n",
    "- When Lasso is too unstable\n",
    "- When you want feature selection AND stability\n",
    "- When you're unsure whether to use Ridge or Lasso\n",
    "\n",
    "**Recommended l1_ratio Values:**\n",
    "- 0.5: Balanced combination (common default)\n",
    "- 0.9: More Lasso-like (more feature selection)\n",
    "- 0.1: More Ridge-like (less feature selection)\n",
    "\n",
    "**ğŸ“– References:**\n",
    "- [Scikit-learn: ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)\n",
    "- [Original Paper: Zou & Hastie (2005)](https://www.jstor.org/stable/3647580)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ElasticNet\n",
    "print(\"ğŸš€ Training ElasticNet (L1 + L2 Regularization)...\\n\")\n",
    "\n",
    "elastic_model = ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42, max_iter=10000)\n",
    "elastic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "n_nonzero_elastic = np.sum(elastic_model.coef_ != 0)\n",
    "\n",
    "print(f\"âœ… ElasticNet trained with Î± = 1.0, l1_ratio = 0.5\")\n",
    "print(f\"\\nğŸ¯ Feature Selection Results:\")\n",
    "print(f\"   Selected features: {n_nonzero_elastic}/{len(elastic_model.coef_)}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred_elastic = elastic_model.predict(X_val_scaled)\n",
    "\n",
    "elastic_rmse = np.sqrt(mean_squared_error(y_val, y_pred_elastic))\n",
    "elastic_mae = mean_absolute_error(y_val, y_pred_elastic)\n",
    "elastic_r2 = r2_score(y_val, y_pred_elastic)\n",
    "\n",
    "print(\"\\nğŸ“Š ElasticNet - Validation Set Performance:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Root Mean Squared Error (RMSE): ${elastic_rmse:>12,.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE):      ${elastic_mae:>12,.2f}\")\n",
    "print(f\"RÂ² Score:                       {elastic_r2:>12.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearchCV (alpha AND l1_ratio)\n",
    "print(\"ğŸ¯ Tuning ElasticNet hyperparameters with GridSearchCV...\\n\")\n",
    "print(\"â³ This may take longer (2 parameters to tune)...\\n\")\n",
    "\n",
    "param_grid_elastic = {\n",
    "    'alpha': [0.01, 0.1, 1.0, 10],\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "elastic_grid = GridSearchCV(\n",
    "    ElasticNet(random_state=42, max_iter=10000),\n",
    "    param_grid_elastic,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "elastic_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nâœ… Best parameters: {elastic_grid.best_params_}\")\n",
    "print(f\"ğŸ“Š Best CV RMSE: ${-elastic_grid.best_score_:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best ElasticNet model\n",
    "best_elastic = elastic_grid.best_estimator_\n",
    "y_pred_elastic_tuned = best_elastic.predict(X_val_scaled)\n",
    "\n",
    "elastic_tuned_rmse = np.sqrt(mean_squared_error(y_val, y_pred_elastic_tuned))\n",
    "elastic_tuned_mae = mean_absolute_error(y_val, y_pred_elastic_tuned)\n",
    "elastic_tuned_r2 = r2_score(y_val, y_pred_elastic_tuned)\n",
    "\n",
    "n_nonzero_elastic_tuned = np.sum(best_elastic.coef_ != 0)\n",
    "\n",
    "print(\"ğŸ“Š ElasticNet (Tuned) - Validation Set Performance:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Root Mean Squared Error (RMSE): ${elastic_tuned_rmse:>12,.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE):      ${elastic_tuned_mae:>12,.2f}\")\n",
    "print(f\"RÂ² Score:                       {elastic_tuned_r2:>12.4f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ¯ Best Hyperparameters:\")\n",
    "print(f\"   Alpha: {elastic_grid.best_params_['alpha']}\")\n",
    "print(f\"   L1 Ratio: {elastic_grid.best_params_['l1_ratio']}\")\n",
    "print(f\"   Selected features: {n_nonzero_elastic_tuned}/{len(best_elastic.coef_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Model Comparison\n",
    "\n",
    "**What you need to do:**  \n",
    "Compare all linear models side-by-side to identify the best performer.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create summary table of all models\n",
    "2. Compare metrics: RMSE, MAE, RÂ²\n",
    "3. Visualize performance comparison\n",
    "4. Select best model for final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Linear Regression (OLS)',\n",
    "        'Ridge (default)',\n",
    "        'Ridge (tuned)',\n",
    "        'Lasso (default)',\n",
    "        'Lasso (tuned)',\n",
    "        'ElasticNet (default)',\n",
    "        'ElasticNet (tuned)'\n",
    "    ],\n",
    "    'RMSE': [\n",
    "        lr_rmse, ridge_rmse, ridge_tuned_rmse,\n",
    "        lasso_rmse, lasso_tuned_rmse,\n",
    "        elastic_rmse, elastic_tuned_rmse\n",
    "    ],\n",
    "    'MAE': [\n",
    "        lr_mae, ridge_mae, ridge_tuned_mae,\n",
    "        lasso_mae, lasso_tuned_mae,\n",
    "        elastic_mae, elastic_tuned_mae\n",
    "    ],\n",
    "    'RÂ²': [\n",
    "        lr_r2, ridge_r2, ridge_tuned_r2,\n",
    "        lasso_r2, lasso_tuned_r2,\n",
    "        elastic_r2, elastic_tuned_r2\n",
    "    ],\n",
    "    'Features Used': [\n",
    "        len(X_train.columns),\n",
    "        len(X_train.columns),\n",
    "        len(X_train.columns),\n",
    "        n_nonzero,\n",
    "        n_nonzero_tuned,\n",
    "        n_nonzero_elastic,\n",
    "        n_nonzero_elastic_tuned\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Sort by RMSE (lower is better)\n",
    "comparison_df = comparison_df.sort_values('RMSE')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š LINEAR MODELS COMPARISON - VALIDATION SET PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "print(f\"\\nğŸ† BEST MODEL: {best_model_name}\")\n",
    "print(f\"   RMSE: ${comparison_df.iloc[0]['RMSE']:,.2f}\")\n",
    "print(f\"   RÂ²: {comparison_df.iloc[0]['RÂ²']:.4f}\")\n",
    "print(f\"   Features: {comparison_df.iloc[0]['Features Used']:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0].barh(comparison_df['Model'], comparison_df['RMSE'], color='steelblue')\n",
    "axes[0].set_xlabel('RMSE ($)', fontsize=11)\n",
    "axes[0].set_title('Model Comparison: RMSE\\n(Lower is Better)', fontsize=12, pad=15)\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# MAE comparison\n",
    "axes[1].barh(comparison_df['Model'], comparison_df['MAE'], color='coral')\n",
    "axes[1].set_xlabel('MAE ($)', fontsize=11)\n",
    "axes[1].set_title('Model Comparison: MAE\\n(Lower is Better)', fontsize=12, pad=15)\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "# RÂ² comparison\n",
    "axes[2].barh(comparison_df['Model'], comparison_df['RÂ²'], color='seagreen')\n",
    "axes[2].set_xlabel('RÂ² Score', fontsize=11)\n",
    "axes[2].set_title('Model Comparison: RÂ²\\n(Higher is Better)', fontsize=12, pad=15)\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Final Evaluation on Test Set\n",
    "\n",
    "**âš ï¸ CRITICAL: This is your ONE AND ONLY test set evaluation!**\n",
    "\n",
    "**Theory:**  \n",
    "The test set provides an unbiased estimate of production performance. This is your final report card.\n",
    "\n",
    "**What you need to do:**  \n",
    "Evaluate the best model on the held-out test set.\n",
    "\n",
    "**Tasks:**\n",
    "1. Select best model from validation performance\n",
    "2. Make predictions on test set\n",
    "3. Calculate final metrics\n",
    "4. Visualize predictions vs actual\n",
    "5. Analyze residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model (based on validation RMSE)\n",
    "if best_model_name == 'Ridge (tuned)':\n",
    "    final_model = best_ridge\n",
    "    X_test_final = X_test_scaled\n",
    "elif best_model_name == 'Lasso (tuned)':\n",
    "    final_model = best_lasso\n",
    "    X_test_final = X_test_scaled\n",
    "elif best_model_name == 'ElasticNet (tuned)':\n",
    "    final_model = best_elastic\n",
    "    X_test_final = X_test_scaled\n",
    "else:\n",
    "    final_model = lr_model\n",
    "    X_test_final = X_test\n",
    "\n",
    "print(f\"ğŸ† Selected Model: {best_model_name}\")\n",
    "print(f\"\\nğŸ”“ Unlocking test set for final evaluation...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final predictions on test set\n",
    "y_pred_test = final_model.predict(X_test_final)\n",
    "\n",
    "# Calculate final metrics\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"ğŸ“Š FINAL TEST SET PERFORMANCE: {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Root Mean Squared Error (RMSE): ${test_rmse:>12,.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE):      ${test_mae:>12,.2f}\")\n",
    "print(f\"RÂ² Score:                       {test_r2:>12.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare to validation performance\n",
    "val_rmse = comparison_df.iloc[0]['RMSE']\n",
    "val_r2 = comparison_df.iloc[0]['RÂ²']\n",
    "\n",
    "print(f\"\\nğŸ” Validation vs Test Comparison:\")\n",
    "print(f\"   Validation RMSE: ${val_rmse:,.2f}  â†’  Test RMSE: ${test_rmse:,.2f}\")\n",
    "print(f\"   Validation RÂ²: {val_r2:.4f}  â†’  Test RÂ²: {test_r2:.4f}\")\n",
    "\n",
    "if abs(test_r2 - val_r2) < 0.05:\n",
    "    print(f\"\\nâœ… Excellent! Test performance is consistent with validation.\")\n",
    "elif test_r2 < val_r2 - 0.05:\n",
    "    print(f\"\\nâš ï¸ Warning: Test performance is worse than validation. Possible overfitting.\")\n",
    "else:\n",
    "    print(f\"\\nğŸ‰ Great! Test performance exceeds validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot: Predicted vs Actual\n",
    "axes[0].scatter(y_test, y_pred_test, alpha=0.5, s=20)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Total Sales ($)', fontsize=11)\n",
    "axes[0].set_ylabel('Predicted Total Sales ($)', fontsize=11)\n",
    "axes[0].set_title(f'{best_model_name}\\nPredictions vs Actual (Test Set)', fontsize=12, pad=15)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_test - y_pred_test\n",
    "axes[1].scatter(y_pred_test, residuals, alpha=0.5, s=20)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Total Sales ($)', fontsize=11)\n",
    "axes[1].set_ylabel('Residuals ($)', fontsize=11)\n",
    "axes[1].set_title('Residual Plot\\n(Should be randomly scattered around zero)', fontsize=12, pad=15)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Good residual plot: Points randomly scattered around zero (no patterns).\")\n",
    "print(\"   Patterns in residuals suggest the model is missing important relationships.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(residuals.mean(), color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: ${residuals.mean():.2f}')\n",
    "plt.axvline(residuals.median(), color='green', linestyle='--', linewidth=2,\n",
    "            label=f'Median: ${residuals.median():.2f}')\n",
    "plt.xlabel('Residuals ($)', fontsize=11)\n",
    "plt.ylabel('Frequency', fontsize=11)\n",
    "plt.title('Distribution of Residuals (Test Set)', fontsize=12, pad=15)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ“Š Residual Statistics:\")\n",
    "print(f\"   Mean: ${residuals.mean():.2f} (should be close to 0)\")\n",
    "print(f\"   Std Dev: ${residuals.std():.2f}\")\n",
    "print(f\"   Min: ${residuals.min():.2f}\")\n",
    "print(f\"   Max: ${residuals.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Key Takeaways & Model Insights\n",
    "\n",
    "**What you should have learned:**\n",
    "\n",
    "### 1ï¸âƒ£ Linear Model Family\n",
    "âœ… **Linear Regression (OLS)**\n",
    "- Simple, fast, interpretable baseline\n",
    "- No regularization = risk of overfitting\n",
    "- Use when: few features, interpretability matters\n",
    "\n",
    "âœ… **Ridge Regression (L2)**\n",
    "- Shrinks coefficients, keeps all features\n",
    "- Good for correlated features\n",
    "- Use when: all features are relevant\n",
    "\n",
    "âœ… **Lasso Regression (L1)**\n",
    "- Performs feature selection (sets coefs to zero)\n",
    "- Creates sparse models\n",
    "- Use when: many irrelevant features\n",
    "\n",
    "âœ… **ElasticNet (L1 + L2)**\n",
    "- Best of both worlds\n",
    "- More stable than Lasso with correlated features\n",
    "- Use when: unsure between Ridge/Lasso\n",
    "\n",
    "### 2ï¸âƒ£ Complete ML Workflow\n",
    "âœ… Proper data splitting (train/val/test)\n",
    "âœ… Feature engineering and EDA\n",
    "âœ… Feature scaling for regularized models\n",
    "âœ… Cross-validation for robust evaluation\n",
    "âœ… Hyperparameter tuning with GridSearchCV\n",
    "âœ… Final test set evaluation\n",
    "\n",
    "### 3ï¸âƒ£ Model Selection Criteria\n",
    "- **Accuracy:** Which model has lowest RMSE/MAE?\n",
    "- **Simplicity:** How many features does it use?\n",
    "- **Interpretability:** Can we explain predictions?\n",
    "- **Stability:** Is performance consistent across CV folds?\n",
    "- **Generalization:** Does validation match test performance?\n",
    "\n",
    "### 4ï¸âƒ£ Real-World Insights\n",
    "- Linear models work well when relationships are approximately linear\n",
    "- Feature engineering is crucial (we created meaningful features from raw data)\n",
    "- Regularization helps prevent overfitting\n",
    "- Feature scaling matters for regularized models\n",
    "- Always validate on unseen data (test set)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Reflection Questions\n",
    "1. Which linear model performed best on our e-commerce data? Why?\n",
    "2. How did regularization affect the model coefficients?\n",
    "3. Which features were most important for predicting sales?\n",
    "4. How does feature scaling affect regularized models?\n",
    "5. When would you choose Lasso over Ridge?\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ Next Steps: Week 9 Session 2\n",
    "**Tree-Based Models:**\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- Gradient Boosting (XGBoost, LightGBM)\n",
    "\n",
    "**Questions to explore:**\n",
    "- Can non-linear models outperform linear models on this dataset?\n",
    "- How do tree-based models handle feature interactions?\n",
    "- What are the trade-offs between linear and tree-based models?\n",
    "\n",
    "---\n",
    "\n",
    "**AI Tech Institute** | *Building Tomorrow's AI Engineers Today*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
