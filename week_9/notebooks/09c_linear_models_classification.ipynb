{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 10 Session 1: Linear Models for Classification\n",
    "**Instructor:** Amir Charkhi | **Goal:** Master Linear Classification Models\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand logistic regression and linear classification\n",
    "- Learn classification metrics: accuracy, precision, recall, F1, ROC-AUC\n",
    "- Master confusion matrix interpretation\n",
    "- Apply regularization for classification: Ridge and Lasso equivalents\n",
    "- Handle imbalanced classes\n",
    "- Compare classification models systematically\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "**What you need to do:**  \n",
    "Import all necessary libraries for classification tasks.\n",
    "\n",
    "**Required imports:**\n",
    "- NumPy and Pandas for data handling\n",
    "- Matplotlib and Seaborn for visualization\n",
    "- Scikit-learn for classification models and metrics\n",
    "\n",
    "**üí° Hint:** We'll need `LogisticRegression`, `RidgeClassifier`, `SGDClassifier`, and classification metrics like `classification_report`, `confusion_matrix`, `roc_auc_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load the Dataset\n",
    "\n",
    "**What you need to do:**  \n",
    "Load the Online Shoppers Purchasing Intention dataset from UCI.\n",
    "\n",
    "**Theory:**  \n",
    "This dataset contains **12,330 sessions** from an online shopping website. Each session belongs to a different user over a 1-year period. The goal is to predict whether a visitor will make a purchase (Revenue: True/False).\n",
    "\n",
    "**Features:**\n",
    "- **Administrative, Administrative_Duration:** Pages and time on admin pages\n",
    "- **Informational, Informational_Duration:** Pages and time on info pages\n",
    "- **ProductRelated, ProductRelated_Duration:** Pages and time on product pages\n",
    "- **BounceRates, ExitRates, PageValues:** Website metrics\n",
    "- **SpecialDay:** Closeness to special days (Valentine's, Mother's Day, etc.)\n",
    "- **Month:** Month of the year\n",
    "- **OperatingSystems, Browser, Region, TrafficType:** Technical attributes\n",
    "- **VisitorType:** New, Returning, or Other\n",
    "- **Weekend:** Whether the session was on a weekend\n",
    "- **Revenue:** Target variable (True/False) - Did the user make a purchase?\n",
    "\n",
    "**Our Goal:** Predict **Revenue** (purchase or not) - **Binary Classification**\n",
    "\n",
    "**üí° Hint:** We'll load directly from UCI or use a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Online Shoppers Purchasing Intention dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv'\n",
    "\n",
    "print(\"üì• Loading Online Shoppers dataset from UCI...\")\n",
    "print(\"This dataset predicts whether a visitor will make a purchase.\\n\")\n",
    "\n",
    "# Load the data\n",
    "df_raw = pd.read_csv(url)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"üìä Shape: {df_raw.shape[0]:,} rows √ó {df_raw.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Initial Data Inspection & Preprocessing\n",
    "\n",
    "**What you need to do:**  \n",
    "Inspect the raw data and prepare it for classification.\n",
    "\n",
    "**Tasks:**\n",
    "1. Display first few rows\n",
    "2. Check data types and missing values\n",
    "3. Examine target variable distribution (class imbalance?)\n",
    "4. Encode categorical variables\n",
    "5. Handle any data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect raw data\n",
    "print(\"üìã First few rows:\")\n",
    "print(df_raw.head())\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"üîç Data Info:\")\n",
    "print(df_raw.info())\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"‚ùì Missing Values:\")\n",
    "print(df_raw.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine target variable\n",
    "print(\"üéØ Target Variable Distribution:\")\n",
    "print(\"=\"*60)\n",
    "print(df_raw['Revenue'].value_counts())\n",
    "print(\"\\nüìä Percentage:\")\n",
    "print(df_raw['Revenue'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "df_raw['Revenue'].value_counts().plot(kind='bar', ax=axes[0], color=['steelblue', 'coral'])\n",
    "axes[0].set_title('Target Variable Distribution', fontsize=12, pad=15)\n",
    "axes[0].set_xlabel('Revenue (Purchase Made)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['No Purchase', 'Purchase'], rotation=0)\n",
    "\n",
    "df_raw['Revenue'].value_counts(normalize=True).plot(kind='pie', ax=axes[1], \n",
    "                                                      autopct='%1.1f%%', colors=['steelblue', 'coral'])\n",
    "axes[1].set_title('Class Distribution (%)', fontsize=12, pad=15)\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for imbalance\n",
    "positive_pct = df_raw['Revenue'].value_counts(normalize=True)[True] * 100\n",
    "if positive_pct < 30:\n",
    "    print(f\"\\n‚ö†Ô∏è Class Imbalance Detected: Only {positive_pct:.1f}% positive class (Revenue=True)\")\n",
    "    print(\"We'll need to consider this when evaluating models.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Reasonably balanced: {positive_pct:.1f}% positive class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "print(\"üßπ Preprocessing data...\\n\")\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Convert target to binary (0/1)\n",
    "df['Revenue'] = df['Revenue'].astype(int)\n",
    "\n",
    "# Encode categorical variables\n",
    "print(\"üìù Encoding categorical variables...\")\n",
    "\n",
    "# Month encoding (ordinal makes sense here)\n",
    "month_map = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'June': 6,\n",
    "             'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}\n",
    "df['Month'] = df['Month'].map(month_map)\n",
    "\n",
    "# VisitorType: one-hot encoding\n",
    "visitor_dummies = pd.get_dummies(df['VisitorType'], prefix='Visitor', drop_first=True)\n",
    "visitor_dummies = visitor_dummies.astype(int)\n",
    "df = pd.concat([df, visitor_dummies], axis=1)\n",
    "\n",
    "# Weekend: already boolean, convert to int\n",
    "df['Weekend'] = df['Weekend'].astype(int)\n",
    "\n",
    "# Drop original categorical column\n",
    "df = df.drop(columns=['VisitorType'])\n",
    "\n",
    "# Verify all features are numeric\n",
    "feature_cols = [col for col in df.columns if col != 'Revenue']\n",
    "non_numeric = df[feature_cols].select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"‚úÖ Preprocessing complete!\")\n",
    "print(f\"üìä Final shape: {df.shape}\")\n",
    "print(f\"‚úÖ All features numeric: {len(non_numeric) == 0}\")\n",
    "print(f\"\\nüìã Feature columns ({len(feature_cols)}): {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Train-Validation-Test Split\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL: Split BEFORE detailed EDA to prevent data leakage!**\n",
    "\n",
    "**What you need to do:**  \n",
    "Split data: 60% train, 20% validation, 20% test\n",
    "\n",
    "**üí° Important for Classification:** Use `stratify` parameter to maintain class balance across splits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X = df[feature_cols].copy()\n",
    "y = df['Revenue'].copy()\n",
    "\n",
    "print(f\"üéØ Features: {len(feature_cols)} columns\")\n",
    "print(f\"üìä X shape: {X.shape}\")\n",
    "print(f\"üìä y shape: {y.shape}\")\n",
    "print(f\"\\nüìä Class distribution in full dataset:\")\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with stratification\n",
    "print(\"‚úÇÔ∏è Splitting data with stratification...\\n\")\n",
    "\n",
    "# First split: 80% train+val, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: 75% of temp = 60% train, 25% of temp = 20% val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data split complete with stratification!\")\n",
    "print(f\"\\nüìä Training set:   {X_train.shape[0]:>6,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"üìä Validation set: {X_val.shape[0]:>6,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"üìä Test set:       {X_test.shape[0]:>6,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify stratification worked\n",
    "print(f\"\\n‚úÖ Class distribution maintained:\")\n",
    "print(f\"   Train:      {y_train.value_counts(normalize=True)[1]:.3f} positive class\")\n",
    "print(f\"   Validation: {y_val.value_counts(normalize=True)[1]:.3f} positive class\")\n",
    "print(f\"   Test:       {y_test.value_counts(normalize=True)[1]:.3f} positive class\")\n",
    "\n",
    "print(f\"\\nüîí Test set is now LOCKED until final evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Perform EDA ONLY on the training set!**\n",
    "\n",
    "**What you need to do:**  \n",
    "Understand patterns in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by class\n",
    "print(\"üìä Training Set Summary Statistics by Class:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_data = X_train.copy()\n",
    "train_data['Revenue'] = y_train.values\n",
    "\n",
    "# Select a few key features to compare\n",
    "key_features = ['ProductRelated', 'ProductRelated_Duration', 'BounceRates', \n",
    "                'ExitRates', 'PageValues', 'Month']\n",
    "\n",
    "for feature in key_features:\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(train_data.groupby('Revenue')[feature].describe()[['mean', '50%', 'std']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key features by class\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    train_data[train_data['Revenue']==0][feature].hist(bins=30, alpha=0.6, \n",
    "                                                         label='No Purchase', ax=axes[idx], color='steelblue')\n",
    "    train_data[train_data['Revenue']==1][feature].hist(bins=30, alpha=0.6, \n",
    "                                                         label='Purchase', ax=axes[idx], color='coral')\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].set_title(f'{feature} Distribution by Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target\n",
    "# Use only numeric columns\n",
    "numeric_train = train_data.select_dtypes(include=[np.number])\n",
    "correlations = numeric_train.corr()['Revenue'].drop('Revenue').sort_values(ascending=False)\n",
    "\n",
    "print(\"üîó Top 10 Features Correlated with Revenue:\")\n",
    "print(\"=\"*60)\n",
    "for feature, corr in correlations.head(10).items():\n",
    "    print(f\"{feature:.<45} {corr:>8.4f}\")\n",
    "\n",
    "print(\"\\nüîó Bottom 5 Features:\")\n",
    "print(\"=\"*60)\n",
    "for feature, corr in correlations.tail(5).items():\n",
    "    print(f\"{feature:.<45} {corr:>8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "top_10_features = correlations.abs().head(10).index.tolist() + ['Revenue']\n",
    "correlation_matrix = numeric_train[top_10_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Correlation Heatmap: Top Features vs Revenue', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Feature Scaling\n",
    "\n",
    "**Theory:**  \n",
    "Linear classification models benefit from feature scaling, especially when using regularization.\n",
    "\n",
    "**What you need to do:**  \n",
    "Standardize features using StandardScaler.\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL:** Fit the scaler ONLY on training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "print(\"‚öñÔ∏è Scaling features...\\n\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Features scaled successfully!\")\n",
    "print(f\"\\nüìä Scaled training features - Mean: {X_train_scaled.mean():.6f}, Std: {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Helper Function: Classification Metrics\n",
    "\n",
    "**What you need to do:**  \n",
    "Create a reusable function to evaluate classification models.\n",
    "\n",
    "**Classification Metrics:**\n",
    "- **Accuracy:** Overall correctness\n",
    "- **Precision:** Of predicted positives, how many are actually positive?\n",
    "- **Recall (Sensitivity):** Of actual positives, how many did we find?\n",
    "- **F1-Score:** Harmonic mean of precision and recall\n",
    "- **ROC-AUC:** Area under ROC curve (probability ranking metric)\n",
    "- **Confusion Matrix:** Breakdown of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(model, X_train, y_train, X_val, y_val, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of a classification model.\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Probabilities (if available)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_val_prob = model.predict_proba(X_val)[:, 1]\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        y_val_prob = model.decision_function(X_val)\n",
    "    else:\n",
    "        y_val_prob = None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    val_precision = precision_score(y_val, y_val_pred)\n",
    "    val_recall = recall_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    \n",
    "    if y_val_prob is not None:\n",
    "        val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "    else:\n",
    "        val_auc = None\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"üìä {model_name} Performance:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Metric':<30} {'Training':>15} {'Validation':>15}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Accuracy':<30} {train_acc:>15.4f} {val_acc:>15.4f}\")\n",
    "    print(f\"{'Precision':<30} {'':>15} {val_precision:>15.4f}\")\n",
    "    print(f\"{'Recall':<30} {'':>15} {val_recall:>15.4f}\")\n",
    "    print(f\"{'F1-Score':<30} {'':>15} {val_f1:>15.4f}\")\n",
    "    if val_auc is not None:\n",
    "        print(f\"{'ROC-AUC':<30} {'':>15} {val_auc:>15.4f}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_val, y_val_pred)\n",
    "    print(f\"\\nüìä Confusion Matrix (Validation):\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"                 Predicted No  Predicted Yes\")\n",
    "    print(f\"Actual No        {cm[0,0]:>12}  {cm[0,1]:>13}\")\n",
    "    print(f\"Actual Yes       {cm[1,0]:>12}  {cm[1,1]:>13}\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    return {\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'val_precision': val_precision,\n",
    "        'val_recall': val_recall,\n",
    "        'val_f1': val_f1,\n",
    "        'val_auc': val_auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_val_pred': y_val_pred,\n",
    "        'y_val_prob': y_val_prob\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Helper function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Model 1: Logistic Regression\n",
    "\n",
    "**üìö Theory:**  \n",
    "Logistic Regression is the fundamental linear classification algorithm. Despite its name, it's a **classification** algorithm, not regression.\n",
    "\n",
    "**How It Works:**\n",
    "1. Computes linear combination: $z = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n$\n",
    "2. Applies sigmoid function: $P(y=1|x) = \\frac{1}{1 + e^{-z}}$\n",
    "3. Outputs probability between 0 and 1\n",
    "4. Predicts class based on threshold (usually 0.5)\n",
    "\n",
    "**Mathematical Form:**\n",
    "$$P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n)}}$$\n",
    "\n",
    "**Loss Function (Log Loss / Cross-Entropy):**\n",
    "$$L = -\\frac{1}{m} \\sum_{i=1}^{m} [y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]$$\n",
    "\n",
    "**Key Parameters:**\n",
    "- **C:** Inverse of regularization strength (higher C = less regularization)\n",
    "- **penalty:** Type of regularization ('l1', 'l2', 'elasticnet', 'none')\n",
    "- **solver:** Algorithm ('lbfgs', 'liblinear', 'saga')\n",
    "- **class_weight:** Handle imbalanced classes ('balanced' or dict)\n",
    "- **max_iter:** Maximum iterations for convergence\n",
    "\n",
    "**Pros:**\n",
    "- Fast training and prediction\n",
    "- Outputs calibrated probabilities\n",
    "- Interpretable coefficients\n",
    "- Works well with high-dimensional data\n",
    "- Supports regularization\n",
    "\n",
    "**Cons:**\n",
    "- Assumes linear decision boundary\n",
    "- Can't capture complex patterns\n",
    "- Sensitive to outliers\n",
    "- Requires feature scaling for regularization\n",
    "\n",
    "**When to Use:**\n",
    "- As a strong baseline\n",
    "- When you need probability estimates\n",
    "- When interpretability is important\n",
    "- For real-time prediction systems\n",
    "\n",
    "**üìñ References:**\n",
    "- [Scikit-learn: Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- [ISL Book - Chapter 4: Logistic Regression](https://www.statlearning.com/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"üöÄ Training Logistic Regression...\\n\")\n",
    "\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"‚úÖ Logistic Regression trained!\\n\")\n",
    "\n",
    "# Evaluate\n",
    "lr_results = evaluate_classifier(lr_model, X_train_scaled, y_train, \n",
    "                                  X_val_scaled, y_val, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance (Coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "lr_coef = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Coefficient': lr_model.coef_[0]\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"üéØ Logistic Regression Coefficients (Top 10):\")\n",
    "print(\"=\"*60)\n",
    "print(lr_coef.head(10).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_coef = lr_coef.head(15).sort_values('Coefficient')\n",
    "plt.barh(top_coef['Feature'], top_coef['Coefficient'])\n",
    "plt.xlabel('Coefficient Value', fontsize=11)\n",
    "plt.title('Logistic Regression: Top 15 Coefficients', fontsize=12, pad=15)\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Positive coefficients increase purchase probability.\")\n",
    "print(\"   Negative coefficients decrease purchase probability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve and Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and Precision-Recall curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_val, lr_results['y_val_prob'])\n",
    "axes[0].plot(fpr, tpr, linewidth=2, label=f\"ROC (AUC = {lr_results['val_auc']:.3f})\")\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=11)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=11)\n",
    "axes[0].set_title('ROC Curve', fontsize=12, pad=15)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_val, lr_results['y_val_prob'])\n",
    "avg_precision = average_precision_score(y_val, lr_results['y_val_prob'])\n",
    "axes[1].plot(recall, precision, linewidth=2, label=f\"PR (AP = {avg_precision:.3f})\")\n",
    "axes[1].axhline(y=y_val.mean(), color='k', linestyle='--', linewidth=1, label='Baseline')\n",
    "axes[1].set_xlabel('Recall', fontsize=11)\n",
    "axes[1].set_ylabel('Precision', fontsize=11)\n",
    "axes[1].set_title('Precision-Recall Curve', fontsize=12, pad=15)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° ROC-AUC: Good for balanced classes. Measures ranking ability.\")\n",
    "print(\"üí° PR Curve: Better for imbalanced classes. Shows precision-recall tradeoff.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "print(\"üîÑ Performing 5-Fold Cross-Validation...\\n\")\n",
    "\n",
    "cv_scores_acc = cross_val_score(lr_model, X_train_scaled, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "cv_scores_f1 = cross_val_score(lr_model, X_train_scaled, y_train, cv=5, scoring='f1', n_jobs=-1)\n",
    "cv_scores_auc = cross_val_score(lr_model, X_train_scaled, y_train, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "print(\"üìä Cross-Validation Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {cv_scores_acc.mean():.4f} (¬± {cv_scores_acc.std():.4f})\")\n",
    "print(f\"F1-Score: {cv_scores_f1.mean():.4f} (¬± {cv_scores_f1.std():.4f})\")\n",
    "print(f\"ROC-AUC:  {cv_scores_auc.mean():.4f} (¬± {cv_scores_auc.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "print(\"üéØ Tuning Logistic Regression hyperparameters...\\n\")\n",
    "\n",
    "param_grid_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    LogisticRegression(random_state=42, max_iter=1000),\n",
    "    param_grid_lr,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best parameters: {lr_grid.best_params_}\")\n",
    "print(f\"üìä Best CV ROC-AUC: {lr_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model\n",
    "best_lr = lr_grid.best_estimator_\n",
    "lr_tuned_results = evaluate_classifier(best_lr, X_train_scaled, y_train,\n",
    "                                        X_val_scaled, y_val, \"Logistic Regression (Tuned)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model 2: Ridge Classifier (L2 Regularization)\n",
    "\n",
    "**üìö Theory:**  \n",
    "Ridge Classifier converts classification to regression by encoding labels, then applies Ridge regression. It's fast and works well with many features.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Uses squared hinge loss (different from logistic loss)\n",
    "- L2 regularization like Ridge regression\n",
    "- Doesn't output probabilities (only class predictions)\n",
    "- Very fast training\n",
    "\n",
    "**Pros:**\n",
    "- Extremely fast\n",
    "- Good with high-dimensional data\n",
    "- Handles multicollinearity well\n",
    "\n",
    "**Cons:**\n",
    "- No probability estimates\n",
    "- Less common than Logistic Regression\n",
    "- Different loss function may not suit all problems\n",
    "\n",
    "**üìñ References:**\n",
    "- [Scikit-learn: Ridge Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ridge Classifier\n",
    "print(\"üöÄ Training Ridge Classifier...\\n\")\n",
    "\n",
    "ridge_clf = RidgeClassifier(random_state=42)\n",
    "ridge_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"‚úÖ Ridge Classifier trained!\\n\")\n",
    "\n",
    "# Evaluate (note: no probabilities available)\n",
    "y_train_pred_ridge = ridge_clf.predict(X_train_scaled)\n",
    "y_val_pred_ridge = ridge_clf.predict(X_val_scaled)\n",
    "\n",
    "train_acc_ridge = accuracy_score(y_train, y_train_pred_ridge)\n",
    "val_acc_ridge = accuracy_score(y_val, y_val_pred_ridge)\n",
    "val_precision_ridge = precision_score(y_val, y_val_pred_ridge)\n",
    "val_recall_ridge = recall_score(y_val, y_val_pred_ridge)\n",
    "val_f1_ridge = f1_score(y_val, y_val_pred_ridge)\n",
    "\n",
    "print(\"üìä Ridge Classifier Performance:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<30} {'Training':>15} {'Validation':>15}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Accuracy':<30} {train_acc_ridge:>15.4f} {val_acc_ridge:>15.4f}\")\n",
    "print(f\"{'Precision':<30} {'':>15} {val_precision_ridge:>15.4f}\")\n",
    "print(f\"{'Recall':<30} {'':>15} {val_recall_ridge:>15.4f}\")\n",
    "print(f\"{'F1-Score':<30} {'':>15} {val_f1_ridge:>15.4f}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚ö†Ô∏è Note: Ridge Classifier doesn't provide probability estimates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "print(\"üéØ Tuning Ridge Classifier...\\n\")\n",
    "\n",
    "param_grid_ridge = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1.0, 10, 100]\n",
    "}\n",
    "\n",
    "ridge_grid = GridSearchCV(\n",
    "    RidgeClassifier(random_state=42),\n",
    "    param_grid_ridge,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "ridge_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best parameters: {ridge_grid.best_params_}\")\n",
    "print(f\"üìä Best CV F1-Score: {ridge_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned Ridge\n",
    "best_ridge = ridge_grid.best_estimator_\n",
    "y_val_pred_ridge_tuned = best_ridge.predict(X_val_scaled)\n",
    "\n",
    "val_acc_ridge_tuned = accuracy_score(y_val, y_val_pred_ridge_tuned)\n",
    "val_precision_ridge_tuned = precision_score(y_val, y_val_pred_ridge_tuned)\n",
    "val_recall_ridge_tuned = recall_score(y_val, y_val_pred_ridge_tuned)\n",
    "val_f1_ridge_tuned = f1_score(y_val, y_val_pred_ridge_tuned)\n",
    "\n",
    "print(\"üìä Ridge Classifier (Tuned) Performance:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy:  {val_acc_ridge_tuned:.4f}\")\n",
    "print(f\"Precision: {val_precision_ridge_tuned:.4f}\")\n",
    "print(f\"Recall:    {val_recall_ridge_tuned:.4f}\")\n",
    "print(f\"F1-Score:  {val_f1_ridge_tuned:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Model 3: Linear SVC (Support Vector Classifier)\n",
    "\n",
    "**üìö Theory:**  \n",
    "Linear SVC finds the hyperplane that maximizes the margin between classes. It's based on Support Vector Machine theory but optimized for linear decision boundaries.\n",
    "\n",
    "**How It Works:**\n",
    "- Finds optimal separating hyperplane\n",
    "- Maximizes margin (distance to nearest points)\n",
    "- Only support vectors (points near boundary) matter\n",
    "- Uses hinge loss\n",
    "\n",
    "**Mathematical Objective:**\n",
    "$$\\min_{w,b} \\frac{1}{2}||w||^2 + C \\sum_{i=1}^{m} \\max(0, 1 - y_i(w^T x_i + b))$$\n",
    "\n",
    "**Key Parameters:**\n",
    "- **C:** Regularization parameter (inverse strength)\n",
    "- **loss:** 'hinge' or 'squared_hinge'\n",
    "- **penalty:** 'l1' or 'l2'\n",
    "- **dual:** Solve dual or primal problem\n",
    "\n",
    "**Pros:**\n",
    "- Effective in high dimensions\n",
    "- Memory efficient (only stores support vectors)\n",
    "- Works well when classes are separable\n",
    "\n",
    "**Cons:**\n",
    "- Sensitive to feature scaling\n",
    "- No probability estimates by default\n",
    "- Sensitive to C parameter\n",
    "\n",
    "**üìñ References:**\n",
    "- [Scikit-learn: Linear SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)\n",
    "- [ISL Book - Chapter 9: Support Vector Machines](https://www.statlearning.com/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear SVC\n",
    "print(\"üöÄ Training Linear SVC...\\n\")\n",
    "\n",
    "svc_model = LinearSVC(random_state=42, max_iter=2000, dual=False)\n",
    "svc_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"‚úÖ Linear SVC trained!\\n\")\n",
    "\n",
    "# Evaluate\n",
    "y_train_pred_svc = svc_model.predict(X_train_scaled)\n",
    "y_val_pred_svc = svc_model.predict(X_val_scaled)\n",
    "\n",
    "train_acc_svc = accuracy_score(y_train, y_train_pred_svc)\n",
    "val_acc_svc = accuracy_score(y_val, y_val_pred_svc)\n",
    "val_precision_svc = precision_score(y_val, y_val_pred_svc)\n",
    "val_recall_svc = recall_score(y_val, y_val_pred_svc)\n",
    "val_f1_svc = f1_score(y_val, y_val_pred_svc)\n",
    "\n",
    "# Get decision function for AUC\n",
    "y_val_decision = svc_model.decision_function(X_val_scaled)\n",
    "val_auc_svc = roc_auc_score(y_val, y_val_decision)\n",
    "\n",
    "print(\"üìä Linear SVC Performance:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<30} {'Training':>15} {'Validation':>15}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Accuracy':<30} {train_acc_svc:>15.4f} {val_acc_svc:>15.4f}\")\n",
    "print(f\"{'Precision':<30} {'':>15} {val_precision_svc:>15.4f}\")\n",
    "print(f\"{'Recall':<30} {'':>15} {val_recall_svc:>15.4f}\")\n",
    "print(f\"{'F1-Score':<30} {'':>15} {val_f1_svc:>15.4f}\")\n",
    "print(f\"{'ROC-AUC':<30} {'':>15} {val_auc_svc:>15.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "print(\"üéØ Tuning Linear SVC...\\n\")\n",
    "\n",
    "param_grid_svc = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'loss': ['hinge', 'squared_hinge']\n",
    "}\n",
    "\n",
    "svc_grid = GridSearchCV(\n",
    "    LinearSVC(random_state=42, max_iter=2000, dual=False),\n",
    "    param_grid_svc,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "svc_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best parameters: {svc_grid.best_params_}\")\n",
    "print(f\"üìä Best CV F1-Score: {svc_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned SVC\n",
    "best_svc = svc_grid.best_estimator_\n",
    "y_val_pred_svc_tuned = best_svc.predict(X_val_scaled)\n",
    "y_val_decision_tuned = best_svc.decision_function(X_val_scaled)\n",
    "\n",
    "val_acc_svc_tuned = accuracy_score(y_val, y_val_pred_svc_tuned)\n",
    "val_precision_svc_tuned = precision_score(y_val, y_val_pred_svc_tuned)\n",
    "val_recall_svc_tuned = recall_score(y_val, y_val_pred_svc_tuned)\n",
    "val_f1_svc_tuned = f1_score(y_val, y_val_pred_svc_tuned)\n",
    "val_auc_svc_tuned = roc_auc_score(y_val, y_val_decision_tuned)\n",
    "\n",
    "print(\"üìä Linear SVC (Tuned) Performance:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Accuracy:  {val_acc_svc_tuned:.4f}\")\n",
    "print(f\"Precision: {val_precision_svc_tuned:.4f}\")\n",
    "print(f\"Recall:    {val_recall_svc_tuned:.4f}\")\n",
    "print(f\"F1-Score:  {val_f1_svc_tuned:.4f}\")\n",
    "print(f\"ROC-AUC:   {val_auc_svc_tuned:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Model Comparison\n",
    "\n",
    "**What you need to do:**  \n",
    "Compare all linear classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Logistic Regression',\n",
    "        'Logistic Regression (Tuned)',\n",
    "        'Ridge Classifier',\n",
    "        'Ridge Classifier (Tuned)',\n",
    "        'Linear SVC',\n",
    "        'Linear SVC (Tuned)'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        lr_results['val_acc'],\n",
    "        lr_tuned_results['val_acc'],\n",
    "        val_acc_ridge,\n",
    "        val_acc_ridge_tuned,\n",
    "        val_acc_svc,\n",
    "        val_acc_svc_tuned\n",
    "    ],\n",
    "    'Precision': [\n",
    "        lr_results['val_precision'],\n",
    "        lr_tuned_results['val_precision'],\n",
    "        val_precision_ridge,\n",
    "        val_precision_ridge_tuned,\n",
    "        val_precision_svc,\n",
    "        val_precision_svc_tuned\n",
    "    ],\n",
    "    'Recall': [\n",
    "        lr_results['val_recall'],\n",
    "        lr_tuned_results['val_recall'],\n",
    "        val_recall_ridge,\n",
    "        val_recall_ridge_tuned,\n",
    "        val_recall_svc,\n",
    "        val_recall_svc_tuned\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        lr_results['val_f1'],\n",
    "        lr_tuned_results['val_f1'],\n",
    "        val_f1_ridge,\n",
    "        val_f1_ridge_tuned,\n",
    "        val_f1_svc,\n",
    "        val_f1_svc_tuned\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        lr_results['val_auc'],\n",
    "        lr_tuned_results['val_auc'],\n",
    "        np.nan,\n",
    "        np.nan,\n",
    "        val_auc_svc,\n",
    "        val_auc_svc_tuned\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Sort by F1-Score\n",
    "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"üìä LINEAR CLASSIFICATION MODELS COMPARISON - VALIDATION SET\")\n",
    "print(\"=\"*90)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   F1-Score: {comparison_df.iloc[0]['F1-Score']:.4f}\")\n",
    "print(f\"   Accuracy: {comparison_df.iloc[0]['Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Accuracy\n",
    "axes[0,0].barh(comparison_df['Model'], comparison_df['Accuracy'], color='steelblue')\n",
    "axes[0,0].set_xlabel('Accuracy')\n",
    "axes[0,0].set_title('Model Comparison: Accuracy')\n",
    "axes[0,0].invert_yaxis()\n",
    "\n",
    "# Precision\n",
    "axes[0,1].barh(comparison_df['Model'], comparison_df['Precision'], color='coral')\n",
    "axes[0,1].set_xlabel('Precision')\n",
    "axes[0,1].set_title('Model Comparison: Precision')\n",
    "axes[0,1].invert_yaxis()\n",
    "\n",
    "# Recall\n",
    "axes[1,0].barh(comparison_df['Model'], comparison_df['Recall'], color='seagreen')\n",
    "axes[1,0].set_xlabel('Recall')\n",
    "axes[1,0].set_title('Model Comparison: Recall')\n",
    "axes[1,0].invert_yaxis()\n",
    "\n",
    "# F1-Score\n",
    "axes[1,1].barh(comparison_df['Model'], comparison_df['F1-Score'], color='purple')\n",
    "axes[1,1].set_xlabel('F1-Score')\n",
    "axes[1,1].set_title('Model Comparison: F1-Score (Higher is Better)')\n",
    "axes[1,1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Final Evaluation on Test Set\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL: Test set evaluation for best model**\n",
    "\n",
    "**What you need to do:**  \n",
    "Evaluate the best model on held-out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "if best_model_name == 'Logistic Regression (Tuned)':\n",
    "    final_model = best_lr\n",
    "elif best_model_name == 'Logistic Regression':\n",
    "    final_model = lr_model\n",
    "elif best_model_name == 'Ridge Classifier (Tuned)':\n",
    "    final_model = best_ridge\n",
    "elif best_model_name == 'Linear SVC (Tuned)':\n",
    "    final_model = best_svc\n",
    "else:\n",
    "    final_model = lr_model\n",
    "\n",
    "print(f\"üèÜ Selected Model: {best_model_name}\")\n",
    "print(f\"\\nüîì Unlocking test set for final evaluation...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final test set evaluation\n",
    "y_test_pred = final_model.predict(X_test_scaled)\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Get probabilities/decision function if available\n",
    "if hasattr(final_model, 'predict_proba'):\n",
    "    y_test_prob = final_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "elif hasattr(final_model, 'decision_function'):\n",
    "    y_test_decision = final_model.decision_function(X_test_scaled)\n",
    "    test_auc = roc_auc_score(y_test, y_test_decision)\n",
    "else:\n",
    "    test_auc = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üìä FINAL TEST SET PERFORMANCE: {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall:    {test_recall:.4f}\")\n",
    "print(f\"F1-Score:  {test_f1:.4f}\")\n",
    "if test_auc is not None:\n",
    "    print(f\"ROC-AUC:   {test_auc:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nüìä Detailed Classification Report (Test Set):\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_test_pred, target_names=['No Purchase', 'Purchase']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['No Purchase', 'Purchase'],\n",
    "            yticklabels=['No Purchase', 'Purchase'])\n",
    "plt.xlabel('Predicted Label', fontsize=11)\n",
    "plt.ylabel('True Label', fontsize=11)\n",
    "plt.title(f'Confusion Matrix: {best_model_name} (Test Set)', fontsize=12, pad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Confusion Matrix Breakdown:\")\n",
    "print(f\"   True Negatives:  {cm_test[0,0]:>6,} (Correct No Purchase)\")\n",
    "print(f\"   False Positives: {cm_test[0,1]:>6,} (Predicted Purchase, Actually No)\")\n",
    "print(f\"   False Negatives: {cm_test[1,0]:>6,} (Predicted No, Actually Purchase)\")\n",
    "print(f\"   True Positives:  {cm_test[1,1]:>6,} (Correct Purchase)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Key Takeaways & Classification Insights\n",
    "\n",
    "**What you should have learned:**\n",
    "\n",
    "### 1Ô∏è‚É£ Linear Classification Models\n",
    "\n",
    "‚úÖ **Logistic Regression**\n",
    "- Gold standard for binary classification\n",
    "- Outputs calibrated probabilities\n",
    "- Supports L1, L2, and ElasticNet regularization\n",
    "- Use when: You need probabilities or interpretability\n",
    "\n",
    "‚úÖ **Ridge Classifier**\n",
    "- Fast alternative to Logistic Regression\n",
    "- No probability estimates\n",
    "- Good with high-dimensional data\n",
    "- Use when: Speed matters, probabilities not needed\n",
    "\n",
    "‚úÖ **Linear SVC**\n",
    "- Based on Support Vector Machine theory\n",
    "- Maximizes margin between classes\n",
    "- Effective in high dimensions\n",
    "- Use when: Classes are well-separated\n",
    "\n",
    "### 2Ô∏è‚É£ Classification Metrics Deep Dive\n",
    "\n",
    "**Accuracy:** Overall correctness\n",
    "- Good for: Balanced classes\n",
    "- Bad for: Imbalanced classes\n",
    "\n",
    "**Precision:** How many predicted positives are correct?\n",
    "- Use when: False positives are costly\n",
    "- Example: Spam detection (don't want to mark real emails as spam)\n",
    "\n",
    "**Recall (Sensitivity):** How many actual positives did we find?\n",
    "- Use when: False negatives are costly\n",
    "- Example: Disease detection (don't want to miss sick patients)\n",
    "\n",
    "**F1-Score:** Balance of precision and recall\n",
    "- Use when: You want balance between precision/recall\n",
    "- Good for: Imbalanced classes\n",
    "\n",
    "**ROC-AUC:** Probability ranking metric\n",
    "- Use when: You care about ranking quality\n",
    "- Threshold-independent\n",
    "\n",
    "### 3Ô∏è‚É£ Important Classification Concepts\n",
    "\n",
    "**Class Imbalance:**\n",
    "- Our dataset: ~84% no purchase, ~16% purchase\n",
    "- Solutions: class_weight='balanced', SMOTE, threshold tuning\n",
    "- Metrics: Prefer F1, precision/recall, PR-AUC over accuracy\n",
    "\n",
    "**Threshold Tuning:**\n",
    "- Default: 0.5 probability threshold\n",
    "- Can adjust based on business needs\n",
    "- Higher threshold ‚Üí More precision, less recall\n",
    "- Lower threshold ‚Üí More recall, less precision\n",
    "\n",
    "**Stratification:**\n",
    "- Always use `stratify=y` in train_test_split\n",
    "- Maintains class distribution across splits\n",
    "- Critical for imbalanced datasets\n",
    "\n",
    "### 4Ô∏è‚É£ Model Selection for Classification\n",
    "\n",
    "**Start with Logistic Regression:**\n",
    "- Excellent baseline\n",
    "- Fast, interpretable, probabilities\n",
    "- Hard to beat on many problems\n",
    "\n",
    "**Try Ridge Classifier if:**\n",
    "- You need speed\n",
    "- You don't need probabilities\n",
    "- You have many features\n",
    "\n",
    "**Try Linear SVC if:**\n",
    "- Classes are well-separated\n",
    "- You want margin maximization\n",
    "- High-dimensional space\n",
    "\n",
    "### 5Ô∏è‚É£ Real-World Insights\n",
    "\n",
    "- **PageValues** was likely the most important feature (higher page values ‚Üí more likely to purchase)\n",
    "- **ProductRelated_Duration** matters (time spent on product pages)\n",
    "- **BounceRates and ExitRates** are negative indicators\n",
    "- Linear models work well for binary classification even with imbalanced data\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Reflection Questions\n",
    "1. Why is accuracy not the best metric for imbalanced classification?\n",
    "2. When would you prefer precision over recall?\n",
    "3. How does regularization affect classification boundaries?\n",
    "4. What's the difference between ROC-AUC and PR-AUC?\n",
    "5. Why might Logistic Regression outperform more complex models?\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Next Steps: Week 10 Session 2\n",
    "**Tree-Based Classification:**\n",
    "- Decision Trees for classification\n",
    "- Random Forests\n",
    "- Gradient Boosting\n",
    "- Feature importance\n",
    "- Handling non-linear decision boundaries\n",
    "\n",
    "---\n",
    "\n",
    "**AI Tech Institute** | *Building Tomorrow's AI Engineers Today*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
