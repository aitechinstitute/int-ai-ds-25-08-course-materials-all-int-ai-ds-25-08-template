{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 9 Session 2: Tree-Based Models for Regression\n",
    "**Instructor:** Amir Charkhi | **Goal:** Master Decision Trees and Ensemble Methods\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand decision tree fundamentals and splitting criteria\n",
    "- Learn ensemble methods: Random Forest and Gradient Boosting\n",
    "- Compare tree-based models with linear models\n",
    "- Master feature importance interpretation\n",
    "- Apply advanced hyperparameter tuning\n",
    "- Understand bias-variance tradeoff in practice\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "**What you need to do:**  \n",
    "Import all necessary libraries for tree-based modeling.\n",
    "\n",
    "**Required imports:**\n",
    "- NumPy and Pandas for data handling\n",
    "- Matplotlib and Seaborn for visualization\n",
    "- Scikit-learn for tree models and evaluation\n",
    "- XGBoost and LightGBM for advanced gradient boosting\n",
    "\n",
    "**üí° Hint:** We'll need `DecisionTreeRegressor`, `RandomForestRegressor`, `GradientBoostingRegressor`, and optionally `XGBRegressor` and `LGBMRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Advanced gradient boosting libraries\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    print(\"‚úÖ XGBoost available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è XGBoost not installed. Install with: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    print(\"‚úÖ LightGBM available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è LightGBM not installed. Install with: pip install lightgbm\")\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"\\n‚úÖ All core libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load and Prepare Dataset\n",
    "\n",
    "**What you need to do:**  \n",
    "Load the same Online Retail dataset we used for linear models.\n",
    "\n",
    "**Theory:**  \n",
    "We'll use the identical dataset to **fairly compare** tree-based models with linear models.\n",
    "\n",
    "**Our Goal:** Predict **TotalSales** and compare with linear model performance.\n",
    "\n",
    "**üí° Hint:** We'll reuse the same feature engineering pipeline for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Online Retail dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx'\n",
    "\n",
    "print(\"üì• Loading Online Retail dataset from UCI...\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "df_raw = pd.read_excel(url)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"üìä Shape: {df_raw.shape[0]:,} rows √ó {df_raw.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Cleaning & Feature Engineering\n",
    "\n",
    "**What you need to do:**  \n",
    "Apply the same preprocessing steps as the linear models notebook.\n",
    "\n",
    "**Note:** Tree-based models have different characteristics:\n",
    "- ‚úÖ Don't require feature scaling (scale-invariant)\n",
    "- ‚úÖ Handle non-linear relationships naturally\n",
    "- ‚úÖ Automatically capture feature interactions\n",
    "- ‚úÖ Robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "print(\"üßπ Cleaning data...\\n\")\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Remove missing CustomerIDs and cancellations\n",
    "df = df.dropna(subset=['CustomerID'])\n",
    "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
    "df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
    "\n",
    "# Create target variable\n",
    "df['TotalSales'] = df['Quantity'] * df['UnitPrice']\n",
    "\n",
    "# Extract time-based features\n",
    "df['Year'] = df['InvoiceDate'].dt.year\n",
    "df['Month'] = df['InvoiceDate'].dt.month\n",
    "df['DayOfWeek'] = df['InvoiceDate'].dt.dayofweek\n",
    "df['Hour'] = df['InvoiceDate'].dt.hour\n",
    "\n",
    "print(f\"‚úÖ Cleaned dataset: {df.shape[0]:,} transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering at Invoice Level\n",
    "print(\"üî® Engineering features...\\n\")\n",
    "\n",
    "invoice_features = df.groupby('InvoiceNo').agg({\n",
    "    'TotalSales': 'sum',\n",
    "    'Quantity': 'sum',\n",
    "    'UnitPrice': 'mean',\n",
    "    'StockCode': 'nunique',\n",
    "    'CustomerID': 'first',\n",
    "    'Country': 'first',\n",
    "    'Year': 'first',\n",
    "    'Month': 'first',\n",
    "    'DayOfWeek': 'first',\n",
    "    'Hour': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "invoice_features.rename(columns={\n",
    "    'Quantity': 'TotalItems',\n",
    "    'UnitPrice': 'AvgItemPrice',\n",
    "    'StockCode': 'NumUniqueProducts'\n",
    "}, inplace=True)\n",
    "\n",
    "invoice_features['AvgPricePerItem'] = invoice_features['TotalSales'] / invoice_features['TotalItems']\n",
    "\n",
    "# Country encoding\n",
    "top_countries = invoice_features['Country'].value_counts().head(5).index.tolist()\n",
    "invoice_features['Country_Group'] = invoice_features['Country'].apply(\n",
    "    lambda x: x if x in top_countries else 'Other'\n",
    ")\n",
    "country_dummies = pd.get_dummies(invoice_features['Country_Group'], prefix='Country', drop_first=True)\n",
    "invoice_features = pd.concat([invoice_features, country_dummies], axis=1)\n",
    "\n",
    "invoice_features['IsWeekend'] = (invoice_features['DayOfWeek'] >= 5).astype(int)\n",
    "\n",
    "print(f\"‚úÖ Created {invoice_features.shape[0]:,} invoice-level samples\")\n",
    "print(f\"üìä Total features: {invoice_features.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Train-Validation-Test Split\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL: Same split strategy as linear models for fair comparison**\n",
    "\n",
    "**What you need to do:**  \n",
    "Split data: 60% train, 20% validation, 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features - EXCLUDE original string columns\n",
    "feature_cols = [\n",
    "    'TotalItems', 'AvgItemPrice', 'NumUniqueProducts', \n",
    "    'AvgPricePerItem', 'Year', 'Month', 'DayOfWeek', \n",
    "    'Hour', 'IsWeekend'\n",
    "]\n",
    "\n",
    "# Add country dummy variables\n",
    "country_cols = [col for col in invoice_features.columns if col.startswith('Country_')]\n",
    "feature_cols.extend(country_cols)\n",
    "\n",
    "# Create feature matrix - this will automatically exclude Country and Country_Group\n",
    "X = invoice_features[feature_cols].copy()\n",
    "y = invoice_features['TotalSales'].copy()\n",
    "\n",
    "print(f\"üéØ Features: {len(feature_cols)} columns\")\n",
    "print(f\"üìä X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# Verify all columns are numeric\n",
    "print(f\"\\n‚úÖ All features are numeric: {X.select_dtypes(include=[np.number]).shape[1] == X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "print(\"‚úÇÔ∏è Splitting data...\\n\")\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"üìä Training:   {X_train.shape[0]:>6,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"üìä Validation: {X_val.shape[0]:>6,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"üìä Test:       {X_test.shape[0]:>6,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nüîí Test set locked until final evaluation!\")\n",
    "\n",
    "print(f\"\\nüí° Note: Tree-based models DON'T require feature scaling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß FIX: Remove string column and convert booleans to integers\n",
    "print(\"üîß Fixing data types...\\n\")\n",
    "\n",
    "# Drop the Country_Group column if it exists\n",
    "if 'Country_Group' in X_train.columns:\n",
    "    X_train = X_train.drop(columns=['Country_Group'])\n",
    "    X_val = X_val.drop(columns=['Country_Group'])\n",
    "    X_test = X_test.drop(columns=['Country_Group'])\n",
    "    print(\"‚úÖ Dropped Country_Group column\")\n",
    "\n",
    "# Convert boolean columns to int\n",
    "bool_cols = X_train.select_dtypes(include=['bool']).columns.tolist()\n",
    "if bool_cols:\n",
    "    X_train[bool_cols] = X_train[bool_cols].astype(int)\n",
    "    X_val[bool_cols] = X_val[bool_cols].astype(int)\n",
    "    X_test[bool_cols] = X_test[bool_cols].astype(int)\n",
    "    print(f\"‚úÖ Converted {len(bool_cols)} boolean columns to int\")\n",
    "\n",
    "# Verify fix\n",
    "print(f\"\\n‚úÖ Final verification:\")\n",
    "print(f\"   X_train shape: {X_train.shape}\")\n",
    "print(f\"   All numeric: {X_train.select_dtypes(include=[np.number]).shape[1] == X_train.shape[1]}\")\n",
    "print(f\"   No objects: {len(X_train.select_dtypes(include=['object']).columns) == 0}\")\n",
    "print(f\"   No booleans: {len(X_train.select_dtypes(include=['bool']).columns) == 0}\")\n",
    "\n",
    "print(f\"\\nüìä X_train dtypes after fix:\")\n",
    "print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Quick EDA Summary\n",
    "\n",
    "**What you need to do:**  \n",
    "Brief exploration of training data (detailed EDA was done in linear models notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick summary\n",
    "print(\"üìä Training Set Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(X_train.describe())\n",
    "print(\"\\nüéØ Target Variable:\")\n",
    "print(y_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model 1: Decision Tree Regressor\n",
    "\n",
    "**üìö Theory:**  \n",
    "Decision Trees make predictions by learning decision rules from features. They recursively partition the feature space into regions.\n",
    "\n",
    "**How Decision Trees Work:**\n",
    "1. Start at root node with all training data\n",
    "2. Find best feature and split point that minimizes error\n",
    "3. Create child nodes and repeat recursively\n",
    "4. Stop when reaching stopping criteria (max_depth, min_samples, etc.)\n",
    "5. Predict: average target value in leaf node\n",
    "\n",
    "**Splitting Criteria for Regression:**\n",
    "- **Mean Squared Error (MSE):** Most common, minimizes squared differences\n",
    "- **Mean Absolute Error (MAE):** More robust to outliers\n",
    "\n",
    "**Mathematical Form:**\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$$\n",
    "\n",
    "Where $\\bar{y}$ is the mean target value in that node.\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- **max_depth:** Maximum depth of tree (prevents overfitting)\n",
    "- **min_samples_split:** Minimum samples required to split node\n",
    "- **min_samples_leaf:** Minimum samples required in leaf node\n",
    "- **max_features:** Number of features to consider for best split\n",
    "- **min_impurity_decrease:** Minimum decrease in impurity required to split\n",
    "\n",
    "**Pros:**\n",
    "- Easy to understand and visualize\n",
    "- Handles non-linear relationships naturally\n",
    "- No feature scaling needed\n",
    "- Captures feature interactions automatically\n",
    "- Works with numerical and categorical features\n",
    "- Fast predictions\n",
    "\n",
    "**Cons:**\n",
    "- **Prone to overfitting** (high variance)\n",
    "- Unstable: small data changes ‚Üí different trees\n",
    "- Can create biased trees with imbalanced data\n",
    "- Not optimal for extrapolation\n",
    "\n",
    "**When to Use:**\n",
    "- As a baseline for tree-based models\n",
    "- When interpretability is critical\n",
    "- When you have non-linear relationships\n",
    "- As a building block for ensembles (Random Forest, Boosting)\n",
    "\n",
    "**Common Issue: Overfitting**\n",
    "- Unrestricted trees can memorize training data (100% training accuracy)\n",
    "- This leads to poor generalization on new data\n",
    "- Solution: Constrain tree growth with hyperparameters\n",
    "\n",
    "**üìñ References:**\n",
    "- [Scikit-learn: Decision Trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "- [ISL Book - Chapter 8: Tree-Based Methods](https://www.statlearning.com/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree (unrestricted)\n",
    "print(\"üå≥ Training Decision Tree Regressor (Unrestricted)...\\n\")\n",
    "\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Decision Tree trained!\")\n",
    "print(f\"\\nüìä Tree Structure:\")\n",
    "print(f\"   Max depth reached: {dt_model.get_depth()}\")\n",
    "print(f\"   Number of leaves: {dt_model.get_n_leaves()}\")\n",
    "print(f\"   Total nodes: {dt_model.tree_.node_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training and validation sets\n",
    "y_pred_train_dt = dt_model.predict(X_train)\n",
    "y_pred_val_dt = dt_model.predict(X_val)\n",
    "\n",
    "# Training metrics\n",
    "dt_train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train_dt))\n",
    "dt_train_r2 = r2_score(y_train, y_pred_train_dt)\n",
    "\n",
    "# Validation metrics\n",
    "dt_val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val_dt))\n",
    "dt_val_mae = mean_absolute_error(y_val, y_pred_val_dt)\n",
    "dt_val_r2 = r2_score(y_val, y_pred_val_dt)\n",
    "\n",
    "print(\"üìä Decision Tree Performance:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<30} {'Training':>15} {'Validation':>15}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'RMSE ($)':<30} ${dt_train_rmse:>14,.2f} ${dt_val_rmse:>14,.2f}\")\n",
    "print(f\"{'R¬≤ Score':<30} {dt_train_r2:>15.4f} {dt_val_r2:>15.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if dt_train_r2 > 0.95 and dt_val_r2 < 0.80:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: High training R¬≤ but lower validation R¬≤ indicates OVERFITTING!\")\n",
    "    print(\"The tree has memorized the training data.\")\n",
    "    print(\"Solution: Constrain tree growth with hyperparameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "dt_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': dt_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüéØ Decision Tree Feature Importance (Top 10):\")\n",
    "print(\"=\"*60)\n",
    "print(dt_feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = dt_feature_importance.head(10)\n",
    "plt.barh(top_features['Feature'], top_features['Importance'])\n",
    "plt.xlabel('Feature Importance', fontsize=11)\n",
    "plt.title('Decision Tree: Top 10 Feature Importance', fontsize=12, pad=15)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Higher importance = feature contributes more to predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "print(\"üîÑ Performing 5-Fold Cross-Validation...\\n\")\n",
    "\n",
    "cv_scores_dt = cross_val_score(\n",
    "    dt_model, X_train, y_train,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "cv_scores_dt = -cv_scores_dt\n",
    "\n",
    "print(\"üìä Cross-Validation RMSE:\")\n",
    "print(\"=\"*60)\n",
    "for fold, score in enumerate(cv_scores_dt, 1):\n",
    "    print(f\"Fold {fold}: ${score:,.2f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean CV RMSE:   ${cv_scores_dt.mean():,.2f} (¬± ${cv_scores_dt.std():.2f})\")\n",
    "print(f\"Validation RMSE: ${dt_val_rmse:,.2f}\")\n",
    "\n",
    "if cv_scores_dt.std() > cv_scores_dt.mean() * 0.2:\n",
    "    print(\"\\n‚ö†Ô∏è High variance in CV scores suggests model instability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "print(\"üéØ Tuning Decision Tree hyperparameters...\\n\")\n",
    "\n",
    "param_grid_dt = {\n",
    "    'max_depth': [3, 5, 7, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "dt_grid = GridSearchCV(\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    param_grid_dt,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "dt_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best parameters: {dt_grid.best_params_}\")\n",
    "print(f\"üìä Best CV RMSE: ${-dt_grid.best_score_:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model\n",
    "best_dt = dt_grid.best_estimator_\n",
    "y_pred_train_dt_tuned = best_dt.predict(X_train)\n",
    "y_pred_val_dt_tuned = best_dt.predict(X_val)\n",
    "\n",
    "dt_tuned_train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train_dt_tuned))\n",
    "dt_tuned_train_r2 = r2_score(y_train, y_pred_train_dt_tuned)\n",
    "dt_tuned_val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val_dt_tuned))\n",
    "dt_tuned_val_mae = mean_absolute_error(y_val, y_pred_val_dt_tuned)\n",
    "dt_tuned_val_r2 = r2_score(y_val, y_pred_val_dt_tuned)\n",
    "\n",
    "print(\"üìä Decision Tree (Tuned) Performance:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<30} {'Training':>15} {'Validation':>15}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'RMSE ($)':<30} ${dt_tuned_train_rmse:>14,.2f} ${dt_tuned_val_rmse:>14,.2f}\")\n",
    "print(f\"{'MAE ($)':<30} {'':>15} ${dt_tuned_val_mae:>14,.2f}\")\n",
    "print(f\"{'R¬≤ Score':<30} {dt_tuned_train_r2:>15.4f} {dt_tuned_val_r2:>15.4f}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Tree Structure (Tuned):\")\n",
    "print(f\"   Max depth: {best_dt.get_depth()}\")\n",
    "print(f\"   Number of leaves: {best_dt.get_n_leaves()}\")\n",
    "\n",
    "improvement = ((dt_val_rmse - dt_tuned_val_rmse) / dt_val_rmse) * 100\n",
    "print(f\"\\nüí° Improvement over default: {improvement:.1f}% reduction in RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Model 2: Random Forest Regressor\n",
    "\n",
    "**üìö Theory:**  \n",
    "Random Forest is an **ensemble method** that combines multiple decision trees to create a more robust and accurate model.\n",
    "\n",
    "**How Random Forest Works:**\n",
    "1. Create multiple decision trees (e.g., 100 trees)\n",
    "2. For each tree:\n",
    "   - **Bootstrap sampling:** Randomly sample training data with replacement\n",
    "   - **Random feature subset:** At each split, consider only random subset of features\n",
    "3. Each tree makes predictions independently\n",
    "4. Final prediction = **average** of all tree predictions\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Bagging (Bootstrap Aggregating):** Reduces variance by averaging predictions\n",
    "- **Random Subspace Method:** Each tree sees different feature subset ‚Üí decorrelates trees\n",
    "- **Out-of-Bag (OOB) Samples:** ~37% of data not used in each tree ‚Üí free validation set\n",
    "\n",
    "**Mathematical Form:**\n",
    "$$\\hat{y}_{RF} = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{y}_b(x)$$\n",
    "\n",
    "Where $B$ is the number of trees and $\\hat{y}_b$ is prediction from tree $b$.\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- **n_estimators:** Number of trees (more = better, but slower)\n",
    "- **max_depth:** Maximum depth of each tree\n",
    "- **min_samples_split:** Minimum samples to split node\n",
    "- **min_samples_leaf:** Minimum samples in leaf\n",
    "- **max_features:** Features to consider at each split ('sqrt', 'log2', or number)\n",
    "- **max_samples:** Size of bootstrap sample (None = 100%)\n",
    "\n",
    "**Typical Defaults (often work well):**\n",
    "- n_estimators: 100-500\n",
    "- max_features: 'sqrt' for classification, 1/3 of features for regression\n",
    "- max_depth: None (grow until pure leaves or min_samples_leaf)\n",
    "\n",
    "**Pros:**\n",
    "- **Reduces overfitting** compared to single decision tree\n",
    "- More stable and robust than single trees\n",
    "- Handles high-dimensional data well\n",
    "- Provides feature importance\n",
    "- Works with missing values (in some implementations)\n",
    "- Can estimate prediction uncertainty\n",
    "- Parallelizable (trees are independent)\n",
    "\n",
    "**Cons:**\n",
    "- Less interpretable than single tree\n",
    "- Slower training and prediction than single tree\n",
    "- Larger model size (memory)\n",
    "- Can still overfit with noisy data\n",
    "- Not great for extrapolation\n",
    "\n",
    "**When to Use:**\n",
    "- When single decision tree overfits\n",
    "- When you need robust, accurate predictions\n",
    "- When you have sufficient computational resources\n",
    "- As a strong baseline for tabular data\n",
    "- When you need feature importance rankings\n",
    "\n",
    "**Why Random Forest Works:**\n",
    "- **Wisdom of Crowds:** Averaging reduces variance\n",
    "- **Diversity:** Random sampling creates diverse trees\n",
    "- **Bias-Variance Tradeoff:** Trades small increase in bias for large decrease in variance\n",
    "\n",
    "**üìñ References:**\n",
    "- [Scikit-learn: Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "- [Original Paper: Breiman (2001)](https://link.springer.com/article/10.1023/A:1010933404324)\n",
    "- [ISL Book - Chapter 8: Random Forests](https://www.statlearning.com/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "print(\"üå≤üå≤üå≤ Training Random Forest Regressor...\\n\")\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n‚úÖ Random Forest trained with 100 trees!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest\n",
    "y_pred_train_rf = rf_model.predict(X_train)\n",
    "y_pred_val_rf = rf_model.predict(X_val)\n",
    "\n",
    "rf_train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train_rf))\n",
    "rf_train_r2 = r2_score(y_train, y_pred_train_rf)\n",
    "rf_val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val_rf))\n",
    "rf_val_mae = mean_absolute_error(y_val, y_pred_val_rf)\n",
    "rf_val_r2 = r2_score(y_val, y_pred_val_rf)\n",
    "\n",
    "print(\"üìä Random Forest Performance:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<30} {'Training':>15} {'Validation':>15}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'RMSE ($)':<30} ${rf_train_rmse:>14,.2f} ${rf_val_rmse:>14,.2f}\")\n",
    "print(f\"{'MAE ($)':<30} {'':>15} ${rf_val_mae:>14,.2f}\")\n",
    "print(f\"{'R¬≤ Score':<30} {rf_train_r2:>15.4f} {rf_val_r2:>15.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüí° Notice: Validation performance is much closer to training (less overfitting!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "rf_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"üéØ Random Forest Feature Importance (Top 10):\")\n",
    "print(\"=\"*60)\n",
    "print(rf_feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features_rf = rf_feature_importance.head(10)\n",
    "plt.barh(top_features_rf['Feature'], top_features_rf['Importance'], color='forestgreen')\n",
    "plt.xlabel('Feature Importance', fontsize=11)\n",
    "plt.title('Random Forest: Top 10 Feature Importance', fontsize=12, pad=15)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "print(\"üîÑ Performing 5-Fold Cross-Validation on Random Forest...\\n\")\n",
    "print(\"‚è≥ This may take a few minutes...\\n\")\n",
    "\n",
    "cv_scores_rf = cross_val_score(\n",
    "    rf_model, X_train, y_train,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "cv_scores_rf = -cv_scores_rf\n",
    "\n",
    "print(\"üìä Cross-Validation RMSE:\")\n",
    "print(\"=\"*60)\n",
    "for fold, score in enumerate(cv_scores_rf, 1):\n",
    "    print(f\"Fold {fold}: ${score:,.2f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean CV RMSE: ${cv_scores_rf.mean():,.2f} (¬± ${cv_scores_rf.std():.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with RandomizedSearchCV (faster than GridSearch)\n",
    "print(\"üéØ Tuning Random Forest hyperparameters...\\n\")\n",
    "print(\"Using RandomizedSearchCV for faster search...\\n\")\n",
    "\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', 0.5],\n",
    "    'max_samples': [0.7, 0.8, 0.9, None]\n",
    "}\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    param_dist_rf,\n",
    "    n_iter=20,  # Try 20 random combinations\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best parameters: {rf_random.best_params_}\")\n",
    "print(f\"üìä Best CV RMSE: ${-rf_random.best_score_:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned Random Forest\n",
    "best_rf = rf_random.best_estimator_\n",
    "y_pred_train_rf_tuned = best_rf.predict(X_train)\n",
    "y_pred_val_rf_tuned = best_rf.predict(X_val)\n",
    "\n",
    "rf_tuned_train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train_rf_tuned))\n",
    "rf_tuned_train_r2 = r2_score(y_train, y_pred_train_rf_tuned)\n",
    "rf_tuned_val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val_rf_tuned))\n",
    "rf_tuned_val_mae = mean_absolute_error(y_val, y_pred_val_rf_tuned)\n",
    "rf_tuned_val_r2 = r2_score(y_val, y_pred_val_rf_tuned)\n",
    "\n",
    "print(\"üìä Random Forest (Tuned) Performance:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<30} {'Training':>15} {'Validation':>15}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'RMSE ($)':<30} ${rf_tuned_train_rmse:>14,.2f} ${rf_tuned_val_rmse:>14,.2f}\")\n",
    "print(f\"{'MAE ($)':<30} {'':>15} ${rf_tuned_val_mae:>14,.2f}\")\n",
    "print(f\"{'R¬≤ Score':<30} {rf_tuned_train_r2:>15.4f} {rf_tuned_val_r2:>15.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Model 3: Gradient Boosting Regressor\n",
    "\n",
    "**üìö Theory:**  \n",
    "Gradient Boosting builds trees **sequentially**, where each new tree corrects errors made by previous trees.\n",
    "\n",
    "**How Gradient Boosting Works:**\n",
    "1. Start with simple model (constant prediction = mean)\n",
    "2. Calculate residuals (errors) from current model\n",
    "3. Fit new tree to predict these residuals\n",
    "4. Add new tree's predictions (scaled by learning rate) to ensemble\n",
    "5. Repeat steps 2-4 for N iterations\n",
    "6. Final prediction = sum of all tree predictions\n",
    "\n",
    "**Key Difference from Random Forest:**\n",
    "- **Random Forest:** Trees built independently in parallel (bagging)\n",
    "- **Gradient Boosting:** Trees built sequentially, each correcting previous errors (boosting)\n",
    "\n",
    "**Mathematical Form:**\n",
    "$$F_M(x) = F_0(x) + \\sum_{m=1}^{M} \\nu \\cdot h_m(x)$$\n",
    "\n",
    "Where:\n",
    "- $F_M(x)$ = Final prediction after M iterations\n",
    "- $F_0(x)$ = Initial prediction (usually mean)\n",
    "- $\\nu$ = Learning rate (shrinkage)\n",
    "- $h_m(x)$ = Prediction from tree $m$ fitted to residuals\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- **n_estimators:** Number of boosting iterations (trees)\n",
    "- **learning_rate:** Shrinkage factor (typically 0.01-0.3)\n",
    "  - Lower learning rate needs more trees but often performs better\n",
    "  - Rule of thumb: learning_rate √ó n_estimators ‚âà constant\n",
    "- **max_depth:** Tree depth (typically 3-8 for boosting, shallow trees work well)\n",
    "- **min_samples_split / min_samples_leaf:** Control tree complexity\n",
    "- **subsample:** Fraction of samples for each tree (0-1, introduces randomness)\n",
    "- **max_features:** Features to consider at each split\n",
    "\n",
    "**Typical Good Defaults:**\n",
    "- n_estimators: 100-1000 (more = better, but watch overfitting)\n",
    "- learning_rate: 0.1 (lower if using many trees)\n",
    "- max_depth: 3-5 (shallow trees work well)\n",
    "- subsample: 0.8 (adds randomness, reduces overfitting)\n",
    "\n",
    "**Pros:**\n",
    "- **Often best performance** on structured/tabular data\n",
    "- Captures complex non-linear relationships\n",
    "- Handles feature interactions naturally\n",
    "- Less prone to overfitting than deep trees (when tuned properly)\n",
    "- Feature importance available\n",
    "\n",
    "**Cons:**\n",
    "- **Sequential training** (not parallelizable like Random Forest)\n",
    "- Slower to train than Random Forest\n",
    "- More hyperparameters to tune\n",
    "- Can overfit if not careful (especially with high learning rate)\n",
    "- Less interpretable than single tree or Random Forest\n",
    "\n",
    "**When to Use:**\n",
    "- When you want best possible accuracy\n",
    "- When you have time for hyperparameter tuning\n",
    "- When training time is not critical\n",
    "- For Kaggle competitions and production systems\n",
    "\n",
    "**Gradient Boosting Variants:**\n",
    "- **Scikit-learn GradientBoosting:** Good baseline, well-tested\n",
    "- **XGBoost:** Faster, more features, industry standard\n",
    "- **LightGBM:** Very fast, memory efficient, handles large datasets\n",
    "- **CatBoost:** Handles categorical features automatically\n",
    "\n",
    "**üìñ References:**\n",
    "- [Scikit-learn: Gradient Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n",
    "- [Original Paper: Friedman (2001)](https://projecteuclid.org/euclid.aos/1013203451)\n",
    "- [ISL Book - Chapter 8: Boosting](https://www.statlearning.com/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting\n",
    "print(\"üöÄ Training Gradient Boosting Regressor...\\n\")\n",
    "\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n‚úÖ Gradient Boosting trained with 100 trees!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Gradient Boosting\n",
    "y_pred_train_gb = gb_model.predict(X_train)\n",
    "y_pred_val_gb = gb_model.predict(X_val)\n",
    "\n",
    "gb_train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train_gb))\n",
    "gb_train_r2 = r2_score(y_train, y_pred_train_gb)\n",
    "gb_val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val_gb))\n",
    "gb_val_mae = mean_absolute_error(y_val, y_pred_val_gb)\n",
    "gb_val_r2 = r2_score(y_val, y_pred_val_gb)\n",
    "\n",
    "print(\"üìä Gradient Boosting Performance:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<30} {'Training':>15} {'Validation':>15}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'RMSE ($)':<30} ${gb_train_rmse:>14,.2f} ${gb_val_rmse:>14,.2f}\")\n",
    "print(f\"{'MAE ($)':<30} {'':>15} ${gb_val_mae:>14,.2f}\")\n",
    "print(f\"{'R¬≤ Score':<30} {gb_train_r2:>15.4f} {gb_val_r2:>15.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "gb_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': gb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"üéØ Gradient Boosting Feature Importance (Top 10):\")\n",
    "print(\"=\"*60)\n",
    "print(gb_feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features_gb = gb_feature_importance.head(10)\n",
    "plt.barh(top_features_gb['Feature'], top_features_gb['Importance'], color='darkorange')\n",
    "plt.xlabel('Feature Importance', fontsize=11)\n",
    "plt.title('Gradient Boosting: Top 10 Feature Importance', fontsize=12, pad=15)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "print(\"üéØ Tuning Gradient Boosting hyperparameters...\\n\")\n",
    "print(\"‚è≥ This will take several minutes...\\n\")\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "gb_random = RandomizedSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_grid_gb,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_random.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best parameters: {gb_random.best_params_}\")\n",
    "print(f\"üìä Best CV RMSE: ${-gb_random.best_score_:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned Gradient Boosting\n",
    "best_gb = gb_random.best_estimator_\n",
    "y_pred_train_gb_tuned = best_gb.predict(X_train)\n",
    "y_pred_val_gb_tuned = best_gb.predict(X_val)\n",
    "\n",
    "gb_tuned_train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train_gb_tuned))\n",
    "gb_tuned_train_r2 = r2_score(y_train, y_pred_train_gb_tuned)\n",
    "gb_tuned_val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val_gb_tuned))\n",
    "gb_tuned_val_mae = mean_absolute_error(y_val, y_pred_val_gb_tuned)\n",
    "gb_tuned_val_r2 = r2_score(y_val, y_pred_val_gb_tuned)\n",
    "\n",
    "print(\"üìä Gradient Boosting (Tuned) Performance:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<30} {'Training':>15} {'Validation':>15}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'RMSE ($)':<30} ${gb_tuned_train_rmse:>14,.2f} ${gb_tuned_val_rmse:>14,.2f}\")\n",
    "print(f\"{'MAE ($)':<30} {'':>15} ${gb_tuned_val_mae:>14,.2f}\")\n",
    "print(f\"{'R¬≤ Score':<30} {gb_tuned_train_r2:>15.4f} {gb_tuned_val_r2:>15.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model Comparison: Tree-Based Models\n",
    "\n",
    "**What you need to do:**  \n",
    "Compare all tree-based models to identify the best performer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "tree_comparison_df = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Decision Tree (default)',\n",
    "        'Decision Tree (tuned)',\n",
    "        'Random Forest (default)',\n",
    "        'Random Forest (tuned)',\n",
    "        'Gradient Boosting (default)',\n",
    "        'Gradient Boosting (tuned)'\n",
    "    ],\n",
    "    'RMSE': [\n",
    "        dt_val_rmse, dt_tuned_val_rmse,\n",
    "        rf_val_rmse, rf_tuned_val_rmse,\n",
    "        gb_val_rmse, gb_tuned_val_rmse\n",
    "    ],\n",
    "    'MAE': [\n",
    "        mean_absolute_error(y_val, y_pred_val_dt), dt_tuned_val_mae,\n",
    "        rf_val_mae, rf_tuned_val_mae,\n",
    "        gb_val_mae, gb_tuned_val_mae\n",
    "    ],\n",
    "    'R¬≤': [\n",
    "        dt_val_r2, dt_tuned_val_r2,\n",
    "        rf_val_r2, rf_tuned_val_r2,\n",
    "        gb_val_r2, gb_tuned_val_r2\n",
    "    ]\n",
    "})\n",
    "\n",
    "tree_comparison_df = tree_comparison_df.sort_values('RMSE')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä TREE-BASED MODELS COMPARISON - VALIDATION SET PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "print(tree_comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_tree_model_name = tree_comparison_df.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ BEST TREE MODEL: {best_tree_model_name}\")\n",
    "print(f\"   RMSE: ${tree_comparison_df.iloc[0]['RMSE']:,.2f}\")\n",
    "print(f\"   R¬≤: {tree_comparison_df.iloc[0]['R¬≤']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# RMSE\n",
    "axes[0].barh(tree_comparison_df['Model'], tree_comparison_df['RMSE'], color='steelblue')\n",
    "axes[0].set_xlabel('RMSE ($)', fontsize=11)\n",
    "axes[0].set_title('Tree Models: RMSE Comparison\\n(Lower is Better)', fontsize=12, pad=15)\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# MAE\n",
    "axes[1].barh(tree_comparison_df['Model'], tree_comparison_df['MAE'], color='coral')\n",
    "axes[1].set_xlabel('MAE ($)', fontsize=11)\n",
    "axes[1].set_title('Tree Models: MAE Comparison\\n(Lower is Better)', fontsize=12, pad=15)\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "# R¬≤\n",
    "axes[2].barh(tree_comparison_df['Model'], tree_comparison_df['R¬≤'], color='seagreen')\n",
    "axes[2].set_xlabel('R¬≤ Score', fontsize=11)\n",
    "axes[2].set_title('Tree Models: R¬≤ Comparison\\n(Higher is Better)', fontsize=12, pad=15)\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Final Evaluation on Test Set\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL: Test set evaluation for best tree model**\n",
    "\n",
    "**What you need to do:**  \n",
    "Evaluate the best tree model on held-out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "if best_tree_model_name == 'Decision Tree (tuned)':\n",
    "    final_tree_model = best_dt\n",
    "elif best_tree_model_name == 'Random Forest (tuned)':\n",
    "    final_tree_model = best_rf\n",
    "elif best_tree_model_name == 'Gradient Boosting (tuned)':\n",
    "    final_tree_model = best_gb\n",
    "else:\n",
    "    final_tree_model = rf_model  # Default to RF\n",
    "\n",
    "print(f\"üèÜ Selected Model: {best_tree_model_name}\")\n",
    "print(f\"\\nüîì Unlocking test set for final evaluation...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final test set evaluation\n",
    "y_pred_test_tree = final_tree_model.predict(X_test)\n",
    "\n",
    "test_rmse_tree = np.sqrt(mean_squared_error(y_test, y_pred_test_tree))\n",
    "test_mae_tree = mean_absolute_error(y_test, y_pred_test_tree)\n",
    "test_r2_tree = r2_score(y_test, y_pred_test_tree)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üìä FINAL TEST SET PERFORMANCE: {best_tree_model_name}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Root Mean Squared Error (RMSE): ${test_rmse_tree:>12,.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE):      ${test_mae_tree:>12,.2f}\")\n",
    "print(f\"R¬≤ Score:                       {test_r2_tree:>12.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare to validation\n",
    "val_rmse_tree = tree_comparison_df.iloc[0]['RMSE']\n",
    "val_r2_tree = tree_comparison_df.iloc[0]['R¬≤']\n",
    "\n",
    "print(f\"\\nüîç Validation vs Test:\")\n",
    "print(f\"   Validation RMSE: ${val_rmse_tree:,.2f}  ‚Üí  Test RMSE: ${test_rmse_tree:,.2f}\")\n",
    "print(f\"   Validation R¬≤: {val_r2_tree:.4f}  ‚Üí  Test R¬≤: {test_r2_tree:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Predictions vs Actual\n",
    "axes[0].scatter(y_test, y_pred_test_tree, alpha=0.5, s=20)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "             'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Total Sales ($)', fontsize=11)\n",
    "axes[0].set_ylabel('Predicted Total Sales ($)', fontsize=11)\n",
    "axes[0].set_title(f'{best_tree_model_name}\\nPredictions vs Actual (Test Set)', fontsize=12, pad=15)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals_tree = y_test - y_pred_test_tree\n",
    "axes[1].scatter(y_pred_test_tree, residuals_tree, alpha=0.5, s=20)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Total Sales ($)', fontsize=11)\n",
    "axes[1].set_ylabel('Residuals ($)', fontsize=11)\n",
    "axes[1].set_title('Residual Plot', fontsize=12, pad=15)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Key Takeaways & Insights\n",
    "\n",
    "**What you should have learned:**\n",
    "\n",
    "### 1Ô∏è‚É£ Tree-Based Model Family\n",
    "\n",
    "‚úÖ **Decision Tree**\n",
    "- Simple, interpretable, prone to overfitting\n",
    "- Use as baseline or building block for ensembles\n",
    "\n",
    "‚úÖ **Random Forest**\n",
    "- Ensemble of independent trees (bagging)\n",
    "- Reduces overfitting through averaging\n",
    "- More stable than single tree\n",
    "- Parallel training (fast)\n",
    "\n",
    "‚úÖ **Gradient Boosting**\n",
    "- Sequential ensemble (boosting)\n",
    "- Each tree corrects previous errors\n",
    "- Often achieves best performance\n",
    "- Sequential training (slower)\n",
    "\n",
    "### 2Ô∏è‚É£ Key Differences: Trees vs Linear Models\n",
    "\n",
    "**Tree-Based Models:**\n",
    "- ‚úÖ Handle non-linear relationships automatically\n",
    "- ‚úÖ Don't require feature scaling\n",
    "- ‚úÖ Capture feature interactions naturally\n",
    "- ‚úÖ Robust to outliers\n",
    "- ‚ùå Less interpretable (especially ensembles)\n",
    "- ‚ùå Can overfit with small datasets\n",
    "- ‚ùå Poor at extrapolation\n",
    "\n",
    "**Linear Models:**\n",
    "- ‚úÖ Highly interpretable (coefficients = feature effects)\n",
    "- ‚úÖ Fast training and prediction\n",
    "- ‚úÖ Work well with limited data\n",
    "- ‚úÖ Good at extrapolation\n",
    "- ‚ùå Assume linear relationships\n",
    "- ‚ùå Require feature engineering for interactions\n",
    "- ‚ùå Sensitive to feature scales (regularized models)\n",
    "\n",
    "### 3Ô∏è‚É£ Ensemble Methods Wisdom\n",
    "\n",
    "**Why Ensembles Work:**\n",
    "- **Bagging (Random Forest):** Reduces variance by averaging diverse models\n",
    "- **Boosting (Gradient Boosting):** Reduces bias by sequentially correcting errors\n",
    "- **Key Insight:** Ensemble of weak learners ‚Üí strong learner\n",
    "\n",
    "**Trade-offs:**\n",
    "- Accuracy ‚Üë, Interpretability ‚Üì\n",
    "- Stability ‚Üë, Training Time ‚Üë\n",
    "- Generalization ‚Üë, Complexity ‚Üë\n",
    "\n",
    "### 4Ô∏è‚É£ Practical Guidelines\n",
    "\n",
    "**When to Use Which Model:**\n",
    "\n",
    "1. **Start with Random Forest:**\n",
    "   - Works well out-of-the-box\n",
    "   - Good baseline for tree models\n",
    "   - Robust to hyperparameters\n",
    "\n",
    "2. **Try Gradient Boosting if:**\n",
    "   - You need maximum accuracy\n",
    "   - You have time for tuning\n",
    "   - Dataset is not too small\n",
    "\n",
    "3. **Use Single Decision Tree if:**\n",
    "   - Interpretability is critical\n",
    "   - You need to explain every decision\n",
    "   - Dataset is small\n",
    "\n",
    "4. **Consider Linear Models if:**\n",
    "   - Relationships are approximately linear\n",
    "   - You need coefficient interpretation\n",
    "   - Speed is critical\n",
    "   - You need to extrapolate\n",
    "\n",
    "### 5Ô∏è‚É£ Feature Importance Insights\n",
    "- Different models may rank features differently\n",
    "- Ensemble methods provide more stable rankings\n",
    "- Always validate importance with domain knowledge\n",
    "- High importance ‚â† causation\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Reflection Questions\n",
    "1. Did tree models outperform linear models on this dataset? Why?\n",
    "2. Why does Random Forest reduce overfitting compared to single trees?\n",
    "3. What's the trade-off between Random Forest and Gradient Boosting?\n",
    "4. How do feature importance rankings differ across models?\n",
    "5. When would you prefer a linear model over a tree model?\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Next Steps: Week 10\n",
    "**Advanced Topics:**\n",
    "- Classification models (same algorithms, different task)\n",
    "- Advanced boosting: XGBoost, LightGBM, CatBoost\n",
    "- Model deployment (Flask/FastAPI)\n",
    "- Time series forecasting\n",
    "\n",
    "---\n",
    "\n",
    "**AI Tech Institute** | *Building Tomorrow's AI Engineers Today*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
