{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI TECH INSTITUTE** ¬∑ *Intermediate AI & Data Science*\n",
    "### Week 10 Session 2: Tree-Based Models for Classification\n",
    "**Instructor:** Amir Charkhi | **Goal:** Master Tree-Based Classification\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand decision trees for classification\n",
    "- Learn ensemble methods: Random Forest and Gradient Boosting\n",
    "- Compare tree-based with linear classification models\n",
    "- Master feature importance for classification\n",
    "- Handle imbalanced classification with tree models\n",
    "- Apply advanced hyperparameter tuning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "**What you need to do:**  \n",
    "Import all necessary libraries for tree-based classification.\n",
    "\n",
    "**üí° Hint:** We'll need `DecisionTreeClassifier`, `RandomForestClassifier`, `GradientBoostingClassifier`, and classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# Advanced gradient boosting (optional)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    print(\"‚úÖ XGBoost available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è XGBoost not installed\")\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    print(\"‚úÖ LightGBM available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è LightGBM not installed\")\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"\\n‚úÖ All core libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load and Prepare Dataset\n",
    "\n",
    "**What you need to do:**  \n",
    "Load the same Online Shoppers dataset we used for linear models.\n",
    "\n",
    "**Our Goal:** Predict **Revenue** (purchase or not) using tree-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv'\n",
    "\n",
    "print(\"üì• Loading Online Shoppers dataset...\")\n",
    "df_raw = pd.read_csv(url)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded!\")\n",
    "print(f\"üìä Shape: {df_raw.shape[0]:,} rows √ó {df_raw.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Preprocessing\n",
    "\n",
    "**What you need to do:**  \n",
    "Apply the same preprocessing steps as linear models.\n",
    "\n",
    "**Note:** Tree-based models have different characteristics:\n",
    "- ‚úÖ Don't require feature scaling\n",
    "- ‚úÖ Handle non-linear relationships naturally\n",
    "- ‚úÖ Can work with categorical variables (though we'll encode them)\n",
    "- ‚úÖ Robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "print(\"üßπ Preprocessing data...\\n\")\n",
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Convert target to binary\n",
    "df['Revenue'] = df['Revenue'].astype(int)\n",
    "\n",
    "# Encode categorical variables\n",
    "month_map = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'June': 6,\n",
    "             'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}\n",
    "df['Month'] = df['Month'].map(month_map)\n",
    "\n",
    "# VisitorType: one-hot encoding\n",
    "visitor_dummies = pd.get_dummies(df['VisitorType'], prefix='Visitor', drop_first=True)\n",
    "visitor_dummies = visitor_dummies.astype(int)\n",
    "df = pd.concat([df, visitor_dummies], axis=1)\n",
    "\n",
    "# Weekend to int\n",
    "df['Weekend'] = df['Weekend'].astype(int)\n",
    "\n",
    "# Drop original categorical column\n",
    "df = df.drop(columns=['VisitorType'])\n",
    "\n",
    "# Prepare features\n",
    "feature_cols = [col for col in df.columns if col != 'Revenue']\n",
    "X = df[feature_cols].copy()\n",
    "y = df['Revenue'].copy()\n",
    "\n",
    "print(f\"‚úÖ Preprocessing complete!\")\n",
    "print(f\"üìä Features: {len(feature_cols)} columns\")\n",
    "print(f\"üìä Class distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Train-Validation-Test Split\n",
    "\n",
    "**What you need to do:**  \n",
    "Split data with stratification: 60% train, 20% validation, 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with stratification\n",
    "print(\"‚úÇÔ∏è Splitting data with stratification...\\n\")\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"üìä Training:   {X_train.shape[0]:>6,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"üìä Validation: {X_val.shape[0]:>6,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"üìä Test:       {X_test.shape[0]:>6,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Class distribution maintained:\")\n",
    "print(f\"   Train:      {y_train.value_counts(normalize=True)[1]:.3f} positive class\")\n",
    "print(f\"   Validation: {y_val.value_counts(normalize=True)[1]:.3f} positive class\")\n",
    "print(f\"   Test:       {y_test.value_counts(normalize=True)[1]:.3f} positive class\")\n",
    "\n",
    "print(f\"\\nüí° Note: Tree models DON'T require feature scaling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Quick EDA Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick summary\n",
    "print(\"üìä Training Features Summary:\")\n",
    "print(X_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Helper Function: Classification Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(model, X_train, y_train, X_val, y_val, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of classification model.\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Probabilities\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_val_prob = model.predict_proba(X_val)[:, 1]\n",
    "        val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "    else:\n",
    "        y_val_prob = None\n",
    "        val_auc = None\n",
    "    \n",
    "    # Metrics\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    val_precision = precision_score(y_val, y_val_pred)\n",
    "    val_recall = recall_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"üìä {model_name} Performance:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Metric':<30} {'Training':>15} {'Validation':>15}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Accuracy':<30} {train_acc:>15.4f} {val_acc:>15.4f}\")\n",
    "    print(f\"{'Precision':<30} {'':>15} {val_precision:>15.4f}\")\n",
    "    print(f\"{'Recall':<30} {'':>15} {val_recall:>15.4f}\")\n",
    "    print(f\"{'F1-Score':<30} {'':>15} {val_f1:>15.4f}\")\n",
    "    if val_auc is not None:\n",
    "        print(f\"{'ROC-AUC':<30} {'':>15} {val_auc:>15.4f}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check overfitting\n",
    "    if train_acc - val_acc > 0.1:\n",
    "        print(f\"\\n‚ö†Ô∏è Overfitting detected! Training accuracy much higher than validation.\")\n",
    "    \n",
    "    return {\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'val_precision': val_precision,\n",
    "        'val_recall': val_recall,\n",
    "        'val_f1': val_f1,\n",
    "        'val_auc': val_auc,\n",
    "        'y_val_pred': y_val_pred,\n",
    "        'y_val_prob': y_val_prob\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Helper function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Model 1: Decision Tree Classifier\n",
    "\n",
    "**üìö Theory:**  \n",
    "Decision Trees for classification work similarly to regression trees but predict class labels.\n",
    "\n",
    "**How It Works:**\n",
    "1. Start with all data at root\n",
    "2. Find best split that maximizes **information gain** or minimizes **Gini impurity**\n",
    "3. Create child nodes recursively\n",
    "4. Predict: most common class in leaf node\n",
    "\n",
    "**Splitting Criteria:**\n",
    "\n",
    "**Gini Impurity (default):**\n",
    "$$Gini = 1 - \\sum_{i=1}^{c} p_i^2$$\n",
    "Where $p_i$ is probability of class $i$. Lower Gini = purer node.\n",
    "\n",
    "**Entropy (Information Gain):**\n",
    "$$Entropy = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$$\n",
    "Measures uncertainty. Lower entropy = more certain.\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- **criterion:** 'gini' or 'entropy'\n",
    "- **max_depth:** Maximum tree depth\n",
    "- **min_samples_split:** Min samples to split\n",
    "- **min_samples_leaf:** Min samples in leaf\n",
    "- **max_features:** Features per split\n",
    "- **class_weight:** Handle imbalanced classes ('balanced' or dict)\n",
    "\n",
    "**Pros:**\n",
    "- Easy to visualize and interpret\n",
    "- Handles non-linear patterns\n",
    "- No scaling needed\n",
    "- Can output feature importance\n",
    "\n",
    "**Cons:**\n",
    "- **Very prone to overfitting**\n",
    "- Unstable (small changes ‚Üí different tree)\n",
    "- Not optimal for prediction accuracy\n",
    "\n",
    "**üìñ References:**\n",
    "- [Scikit-learn: Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree (unrestricted)\n",
    "print(\"üå≥ Training Decision Tree Classifier (Unrestricted)...\\n\")\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Decision Tree trained!\")\n",
    "print(f\"\\nüìä Tree Structure:\")\n",
    "print(f\"   Max depth: {dt_model.get_depth()}\")\n",
    "print(f\"   Number of leaves: {dt_model.get_n_leaves()}\")\n",
    "print(f\"\\n\")\n",
    "\n",
    "# Evaluate\n",
    "dt_results = evaluate_classifier(dt_model, X_train, y_train, X_val, y_val, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "dt_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': dt_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"üéØ Decision Tree Feature Importance (Top 10):\")\n",
    "print(\"=\"*60)\n",
    "print(dt_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = dt_importance.head(10)\n",
    "plt.barh(top_features['Feature'], top_features['Importance'])\n",
    "plt.xlabel('Feature Importance', fontsize=11)\n",
    "plt.title('Decision Tree: Top 10 Features', fontsize=12, pad=15)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val, dt_results['y_val_pred'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['No Purchase', 'Purchase'],\n",
    "            yticklabels=['No Purchase', 'Purchase'])\n",
    "plt.xlabel('Predicted', fontsize=11)\n",
    "plt.ylabel('Actual', fontsize=11)\n",
    "plt.title('Decision Tree: Confusion Matrix', fontsize=12, pad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "print(\"üéØ Tuning Decision Tree hyperparameters...\\n\")\n",
    "\n",
    "param_grid_dt = {\n",
    "    'max_depth': [3, 5, 7, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "dt_grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid_dt,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "dt_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best parameters: {dt_grid.best_params_}\")\n",
    "print(f\"üìä Best CV F1-Score: {dt_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model\n",
    "best_dt = dt_grid.best_estimator_\n",
    "dt_tuned_results = evaluate_classifier(best_dt, X_train, y_train, X_val, y_val,\n",
    "                                        \"Decision Tree (Tuned)\")\n",
    "\n",
    "print(f\"\\nüìä Tree Structure (Tuned):\")\n",
    "print(f\"   Max depth: {best_dt.get_depth()}\")\n",
    "print(f\"   Number of leaves: {best_dt.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Model 2: Random Forest Classifier\n",
    "\n",
    "**üìö Theory:**  \n",
    "Random Forest for classification: ensemble of decision tree classifiers.\n",
    "\n",
    "**How It Works:**\n",
    "1. Create N decision tree classifiers\n",
    "2. Each tree: bootstrap sample + random feature subset\n",
    "3. Each tree votes for a class\n",
    "4. Final prediction = **majority vote**\n",
    "5. Probabilities = average of tree probabilities\n",
    "\n",
    "**Key Advantages for Classification:**\n",
    "- Reduces overfitting dramatically\n",
    "- Provides probability estimates\n",
    "- Handles imbalanced classes well\n",
    "- Out-of-bag (OOB) score for free validation\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- **n_estimators:** Number of trees\n",
    "- **max_depth:** Tree depth\n",
    "- **min_samples_split / leaf:** Node constraints\n",
    "- **max_features:** Features per split ('sqrt' is common for classification)\n",
    "- **class_weight:** Handle imbalanced classes\n",
    "- **bootstrap:** Use bootstrap sampling (default True)\n",
    "\n",
    "**Pros:**\n",
    "- Excellent performance out-of-the-box\n",
    "- Robust to overfitting\n",
    "- Handles imbalanced data\n",
    "- Provides feature importance\n",
    "- Probability estimates available\n",
    "\n",
    "**Cons:**\n",
    "- Less interpretable than single tree\n",
    "- Slower than single tree\n",
    "- Larger model size\n",
    "\n",
    "**üìñ References:**\n",
    "- [Scikit-learn: Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "print(\"üå≤üå≤üå≤ Training Random Forest Classifier...\\n\")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n‚úÖ Random Forest trained with 100 trees!\\n\")\n",
    "\n",
    "# Evaluate\n",
    "rf_results = evaluate_classifier(rf_model, X_train, y_train, X_val, y_val, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"üéØ Random Forest Feature Importance (Top 10):\")\n",
    "print(\"=\"*60)\n",
    "print(rf_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features_rf = rf_importance.head(10)\n",
    "plt.barh(top_features_rf['Feature'], top_features_rf['Importance'], color='forestgreen')\n",
    "plt.xlabel('Feature Importance', fontsize=11)\n",
    "plt.title('Random Forest: Top 10 Features', fontsize=12, pad=15)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_val, rf_results['y_val_prob'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f\"Random Forest (AUC = {rf_results['val_auc']:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "plt.xlabel('False Positive Rate', fontsize=11)\n",
    "plt.ylabel('True Positive Rate', fontsize=11)\n",
    "plt.title('Random Forest: ROC Curve', fontsize=12, pad=15)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "print(\"üéØ Tuning Random Forest hyperparameters...\\n\")\n",
    "print(\"‚è≥ This may take a few minutes...\\n\")\n",
    "\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_dist_rf,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best parameters: {rf_random.best_params_}\")\n",
    "print(f\"üìä Best CV F1-Score: {rf_random.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned Random Forest\n",
    "best_rf = rf_random.best_estimator_\n",
    "rf_tuned_results = evaluate_classifier(best_rf, X_train, y_train, X_val, y_val,\n",
    "                                        \"Random Forest (Tuned)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model 3: Gradient Boosting Classifier\n",
    "\n",
    "**üìö Theory:**  \n",
    "Gradient Boosting for classification: sequential ensemble that builds trees to correct errors.\n",
    "\n",
    "**How It Works:**\n",
    "1. Start with simple prediction (log-odds of base rate)\n",
    "2. Calculate pseudo-residuals (gradient of loss)\n",
    "3. Fit new tree to residuals\n",
    "4. Update model by adding scaled tree prediction\n",
    "5. Repeat for N iterations\n",
    "6. Final prediction via sigmoid transformation\n",
    "\n",
    "**Loss Function (Log Loss):**\n",
    "$$L = -\\frac{1}{m} \\sum_{i=1}^{m} [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)]$$\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- **n_estimators:** Number of boosting iterations\n",
    "- **learning_rate:** Shrinkage (0.01-0.3)\n",
    "- **max_depth:** Tree depth (3-8 works well)\n",
    "- **subsample:** Fraction of samples per tree\n",
    "- **min_samples_split / leaf:** Node constraints\n",
    "\n",
    "**Pros:**\n",
    "- Often best performance\n",
    "- Handles imbalanced classes well\n",
    "- Provides probability estimates\n",
    "- Feature importance available\n",
    "\n",
    "**Cons:**\n",
    "- Sequential training (slower)\n",
    "- More hyperparameters to tune\n",
    "- Can overfit if not careful\n",
    "- Less interpretable\n",
    "\n",
    "**üìñ References:**\n",
    "- [Scikit-learn: Gradient Boosting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting\n",
    "print(\"üöÄ Training Gradient Boosting Classifier...\\n\")\n",
    "\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n‚úÖ Gradient Boosting trained!\\n\")\n",
    "\n",
    "# Evaluate\n",
    "gb_results = evaluate_classifier(gb_model, X_train, y_train, X_val, y_val,\n",
    "                                  \"Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "gb_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': gb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"üéØ Gradient Boosting Feature Importance (Top 10):\")\n",
    "print(\"=\"*60)\n",
    "print(gb_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features_gb = gb_importance.head(10)\n",
    "plt.barh(top_features_gb['Feature'], top_features_gb['Importance'], color='darkorange')\n",
    "plt.xlabel('Feature Importance', fontsize=11)\n",
    "plt.title('Gradient Boosting: Top 10 Features', fontsize=12, pad=15)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "print(\"üéØ Tuning Gradient Boosting hyperparameters...\\n\")\n",
    "print(\"‚è≥ This will take several minutes...\\n\")\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "gb_random = RandomizedSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    param_grid_gb,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_random.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best parameters: {gb_random.best_params_}\")\n",
    "print(f\"üìä Best CV F1-Score: {gb_random.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned Gradient Boosting\n",
    "best_gb = gb_random.best_estimator_\n",
    "gb_tuned_results = evaluate_classifier(best_gb, X_train, y_train, X_val, y_val,\n",
    "                                        \"Gradient Boosting (Tuned)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Model Comparison\n",
    "\n",
    "**What you need to do:**  \n",
    "Compare all tree-based classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "tree_comparison = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Decision Tree',\n",
    "        'Decision Tree (Tuned)',\n",
    "        'Random Forest',\n",
    "        'Random Forest (Tuned)',\n",
    "        'Gradient Boosting',\n",
    "        'Gradient Boosting (Tuned)'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        dt_results['val_acc'],\n",
    "        dt_tuned_results['val_acc'],\n",
    "        rf_results['val_acc'],\n",
    "        rf_tuned_results['val_acc'],\n",
    "        gb_results['val_acc'],\n",
    "        gb_tuned_results['val_acc']\n",
    "    ],\n",
    "    'Precision': [\n",
    "        dt_results['val_precision'],\n",
    "        dt_tuned_results['val_precision'],\n",
    "        rf_results['val_precision'],\n",
    "        rf_tuned_results['val_precision'],\n",
    "        gb_results['val_precision'],\n",
    "        gb_tuned_results['val_precision']\n",
    "    ],\n",
    "    'Recall': [\n",
    "        dt_results['val_recall'],\n",
    "        dt_tuned_results['val_recall'],\n",
    "        rf_results['val_recall'],\n",
    "        rf_tuned_results['val_recall'],\n",
    "        gb_results['val_recall'],\n",
    "        gb_tuned_results['val_recall']\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        dt_results['val_f1'],\n",
    "        dt_tuned_results['val_f1'],\n",
    "        rf_results['val_f1'],\n",
    "        rf_tuned_results['val_f1'],\n",
    "        gb_results['val_f1'],\n",
    "        gb_tuned_results['val_f1']\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        dt_results['val_auc'],\n",
    "        dt_tuned_results['val_auc'],\n",
    "        rf_results['val_auc'],\n",
    "        rf_tuned_results['val_auc'],\n",
    "        gb_results['val_auc'],\n",
    "        gb_tuned_results['val_auc']\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Sort by F1-Score\n",
    "tree_comparison = tree_comparison.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"üìä TREE-BASED CLASSIFICATION MODELS - VALIDATION SET\")\n",
    "print(\"=\"*90)\n",
    "print(tree_comparison.to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "\n",
    "best_tree_model = tree_comparison.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ BEST TREE MODEL: {best_tree_model}\")\n",
    "print(f\"   F1-Score: {tree_comparison.iloc[0]['F1-Score']:.4f}\")\n",
    "print(f\"   ROC-AUC:  {tree_comparison.iloc[0]['ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Accuracy\n",
    "axes[0,0].barh(tree_comparison['Model'], tree_comparison['Accuracy'], color='steelblue')\n",
    "axes[0,0].set_xlabel('Accuracy')\n",
    "axes[0,0].set_title('Tree Models: Accuracy')\n",
    "axes[0,0].invert_yaxis()\n",
    "\n",
    "# F1-Score\n",
    "axes[0,1].barh(tree_comparison['Model'], tree_comparison['F1-Score'], color='purple')\n",
    "axes[0,1].set_xlabel('F1-Score')\n",
    "axes[0,1].set_title('Tree Models: F1-Score (Higher is Better)')\n",
    "axes[0,1].invert_yaxis()\n",
    "\n",
    "# Precision\n",
    "axes[1,0].barh(tree_comparison['Model'], tree_comparison['Precision'], color='coral')\n",
    "axes[1,0].set_xlabel('Precision')\n",
    "axes[1,0].set_title('Tree Models: Precision')\n",
    "axes[1,0].invert_yaxis()\n",
    "\n",
    "# ROC-AUC\n",
    "axes[1,1].barh(tree_comparison['Model'], tree_comparison['ROC-AUC'], color='seagreen')\n",
    "axes[1,1].set_xlabel('ROC-AUC')\n",
    "axes[1,1].set_title('Tree Models: ROC-AUC')\n",
    "axes[1,1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Final Evaluation on Test Set\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL: Test set evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "if best_tree_model == 'Decision Tree (Tuned)':\n",
    "    final_model = best_dt\n",
    "elif best_tree_model == 'Random Forest (Tuned)':\n",
    "    final_model = best_rf\n",
    "elif best_tree_model == 'Gradient Boosting (Tuned)':\n",
    "    final_model = best_gb\n",
    "else:\n",
    "    final_model = best_rf  # Default\n",
    "\n",
    "print(f\"üèÜ Selected Model: {best_tree_model}\")\n",
    "print(f\"\\nüîì Unlocking test set...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final test evaluation\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "y_test_prob = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üìä FINAL TEST SET PERFORMANCE: {best_tree_model}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall:    {test_recall:.4f}\")\n",
    "print(f\"F1-Score:  {test_f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {test_auc:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìä Detailed Classification Report:\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_test_pred, target_names=['No Purchase', 'Purchase']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['No Purchase', 'Purchase'],\n",
    "            yticklabels=['No Purchase', 'Purchase'])\n",
    "plt.xlabel('Predicted', fontsize=11)\n",
    "plt.ylabel('Actual', fontsize=11)\n",
    "plt.title(f'{best_tree_model}: Confusion Matrix (Test Set)', fontsize=12, pad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Key Takeaways\n",
    "\n",
    "**What you should have learned:**\n",
    "\n",
    "### 1Ô∏è‚É£ Tree-Based Classification Models\n",
    "\n",
    "‚úÖ **Decision Tree Classifier**\n",
    "- Interpretable but prone to overfitting\n",
    "- Uses Gini or Entropy for splits\n",
    "- Good for understanding feature interactions\n",
    "\n",
    "‚úÖ **Random Forest Classifier**\n",
    "- Robust ensemble method\n",
    "- Excellent for imbalanced classification\n",
    "- Provides probability estimates\n",
    "- Often best out-of-the-box performance\n",
    "\n",
    "‚úÖ **Gradient Boosting Classifier**\n",
    "- Sequential ensemble (boosting)\n",
    "- Often achieves best performance when tuned\n",
    "- More sensitive to hyperparameters\n",
    "- Slower training than Random Forest\n",
    "\n",
    "### 2Ô∏è‚É£ Trees vs Linear Models for Classification\n",
    "\n",
    "**Tree-Based Advantages:**\n",
    "- Handle non-linear decision boundaries\n",
    "- No feature scaling needed\n",
    "- Automatic feature interaction detection\n",
    "- Robust to outliers\n",
    "- Work well with mixed data types\n",
    "\n",
    "**Linear Model Advantages:**\n",
    "- More interpretable coefficients\n",
    "- Faster training and prediction\n",
    "- Better with high-dimensional sparse data\n",
    "- Calibrated probabilities (Logistic Regression)\n",
    "\n",
    "### 3Ô∏è‚É£ Handling Imbalanced Classification\n",
    "\n",
    "**Strategies Used:**\n",
    "- `class_weight='balanced'`: Adjusts for class imbalance\n",
    "- `stratify=y`: Maintains class distribution in splits\n",
    "- F1-Score: Better metric than accuracy\n",
    "- ROC-AUC: Threshold-independent metric\n",
    "\n",
    "**Other Options (not covered):**\n",
    "- SMOTE: Synthetic minority oversampling\n",
    "- Undersampling majority class\n",
    "- Threshold tuning based on business costs\n",
    "\n",
    "### 4Ô∏è‚É£ Feature Importance Insights\n",
    "\n",
    "**Key Findings (likely):**\n",
    "- PageValues: Most important (higher values ‚Üí purchase)\n",
    "- ProductRelated_Duration: Time on product pages matters\n",
    "- ExitRates/BounceRates: Negative indicators\n",
    "- Month: Seasonal patterns exist\n",
    "\n",
    "**Important Notes:**\n",
    "- Feature importance ‚â† causation\n",
    "- Different models may rank features differently\n",
    "- Always validate with domain knowledge\n",
    "\n",
    "### 5Ô∏è‚É£ Model Selection Guidelines\n",
    "\n",
    "**For Production Systems:**\n",
    "1. **Start with Random Forest**: Robust, minimal tuning needed\n",
    "2. **Try Gradient Boosting**: If you need max performance\n",
    "3. **Consider Logistic Regression**: If interpretability crucial\n",
    "4. **Avoid single Decision Tree**: Unless interpretability is paramount\n",
    "\n",
    "**For This E-Commerce Dataset:**\n",
    "- Tree models likely outperformed linear models\n",
    "- Random Forest: Great balance of performance and speed\n",
    "- Gradient Boosting: Possibly best F1-score\n",
    "- Class imbalance (~16% purchase) handled well by trees\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Reflection Questions\n",
    "1. Did tree models outperform linear models? Why?\n",
    "2. How did Random Forest reduce overfitting vs single tree?\n",
    "3. What's the precision-recall tradeoff for this business problem?\n",
    "4. Which model would you deploy and why?\n",
    "5. How would you handle false positives vs false negatives?\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Next Steps: Week 11\n",
    "**Advanced Topics:**\n",
    "- Neural Networks for classification\n",
    "- XGBoost and LightGBM deep dive\n",
    "- Model deployment with Flask/FastAPI\n",
    "- Advanced feature engineering\n",
    "- Calibration and threshold optimization\n",
    "\n",
    "---\n",
    "\n",
    "**AI Tech Institute** | *Building Tomorrow's AI Engineers Today*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
